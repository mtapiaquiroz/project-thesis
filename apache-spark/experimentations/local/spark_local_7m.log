Execution 1:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-83a76473-d720-4229-aefc-7534da37fbf6;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (617ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (527ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (348ms)
:: resolution report :: resolve 7988ms :: artifacts dl 1507ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-83a76473-d720-4229-aefc-7534da37fbf6
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 02:59:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:59:42 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:59:42 INFO ResourceUtils: ==============================================================
23/11/29 02:59:42 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:59:42 INFO ResourceUtils: ==============================================================
23/11/29 02:59:42 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:59:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:59:42 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:59:42 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:59:42 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:59:42 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:59:42 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:59:42 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:59:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:59:43 INFO Utils: Successfully started service 'sparkDriver' on port 33887.
23/11/29 02:59:43 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:59:43 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:59:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:59:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:59:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:59:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-94018386-9769-4314-b07b-980437afbe1c
23/11/29 02:59:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:59:43 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:59:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:59:43 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://b35b76aa1a2d:33887/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226782759
23/11/29 02:59:43 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://b35b76aa1a2d:33887/jars/org.tukaani_xz-1.9.jar with timestamp 1701226782759
23/11/29 02:59:43 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://b35b76aa1a2d:33887/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226782759
23/11/29 02:59:43 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://b35b76aa1a2d:33887/jars/spark.jar with timestamp 1701226782759
23/11/29 02:59:43 INFO Executor: Starting executor ID driver on host b35b76aa1a2d
23/11/29 02:59:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:59:43 INFO Executor: Fetching spark://b35b76aa1a2d:33887/jars/spark.jar with timestamp 1701226782759
23/11/29 02:59:43 INFO TransportClientFactory: Successfully created connection to b35b76aa1a2d/172.17.0.2:33887 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:59:43 INFO Utils: Fetching spark://b35b76aa1a2d:33887/jars/spark.jar to /tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/fetchFileTemp12285172946405974520.tmp
23/11/29 02:59:44 INFO Executor: Adding file:/tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/spark.jar to class loader
23/11/29 02:59:44 INFO Executor: Fetching spark://b35b76aa1a2d:33887/jars/org.tukaani_xz-1.9.jar with timestamp 1701226782759
23/11/29 02:59:44 INFO Utils: Fetching spark://b35b76aa1a2d:33887/jars/org.tukaani_xz-1.9.jar to /tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/fetchFileTemp14517352927628347655.tmp
23/11/29 02:59:44 INFO Executor: Adding file:/tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:59:44 INFO Executor: Fetching spark://b35b76aa1a2d:33887/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226782759
23/11/29 02:59:44 INFO Utils: Fetching spark://b35b76aa1a2d:33887/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/fetchFileTemp9055360276174324590.tmp
23/11/29 02:59:44 INFO Executor: Adding file:/tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:59:44 INFO Executor: Fetching spark://b35b76aa1a2d:33887/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226782759
23/11/29 02:59:44 INFO Utils: Fetching spark://b35b76aa1a2d:33887/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/fetchFileTemp16534450576060134850.tmp
23/11/29 02:59:44 INFO Executor: Adding file:/tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f/userFiles-da6ef199-784f-4cd1-a788-fcd95ae50b00/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:59:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38273.
23/11/29 02:59:44 INFO NettyBlockTransferService: Server created on b35b76aa1a2d:38273
23/11/29 02:59:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:59:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b35b76aa1a2d, 38273, None)
23/11/29 02:59:44 INFO BlockManagerMasterEndpoint: Registering block manager b35b76aa1a2d:38273 with 434.4 MiB RAM, BlockManagerId(driver, b35b76aa1a2d, 38273, None)
23/11/29 02:59:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b35b76aa1a2d, 38273, None)
23/11/29 02:59:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b35b76aa1a2d, 38273, None)
23/11/29 02:59:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:59:44 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:59:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:59:45 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
23/11/29 02:59:47 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:59:47 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:59:47 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:59:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:59:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:59:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b35b76aa1a2d:38273 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:59:48 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:59:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:59:48 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:59:48 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:59:48 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:59:48 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:59:48 INFO DAGScheduler: Missing parents: List()
23/11/29 02:59:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:59:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:59:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:59:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b35b76aa1a2d:38273 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:59:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:59:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:59:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:59:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b35b76aa1a2d, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:59:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:59:48 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:59:48 INFO CodeGenerator: Code generated in 130.113551 ms
23/11/29 02:59:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:59:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 422 ms on b35b76aa1a2d (executor driver) (1/1)
23/11/29 02:59:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:59:49 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.514 s
23/11/29 02:59:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:59:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:59:49 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.553837 s
23/11/29 02:59:49 INFO CodeGenerator: Code generated in 12.664445 ms
23/11/29 02:59:49 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:59:49 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:59:49 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:59:49 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:59:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:59:49 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:59:49 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:59:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:59:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b35b76aa1a2d:38273 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:59:49 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:59:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:59:49 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:59:49 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 02:59:49 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:59:49 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:59:49 INFO DAGScheduler: Missing parents: List()
23/11/29 02:59:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:59:49 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:59:49 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 02:59:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b35b76aa1a2d:38273 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 02:59:49 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:59:49 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 02:59:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 02:59:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b35b76aa1a2d, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (b35b76aa1a2d, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (b35b76aa1a2d, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (b35b76aa1a2d, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (b35b76aa1a2d, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (b35b76aa1a2d, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (b35b76aa1a2d, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (b35b76aa1a2d, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:59:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:59:49 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:59:49 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:59:49 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:59:49 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:59:49 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:59:49 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:59:49 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:59:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b35b76aa1a2d:38273 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 02:59:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:59:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b35b76aa1a2d:38273 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 02:59:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 02:59:49 INFO CodeGenerator: Code generated in 96.699143 ms
23/11/29 03:00:12 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000001
23/11/29 03:00:12 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000001_2: Committed. Elapsed time: 21 ms.
23/11/29 03:00:12 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2541 bytes result sent to driver
23/11/29 03:00:12 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (b35b76aa1a2d, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:12 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:00:12 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 22868 ms on b35b76aa1a2d (executor driver) (1/23)
23/11/29 03:00:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:12 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:00:12 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000000
23/11/29 03:00:12 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 03:00:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:00:12 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (b35b76aa1a2d, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:12 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:00:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 23112 ms on b35b76aa1a2d (executor driver) (2/23)
23/11/29 03:00:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:12 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:00:12 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000003
23/11/29 03:00:12 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000003_4: Committed. Elapsed time: 17 ms.
23/11/29 03:00:12 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:00:12 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (b35b76aa1a2d, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:12 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:00:12 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 23412 ms on b35b76aa1a2d (executor driver) (3/23)
23/11/29 03:00:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:12 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:12 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000002
23/11/29 03:00:13 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 03:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000006
23/11/29 03:00:13 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:00:13 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 03:00:13 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (b35b76aa1a2d, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:13 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:00:13 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 23673 ms on b35b76aa1a2d (executor driver) (4/23)
23/11/29 03:00:13 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:00:13 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (b35b76aa1a2d, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:13 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 23677 ms on b35b76aa1a2d (executor driver) (5/23)
23/11/29 03:00:13 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000004
23/11/29 03:00:13 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000004_5: Committed. Elapsed time: 12 ms.
23/11/29 03:00:13 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:00:13 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (b35b76aa1a2d, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:13 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:00:13 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 23921 ms on b35b76aa1a2d (executor driver) (6/23)
23/11/29 03:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000007
23/11/29 03:00:13 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:00:13 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:00:13 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (b35b76aa1a2d, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:13 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 24157 ms on b35b76aa1a2d (executor driver) (7/23)
23/11/29 03:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000005
23/11/29 03:00:13 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 03:00:13 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:00:13 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:00:13 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (b35b76aa1a2d, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:13 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:00:13 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 24184 ms on b35b76aa1a2d (executor driver) (8/23)
23/11/29 03:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000010
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000010_11: Committed. Elapsed time: 2 ms.
23/11/29 03:00:37 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (b35b76aa1a2d, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:00:37 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 24398 ms on b35b76aa1a2d (executor driver) (9/23)
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000011
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000011_12: Committed. Elapsed time: 0 ms.
23/11/29 03:00:37 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (b35b76aa1a2d, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 24183 ms on b35b76aa1a2d (executor driver) (10/23)
23/11/29 03:00:37 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000012
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000012_13: Committed. Elapsed time: 3 ms.
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000014
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000014_15: Committed. Elapsed time: 1 ms.
23/11/29 03:00:37 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (b35b76aa1a2d, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 24381 ms on b35b76aa1a2d (executor driver) (11/23)
23/11/29 03:00:37 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:00:37 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (b35b76aa1a2d, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 23903 ms on b35b76aa1a2d (executor driver) (12/23)
23/11/29 03:00:37 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000008
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000008_9: Committed. Elapsed time: 0 ms.
23/11/29 03:00:37 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (b35b76aa1a2d, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:00:37 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 25491 ms on b35b76aa1a2d (executor driver) (13/23)
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000013
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000013_14: Committed. Elapsed time: 0 ms.
23/11/29 03:00:37 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (b35b76aa1a2d, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:00:37 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 24444 ms on b35b76aa1a2d (executor driver) (14/23)
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000009
23/11/29 03:00:37 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000009_10: Committed. Elapsed time: 0 ms.
23/11/29 03:00:37 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:00:37 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (b35b76aa1a2d, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:00:37 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 25386 ms on b35b76aa1a2d (executor driver) (15/23)
23/11/29 03:00:37 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:00:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:00:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:00:38 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000015
23/11/29 03:00:38 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000015_16: Committed. Elapsed time: 0 ms.
23/11/29 03:00:38 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:00:38 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 24400 ms on b35b76aa1a2d (executor driver) (16/23)
23/11/29 03:00:53 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000022
23/11/29 03:00:53 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:00:53 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2541 bytes result sent to driver
23/11/29 03:00:53 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 15161 ms on b35b76aa1a2d (executor driver) (17/23)
23/11/29 03:00:54 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000017
23/11/29 03:00:54 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:00:54 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:00:54 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 16840 ms on b35b76aa1a2d (executor driver) (18/23)
23/11/29 03:00:54 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000016
23/11/29 03:00:54 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:00:54 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:00:54 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 16997 ms on b35b76aa1a2d (executor driver) (19/23)
23/11/29 03:00:54 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000018
23/11/29 03:00:54 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:00:54 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:00:54 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 17083 ms on b35b76aa1a2d (executor driver) (20/23)
23/11/29 03:00:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000019
23/11/29 03:00:55 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:00:55 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:00:55 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 17586 ms on b35b76aa1a2d (executor driver) (21/23)
23/11/29 03:00:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000021
23/11/29 03:00:55 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000021_22: Committed. Elapsed time: 0 ms.
23/11/29 03:00:55 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:00:55 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 17354 ms on b35b76aa1a2d (executor driver) (22/23)
23/11/29 03:00:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025949437702893473452550_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025949437702893473452550_0001_m_000020
23/11/29 03:00:55 INFO SparkHadoopMapRedUtil: attempt_20231129025949437702893473452550_0001_m_000020_21: Committed. Elapsed time: 0 ms.
23/11/29 03:00:55 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:00:55 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 17384 ms on b35b76aa1a2d (executor driver) (23/23)
23/11/29 03:00:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:00:55 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 65.773 s
23/11/29 03:00:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:00:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:00:55 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 65.780936 s
23/11/29 03:00:55 INFO FileFormatWriter: Start to commit write Job c28a98d6-b4f0-49d8-b799-e1135a711753.
23/11/29 03:00:55 INFO FileFormatWriter: Write Job c28a98d6-b4f0-49d8-b799-e1135a711753 committed. Elapsed time: 28 ms.
23/11/29 03:00:55 INFO FileFormatWriter: Finished processing stats for write job c28a98d6-b4f0-49d8-b799-e1135a711753.
23/11/29 03:00:55 INFO SparkUI: Stopped Spark web UI at http://b35b76aa1a2d:4040
23/11/29 03:00:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:00:55 INFO MemoryStore: MemoryStore cleared
23/11/29 03:00:55 INFO BlockManager: BlockManager stopped
23/11/29 03:00:55 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:00:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:00:55 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 69
23/11/29 03:00:55 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:00:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-9b6b5a3d-90c6-46a4-aa02-643d0db0cb25
23/11/29 03:00:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9bce9b4-b10d-4c64-85be-9cc1edb1c48f
Execution 2:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-95c290e4-c5f0-4ab6-a977-310e33ef00a9;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (510ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2327ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (400ms)
:: resolution report :: resolve 7116ms :: artifacts dl 3255ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-95c290e4-c5f0-4ab6-a977-310e33ef00a9
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 03:01:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:01:09 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:01:09 INFO ResourceUtils: ==============================================================
23/11/29 03:01:09 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:01:09 INFO ResourceUtils: ==============================================================
23/11/29 03:01:09 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:01:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:01:09 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:01:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:01:10 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:01:10 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:01:10 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:01:10 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:01:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:01:10 INFO Utils: Successfully started service 'sparkDriver' on port 38121.
23/11/29 03:01:10 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:01:10 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:01:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:01:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:01:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:01:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e926bdf-9ad0-49b4-b765-a1af34fa05f2
23/11/29 03:01:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:01:10 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:01:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:01:10 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://2c80db00fc4f:38121/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226869839
23/11/29 03:01:10 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://2c80db00fc4f:38121/jars/org.tukaani_xz-1.9.jar with timestamp 1701226869839
23/11/29 03:01:10 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://2c80db00fc4f:38121/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226869839
23/11/29 03:01:10 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://2c80db00fc4f:38121/jars/spark.jar with timestamp 1701226869839
23/11/29 03:01:10 INFO Executor: Starting executor ID driver on host 2c80db00fc4f
23/11/29 03:01:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:01:10 INFO Executor: Fetching spark://2c80db00fc4f:38121/jars/org.tukaani_xz-1.9.jar with timestamp 1701226869839
23/11/29 03:01:10 INFO TransportClientFactory: Successfully created connection to 2c80db00fc4f/172.17.0.2:38121 after 41 ms (0 ms spent in bootstraps)
23/11/29 03:01:11 INFO Utils: Fetching spark://2c80db00fc4f:38121/jars/org.tukaani_xz-1.9.jar to /tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/fetchFileTemp988076675417318246.tmp
23/11/29 03:01:11 INFO Executor: Adding file:/tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:01:11 INFO Executor: Fetching spark://2c80db00fc4f:38121/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226869839
23/11/29 03:01:11 INFO Utils: Fetching spark://2c80db00fc4f:38121/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/fetchFileTemp9977493478443748780.tmp
23/11/29 03:01:11 INFO Executor: Adding file:/tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:01:11 INFO Executor: Fetching spark://2c80db00fc4f:38121/jars/spark.jar with timestamp 1701226869839
23/11/29 03:01:11 INFO Utils: Fetching spark://2c80db00fc4f:38121/jars/spark.jar to /tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/fetchFileTemp5451773689048346913.tmp
23/11/29 03:01:11 INFO Executor: Adding file:/tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/spark.jar to class loader
23/11/29 03:01:11 INFO Executor: Fetching spark://2c80db00fc4f:38121/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226869839
23/11/29 03:01:11 INFO Utils: Fetching spark://2c80db00fc4f:38121/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/fetchFileTemp6591007978919096698.tmp
23/11/29 03:01:11 INFO Executor: Adding file:/tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0/userFiles-04d2955a-479d-4a10-91c8-4f05e6f1835b/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:01:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42619.
23/11/29 03:01:11 INFO NettyBlockTransferService: Server created on 2c80db00fc4f:42619
23/11/29 03:01:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:01:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2c80db00fc4f, 42619, None)
23/11/29 03:01:11 INFO BlockManagerMasterEndpoint: Registering block manager 2c80db00fc4f:42619 with 434.4 MiB RAM, BlockManagerId(driver, 2c80db00fc4f, 42619, None)
23/11/29 03:01:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2c80db00fc4f, 42619, None)
23/11/29 03:01:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2c80db00fc4f, 42619, None)
23/11/29 03:01:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:01:11 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:01:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:01:12 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
23/11/29 03:01:15 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:01:15 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:01:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:01:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:01:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 03:01:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2c80db00fc4f:42619 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 03:01:15 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:01:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:01:15 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:01:15 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:01:15 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:01:15 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:01:15 INFO DAGScheduler: Missing parents: List()
23/11/29 03:01:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:01:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:01:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:01:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2c80db00fc4f:42619 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:01:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:01:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:01:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:01:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2c80db00fc4f, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:01:16 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:01:16 INFO CodeGenerator: Code generated in 168.123986 ms
23/11/29 03:01:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:01:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 443 ms on 2c80db00fc4f (executor driver) (1/1)
23/11/29 03:01:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:01:16 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.566 s
23/11/29 03:01:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:01:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:01:16 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.611779 s
23/11/29 03:01:16 INFO CodeGenerator: Code generated in 14.530606 ms
23/11/29 03:01:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:01:16 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:01:16 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:01:16 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:01:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:01:16 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:01:16 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:01:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:01:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:01:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2c80db00fc4f:42619 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:01:16 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:01:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:01:16 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:01:16 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:01:16 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:01:16 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:01:16 INFO DAGScheduler: Missing parents: List()
23/11/29 03:01:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:01:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:01:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:01:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2c80db00fc4f:42619 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:01:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:01:16 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:01:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:01:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2c80db00fc4f, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (2c80db00fc4f, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (2c80db00fc4f, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (2c80db00fc4f, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (2c80db00fc4f, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (2c80db00fc4f, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:16 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (2c80db00fc4f, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:17 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (2c80db00fc4f, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:01:17 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:01:17 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:01:17 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:01:17 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:01:17 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:01:17 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:01:17 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2c80db00fc4f:42619 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 03:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:01:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2c80db00fc4f:42619 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:01:17 INFO CodeGenerator: Code generated in 107.708307 ms
23/11/29 03:01:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:01:41 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000003
23/11/29 03:01:41 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000006
23/11/29 03:01:41 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000003_4: Committed. Elapsed time: 2 ms.
23/11/29 03:01:41 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000006_7: Committed. Elapsed time: 2 ms.
23/11/29 03:01:41 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 03:01:41 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2541 bytes result sent to driver
23/11/29 03:01:41 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (2c80db00fc4f, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:41 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:01:41 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (2c80db00fc4f, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:41 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:01:41 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 24986 ms on 2c80db00fc4f (executor driver) (1/23)
23/11/29 03:01:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 25016 ms on 2c80db00fc4f (executor driver) (2/23)
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:01:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000001
23/11/29 03:01:42 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 03:01:42 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:01:42 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (2c80db00fc4f, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 25432 ms on 2c80db00fc4f (executor driver) (3/23)
23/11/29 03:01:42 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:01:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000000
23/11/29 03:01:42 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 03:01:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:01:42 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (2c80db00fc4f, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:42 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:01:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 25719 ms on 2c80db00fc4f (executor driver) (4/23)
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:01:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000007
23/11/29 03:01:42 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:01:42 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:01:42 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (2c80db00fc4f, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:42 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 25815 ms on 2c80db00fc4f (executor driver) (5/23)
23/11/29 03:01:42 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:01:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000005
23/11/29 03:01:42 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 03:01:42 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:01:42 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (2c80db00fc4f, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:42 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 25848 ms on 2c80db00fc4f (executor driver) (6/23)
23/11/29 03:01:42 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:01:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:01:43 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000004
23/11/29 03:01:43 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000004_5: Committed. Elapsed time: 6 ms.
23/11/29 03:01:43 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:01:43 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (2c80db00fc4f, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:43 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:01:43 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 26031 ms on 2c80db00fc4f (executor driver) (7/23)
23/11/29 03:01:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:01:43 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000002
23/11/29 03:01:43 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 03:01:43 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:01:43 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (2c80db00fc4f, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:01:43 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 26289 ms on 2c80db00fc4f (executor driver) (8/23)
23/11/29 03:01:43 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:01:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:01:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:01:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:01:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:02:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000012
23/11/29 03:02:02 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:02:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000010
23/11/29 03:02:02 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000010_11: Committed. Elapsed time: 1 ms.
23/11/29 03:02:02 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:02:02 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:02:02 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (2c80db00fc4f, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:02 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (2c80db00fc4f, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:02 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:02:02 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:02:02 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 19732 ms on 2c80db00fc4f (executor driver) (9/23)
23/11/29 03:02:02 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 19348 ms on 2c80db00fc4f (executor driver) (10/23)
23/11/29 03:02:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000015
23/11/29 03:02:02 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000015_16: Committed. Elapsed time: 1 ms.
23/11/29 03:02:02 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:02:02 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (2c80db00fc4f, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:02 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 18883 ms on 2c80db00fc4f (executor driver) (11/23)
23/11/29 03:02:02 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:02:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:02 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:02:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:02 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:02:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:02 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:02:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000013
23/11/29 03:02:02 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000013_14: Committed. Elapsed time: 1 ms.
23/11/29 03:02:02 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:02:02 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (2c80db00fc4f, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:02 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:02:02 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 19609 ms on 2c80db00fc4f (executor driver) (12/23)
23/11/29 03:02:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:02 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:02:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000011
23/11/29 03:02:02 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000011_12: Committed. Elapsed time: 2 ms.
23/11/29 03:02:02 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:02:02 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (2c80db00fc4f, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:02 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 19953 ms on 2c80db00fc4f (executor driver) (13/23)
23/11/29 03:02:02 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:02:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:02 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:02:03 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000014
23/11/29 03:02:03 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000014_15: Committed. Elapsed time: 1 ms.
23/11/29 03:02:03 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:02:03 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (2c80db00fc4f, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:03 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:02:03 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 19987 ms on 2c80db00fc4f (executor driver) (14/23)
23/11/29 03:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:03 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:02:03 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000008
23/11/29 03:02:03 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000008_9: Committed. Elapsed time: 3 ms.
23/11/29 03:02:03 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:02:03 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (2c80db00fc4f, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:03 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:02:03 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 21375 ms on 2c80db00fc4f (executor driver) (15/23)
23/11/29 03:02:03 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:03 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:03 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:03 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:02:03 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000009
23/11/29 03:02:03 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000009_10: Committed. Elapsed time: 1 ms.
23/11/29 03:02:03 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:02:03 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 21566 ms on 2c80db00fc4f (executor driver) (16/23)
23/11/29 03:02:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000022
23/11/29 03:02:16 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:02:16 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:02:16 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 13557 ms on 2c80db00fc4f (executor driver) (17/23)
23/11/29 03:02:17 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000017
23/11/29 03:02:17 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:02:17 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:02:17 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 15591 ms on 2c80db00fc4f (executor driver) (18/23)
23/11/29 03:02:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000016
23/11/29 03:02:18 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:02:18 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:02:18 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 15969 ms on 2c80db00fc4f (executor driver) (19/23)
23/11/29 03:02:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000018
23/11/29 03:02:18 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:02:18 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:02:18 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 16043 ms on 2c80db00fc4f (executor driver) (20/23)
23/11/29 03:02:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000019
23/11/29 03:02:18 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:02:18 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:02:18 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 16248 ms on 2c80db00fc4f (executor driver) (21/23)
23/11/29 03:02:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000020
23/11/29 03:02:18 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000020_21: Committed. Elapsed time: 0 ms.
23/11/29 03:02:18 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:02:18 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 16114 ms on 2c80db00fc4f (executor driver) (22/23)
23/11/29 03:02:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290301167176220220851393717_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290301167176220220851393717_0001_m_000021
23/11/29 03:02:19 INFO SparkHadoopMapRedUtil: attempt_202311290301167176220220851393717_0001_m_000021_22: Committed. Elapsed time: 0 ms.
23/11/29 03:02:19 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:02:19 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 16179 ms on 2c80db00fc4f (executor driver) (23/23)
23/11/29 03:02:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:02:19 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 62.247 s
23/11/29 03:02:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:02:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:02:19 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 62.253945 s
23/11/29 03:02:19 INFO FileFormatWriter: Start to commit write Job e37a1fa8-9b99-4bb0-afb9-ce53893f6bb7.
23/11/29 03:02:19 INFO FileFormatWriter: Write Job e37a1fa8-9b99-4bb0-afb9-ce53893f6bb7 committed. Elapsed time: 26 ms.
23/11/29 03:02:19 INFO FileFormatWriter: Finished processing stats for write job e37a1fa8-9b99-4bb0-afb9-ce53893f6bb7.
23/11/29 03:02:19 INFO SparkUI: Stopped Spark web UI at http://2c80db00fc4f:4040
23/11/29 03:02:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:02:19 INFO MemoryStore: MemoryStore cleared
23/11/29 03:02:19 INFO BlockManager: BlockManager stopped
23/11/29 03:02:19 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:02:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:02:19 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 66
23/11/29 03:02:19 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:02:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-9324c742-d7b6-4724-925e-c02efd3186a7
23/11/29 03:02:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-4ad4f6a1-ed91-402f-999a-575140f9b8c0
Execution 3:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-81daeae1-bb39-471b-ad09-a7ba6bf202a3;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (732ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (604ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (344ms)
:: resolution report :: resolve 7891ms :: artifacts dl 1699ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-81daeae1-bb39-471b-ad09-a7ba6bf202a3
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 03:02:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:02:32 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:02:32 INFO ResourceUtils: ==============================================================
23/11/29 03:02:32 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:02:32 INFO ResourceUtils: ==============================================================
23/11/29 03:02:32 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:02:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:02:32 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:02:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:02:32 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:02:32 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:02:32 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:02:32 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:02:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:02:33 INFO Utils: Successfully started service 'sparkDriver' on port 44703.
23/11/29 03:02:33 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:02:33 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:02:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:02:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:02:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:02:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-acbc01cc-9b30-4a80-ae9e-e4fb58a8949e
23/11/29 03:02:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:02:33 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:02:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:02:33 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://e75151288b89:44703/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://e75151288b89:44703/jars/org.tukaani_xz-1.9.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://e75151288b89:44703/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://e75151288b89:44703/jars/spark.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO Executor: Starting executor ID driver on host e75151288b89
23/11/29 03:02:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:02:33 INFO Executor: Fetching spark://e75151288b89:44703/jars/org.tukaani_xz-1.9.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO TransportClientFactory: Successfully created connection to e75151288b89/172.17.0.2:44703 after 26 ms (0 ms spent in bootstraps)
23/11/29 03:02:33 INFO Utils: Fetching spark://e75151288b89:44703/jars/org.tukaani_xz-1.9.jar to /tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/fetchFileTemp8958066570184005160.tmp
23/11/29 03:02:33 INFO Executor: Adding file:/tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:02:33 INFO Executor: Fetching spark://e75151288b89:44703/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO Utils: Fetching spark://e75151288b89:44703/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/fetchFileTemp9245549742020325672.tmp
23/11/29 03:02:33 INFO Executor: Adding file:/tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:02:33 INFO Executor: Fetching spark://e75151288b89:44703/jars/spark.jar with timestamp 1701226952812
23/11/29 03:02:33 INFO Utils: Fetching spark://e75151288b89:44703/jars/spark.jar to /tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/fetchFileTemp12341591626965588070.tmp
23/11/29 03:02:34 INFO Executor: Adding file:/tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/spark.jar to class loader
23/11/29 03:02:34 INFO Executor: Fetching spark://e75151288b89:44703/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226952812
23/11/29 03:02:34 INFO Utils: Fetching spark://e75151288b89:44703/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/fetchFileTemp7622667104484201314.tmp
23/11/29 03:02:34 INFO Executor: Adding file:/tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509/userFiles-1f650d5b-246e-4b7b-9f7b-e0c6d33b2443/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:02:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39003.
23/11/29 03:02:34 INFO NettyBlockTransferService: Server created on e75151288b89:39003
23/11/29 03:02:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:02:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, e75151288b89, 39003, None)
23/11/29 03:02:34 INFO BlockManagerMasterEndpoint: Registering block manager e75151288b89:39003 with 434.4 MiB RAM, BlockManagerId(driver, e75151288b89, 39003, None)
23/11/29 03:02:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, e75151288b89, 39003, None)
23/11/29 03:02:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, e75151288b89, 39003, None)
23/11/29 03:02:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:02:34 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:02:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:02:35 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.
23/11/29 03:02:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:02:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:02:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:02:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:02:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:02:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on e75151288b89:39003 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:02:38 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:02:38 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:02:38 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:02:38 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:02:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:02:38 INFO DAGScheduler: Missing parents: List()
23/11/29 03:02:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:02:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:02:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:02:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on e75151288b89:39003 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:02:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:02:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:02:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:02:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (e75151288b89, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:02:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:02:39 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:02:39 INFO CodeGenerator: Code generated in 145.240349 ms
23/11/29 03:02:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:02:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 422 ms on e75151288b89 (executor driver) (1/1)
23/11/29 03:02:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:02:39 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.530 s
23/11/29 03:02:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:02:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:02:39 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.573082 s
23/11/29 03:02:39 INFO CodeGenerator: Code generated in 14.049503 ms
23/11/29 03:02:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:02:39 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:02:39 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:02:39 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:02:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:02:39 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:02:39 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:02:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:02:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:02:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on e75151288b89:39003 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:02:39 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:02:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:02:39 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:02:39 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:02:39 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:02:39 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:02:39 INFO DAGScheduler: Missing parents: List()
23/11/29 03:02:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:02:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:02:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:02:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on e75151288b89:39003 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:02:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:02:39 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:02:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:02:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (e75151288b89, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (e75151288b89, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (e75151288b89, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (e75151288b89, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (e75151288b89, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (e75151288b89, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (e75151288b89, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (e75151288b89, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:02:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:02:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:02:39 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:02:39 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:02:39 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:02:39 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:02:39 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:02:39 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:02:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on e75151288b89:39003 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:02:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on e75151288b89:39003 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:02:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:02:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:02:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:02:40 INFO CodeGenerator: Code generated in 108.448142 ms
23/11/29 03:02:40 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:03:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000006
23/11/29 03:03:00 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000006_7: Committed. Elapsed time: 2 ms.
23/11/29 03:03:00 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 03:03:00 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (e75151288b89, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:00 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:03:00 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 20477 ms on e75151288b89 (executor driver) (1/23)
23/11/29 03:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:03:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000003
23/11/29 03:03:00 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000003_4: Committed. Elapsed time: 2 ms.
23/11/29 03:03:00 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:03:00 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (e75151288b89, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:00 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 20598 ms on e75151288b89 (executor driver) (2/23)
23/11/29 03:03:00 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:03:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000004
23/11/29 03:03:00 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 03:03:00 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:03:00 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (e75151288b89, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:00 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:03:00 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 20983 ms on e75151288b89 (executor driver) (3/23)
23/11/29 03:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:03:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000002
23/11/29 03:03:00 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000002_3: Committed. Elapsed time: 7 ms.
23/11/29 03:03:00 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:03:00 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (e75151288b89, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:00 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 21056 ms on e75151288b89 (executor driver) (4/23)
23/11/29 03:03:00 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:03:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:03:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000007
23/11/29 03:03:00 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:03:01 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:03:01 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (e75151288b89, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:01 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:03:01 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 21131 ms on e75151288b89 (executor driver) (5/23)
23/11/29 03:03:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:03:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000001
23/11/29 03:03:01 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 03:03:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000005
23/11/29 03:03:01 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 03:03:01 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:03:01 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:03:01 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (e75151288b89, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:01 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:03:01 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 21267 ms on e75151288b89 (executor driver) (6/23)
23/11/29 03:03:01 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (e75151288b89, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:01 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:03:01 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 21277 ms on e75151288b89 (executor driver) (7/23)
23/11/29 03:03:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:03:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:03:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000000
23/11/29 03:03:01 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 03:03:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:03:01 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (e75151288b89, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21437 ms on e75151288b89 (executor driver) (8/23)
23/11/29 03:03:01 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:03:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:03:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000012
23/11/29 03:03:22 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000012_13: Committed. Elapsed time: 23 ms.
23/11/29 03:03:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000011
23/11/29 03:03:22 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000011_12: Committed. Elapsed time: 1 ms.
23/11/29 03:03:22 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:03:22 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (e75151288b89, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:22 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 21444 ms on e75151288b89 (executor driver) (9/23)
23/11/29 03:03:22 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:03:22 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:03:22 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (e75151288b89, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:22 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 21531 ms on e75151288b89 (executor driver) (10/23)
23/11/29 03:03:22 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:03:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:22 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:03:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:22 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:03:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000008
23/11/29 03:03:23 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000008_9: Committed. Elapsed time: 2 ms.
23/11/29 03:03:23 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:03:23 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (e75151288b89, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:23 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 22884 ms on e75151288b89 (executor driver) (11/23)
23/11/29 03:03:23 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:03:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000013
23/11/29 03:03:23 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000013_14: Committed. Elapsed time: 1 ms.
23/11/29 03:03:23 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:03:23 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (e75151288b89, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:23 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:03:23 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 22150 ms on e75151288b89 (executor driver) (12/23)
23/11/29 03:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:23 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:23 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:03:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000010
23/11/29 03:03:23 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000010_11: Committed. Elapsed time: 3 ms.
23/11/29 03:03:23 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:03:23 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (e75151288b89, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:23 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 22860 ms on e75151288b89 (executor driver) (13/23)
23/11/29 03:03:23 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:03:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:23 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:03:24 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000014
23/11/29 03:03:24 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000014_15: Committed. Elapsed time: 2 ms.
23/11/29 03:03:24 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:03:24 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (e75151288b89, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:24 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 23674 ms on e75151288b89 (executor driver) (14/23)
23/11/29 03:03:24 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:03:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:03:25 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000015
23/11/29 03:03:25 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000015_16: Committed. Elapsed time: 18 ms.
23/11/29 03:03:25 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:03:25 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (e75151288b89, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:03:25 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:03:25 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 23737 ms on e75151288b89 (executor driver) (15/23)
23/11/29 03:03:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:03:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:03:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:03:25 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:03:25 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000009
23/11/29 03:03:25 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000009_10: Committed. Elapsed time: 2 ms.
23/11/29 03:03:25 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:03:25 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 24982 ms on e75151288b89 (executor driver) (16/23)
23/11/29 03:03:38 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000022
23/11/29 03:03:38 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000022_23: Committed. Elapsed time: 1 ms.
23/11/29 03:03:38 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:03:38 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 13538 ms on e75151288b89 (executor driver) (17/23)
23/11/29 03:03:38 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000016
23/11/29 03:03:38 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000016_17: Committed. Elapsed time: 1 ms.
23/11/29 03:03:38 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:03:38 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 16291 ms on e75151288b89 (executor driver) (18/23)
23/11/29 03:03:38 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000017
23/11/29 03:03:38 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000017_18: Committed. Elapsed time: 1 ms.
23/11/29 03:03:38 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:03:38 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 16411 ms on e75151288b89 (executor driver) (19/23)
23/11/29 03:03:39 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000018
23/11/29 03:03:39 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:03:39 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:03:39 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 16214 ms on e75151288b89 (executor driver) (20/23)
23/11/29 03:03:39 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000019
23/11/29 03:03:39 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:03:39 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:03:39 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 16612 ms on e75151288b89 (executor driver) (21/23)
23/11/29 03:03:40 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000020
23/11/29 03:03:40 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000020_21: Committed. Elapsed time: 0 ms.
23/11/29 03:03:40 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:03:40 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 16293 ms on e75151288b89 (executor driver) (22/23)
23/11/29 03:03:40 INFO FileOutputCommitter: Saved output of task 'attempt_202311290302391446054057445746827_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290302391446054057445746827_0001_m_000021
23/11/29 03:03:40 INFO SparkHadoopMapRedUtil: attempt_202311290302391446054057445746827_0001_m_000021_22: Committed. Elapsed time: 0 ms.
23/11/29 03:03:40 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:03:40 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 15853 ms on e75151288b89 (executor driver) (23/23)
23/11/29 03:03:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:03:40 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 60.866 s
23/11/29 03:03:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:03:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:03:40 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 60.873713 s
23/11/29 03:03:40 INFO FileFormatWriter: Start to commit write Job a9331692-0070-4575-a4ad-30d2eb007c7c.
23/11/29 03:03:40 INFO FileFormatWriter: Write Job a9331692-0070-4575-a4ad-30d2eb007c7c committed. Elapsed time: 31 ms.
23/11/29 03:03:40 INFO FileFormatWriter: Finished processing stats for write job a9331692-0070-4575-a4ad-30d2eb007c7c.
23/11/29 03:03:40 INFO SparkUI: Stopped Spark web UI at http://e75151288b89:4040
23/11/29 03:03:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:03:40 INFO MemoryStore: MemoryStore cleared
23/11/29 03:03:40 INFO BlockManager: BlockManager stopped
23/11/29 03:03:40 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:03:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:03:40 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 65
23/11/29 03:03:40 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:03:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-51edfcf0-1b85-4a19-b087-6f356c66f509
23/11/29 03:03:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-a78edc89-af63-44a2-86b2-1e941a1a1b43
Execution 4:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-0f95f441-1db0-4aa8-824a-3a414783a5fb;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (566ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (372ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (395ms)
:: resolution report :: resolve 6374ms :: artifacts dl 1348ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-0f95f441-1db0-4aa8-824a-3a414783a5fb
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/17ms)
23/11/29 03:03:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:03:53 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:03:53 INFO ResourceUtils: ==============================================================
23/11/29 03:03:53 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:03:53 INFO ResourceUtils: ==============================================================
23/11/29 03:03:53 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:03:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:03:53 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:03:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:03:53 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:03:53 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:03:53 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:03:53 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:03:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:03:53 INFO Utils: Successfully started service 'sparkDriver' on port 44247.
23/11/29 03:03:53 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:03:53 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:03:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:03:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:03:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:03:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8e9dabac-2b24-43df-b629-4d679a2119ad
23/11/29 03:03:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:03:53 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:03:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:03:54 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://00c7599bf43e:44247/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://00c7599bf43e:44247/jars/org.tukaani_xz-1.9.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://00c7599bf43e:44247/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://00c7599bf43e:44247/jars/spark.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO Executor: Starting executor ID driver on host 00c7599bf43e
23/11/29 03:03:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:03:54 INFO Executor: Fetching spark://00c7599bf43e:44247/jars/spark.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO TransportClientFactory: Successfully created connection to 00c7599bf43e/172.17.0.2:44247 after 28 ms (0 ms spent in bootstraps)
23/11/29 03:03:54 INFO Utils: Fetching spark://00c7599bf43e:44247/jars/spark.jar to /tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/fetchFileTemp10978905605058247911.tmp
23/11/29 03:03:54 INFO Executor: Adding file:/tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/spark.jar to class loader
23/11/29 03:03:54 INFO Executor: Fetching spark://00c7599bf43e:44247/jars/org.tukaani_xz-1.9.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO Utils: Fetching spark://00c7599bf43e:44247/jars/org.tukaani_xz-1.9.jar to /tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/fetchFileTemp8619019769927556525.tmp
23/11/29 03:03:54 INFO Executor: Adding file:/tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:03:54 INFO Executor: Fetching spark://00c7599bf43e:44247/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO Utils: Fetching spark://00c7599bf43e:44247/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/fetchFileTemp13711806711370825480.tmp
23/11/29 03:03:54 INFO Executor: Adding file:/tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:03:54 INFO Executor: Fetching spark://00c7599bf43e:44247/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227033064
23/11/29 03:03:54 INFO Utils: Fetching spark://00c7599bf43e:44247/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/fetchFileTemp4674196242514549148.tmp
23/11/29 03:03:54 INFO Executor: Adding file:/tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4/userFiles-39cd382f-46bb-4a29-9c83-f1c7f07c9e7d/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:03:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44265.
23/11/29 03:03:54 INFO NettyBlockTransferService: Server created on 00c7599bf43e:44265
23/11/29 03:03:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:03:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 00c7599bf43e, 44265, None)
23/11/29 03:03:54 INFO BlockManagerMasterEndpoint: Registering block manager 00c7599bf43e:44265 with 434.4 MiB RAM, BlockManagerId(driver, 00c7599bf43e, 44265, None)
23/11/29 03:03:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 00c7599bf43e, 44265, None)
23/11/29 03:03:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 00c7599bf43e, 44265, None)
23/11/29 03:03:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:03:55 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:03:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:03:56 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.
23/11/29 03:03:58 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:03:58 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:03:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:03:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:03:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:03:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 00c7599bf43e:44265 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:03:59 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:03:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:03:59 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:03:59 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:03:59 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:03:59 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:03:59 INFO DAGScheduler: Missing parents: List()
23/11/29 03:03:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:03:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:03:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:03:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 00c7599bf43e:44265 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:03:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:03:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:03:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:03:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (00c7599bf43e, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:03:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:03:59 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:03:59 INFO CodeGenerator: Code generated in 190.564786 ms
23/11/29 03:04:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:04:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 584 ms on 00c7599bf43e (executor driver) (1/1)
23/11/29 03:04:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:04:00 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.704 s
23/11/29 03:04:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:04:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:04:00 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.749511 s
23/11/29 03:04:00 INFO CodeGenerator: Code generated in 17.995933 ms
23/11/29 03:04:00 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
23/11/29 03:04:00 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:04:00 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:04:00 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:04:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:04:00 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:04:00 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:04:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:04:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 00c7599bf43e:44265 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:04:00 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:04:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:04:00 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:04:00 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:04:00 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:04:00 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:04:00 INFO DAGScheduler: Missing parents: List()
23/11/29 03:04:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:04:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:04:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:04:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 00c7599bf43e:44265 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:04:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:04:00 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:04:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:04:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (00c7599bf43e, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (00c7599bf43e, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (00c7599bf43e, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (00c7599bf43e, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (00c7599bf43e, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (00c7599bf43e, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (00c7599bf43e, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (00c7599bf43e, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:04:00 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:04:00 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:04:00 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:04:00 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:04:00 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:04:00 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:04:00 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:04:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 00c7599bf43e:44265 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:04:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 00c7599bf43e:44265 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:04:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:04:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:04:01 INFO CodeGenerator: Code generated in 156.778857 ms
23/11/29 03:04:24 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000002
23/11/29 03:04:24 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000002_3: Committed. Elapsed time: 23 ms.
23/11/29 03:04:24 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2541 bytes result sent to driver
23/11/29 03:04:24 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (00c7599bf43e, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:24 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:04:24 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 24202 ms on 00c7599bf43e (executor driver) (1/23)
23/11/29 03:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:25 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:04:25 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000003
23/11/29 03:04:25 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000003_4: Committed. Elapsed time: 10 ms.
23/11/29 03:04:25 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:04:25 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (00c7599bf43e, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:25 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:04:25 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 24703 ms on 00c7599bf43e (executor driver) (2/23)
23/11/29 03:04:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:25 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:04:25 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000004
23/11/29 03:04:25 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000004_5: Committed. Elapsed time: 3 ms.
23/11/29 03:04:25 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:04:25 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (00c7599bf43e, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:25 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 25141 ms on 00c7599bf43e (executor driver) (3/23)
23/11/29 03:04:25 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:04:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000006
23/11/29 03:04:26 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 03:04:26 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:04:26 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (00c7599bf43e, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:26 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:04:26 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 25710 ms on 00c7599bf43e (executor driver) (4/23)
23/11/29 03:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:04:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000000
23/11/29 03:04:26 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 03:04:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:04:26 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (00c7599bf43e, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:26 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:04:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 25833 ms on 00c7599bf43e (executor driver) (5/23)
23/11/29 03:04:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:04:27 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000005
23/11/29 03:04:27 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 03:04:27 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:04:27 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (00c7599bf43e, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:27 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 26487 ms on 00c7599bf43e (executor driver) (6/23)
23/11/29 03:04:27 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:04:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:27 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:04:27 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000007
23/11/29 03:04:27 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:04:27 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:04:27 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (00c7599bf43e, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:27 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:04:27 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 26876 ms on 00c7599bf43e (executor driver) (7/23)
23/11/29 03:04:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:27 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:04:27 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000001
23/11/29 03:04:27 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000001_2: Committed. Elapsed time: 12 ms.
23/11/29 03:04:27 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:04:27 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (00c7599bf43e, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:27 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:04:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 27062 ms on 00c7599bf43e (executor driver) (8/23)
23/11/29 03:04:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:27 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:04:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000012
23/11/29 03:04:46 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:04:46 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:04:46 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (00c7599bf43e, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:46 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:04:46 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 20083 ms on 00c7599bf43e (executor driver) (9/23)
23/11/29 03:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:04:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000010
23/11/29 03:04:46 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000010_11: Committed. Elapsed time: 0 ms.
23/11/29 03:04:46 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:04:46 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (00c7599bf43e, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:46 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 20831 ms on 00c7599bf43e (executor driver) (10/23)
23/11/29 03:04:46 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:04:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000008
23/11/29 03:04:46 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000008_9: Committed. Elapsed time: 1 ms.
23/11/29 03:04:46 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:04:46 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (00c7599bf43e, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:46 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:04:46 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 21883 ms on 00c7599bf43e (executor driver) (11/23)
23/11/29 03:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:04:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000011
23/11/29 03:04:46 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000011_12: Committed. Elapsed time: 1 ms.
23/11/29 03:04:46 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:04:46 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (00c7599bf43e, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:46 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 20485 ms on 00c7599bf43e (executor driver) (12/23)
23/11/29 03:04:46 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:04:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:04:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000013
23/11/29 03:04:47 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000013_14: Committed. Elapsed time: 1 ms.
23/11/29 03:04:47 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:04:47 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (00c7599bf43e, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:47 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 19954 ms on 00c7599bf43e (executor driver) (13/23)
23/11/29 03:04:47 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:47 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:04:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000014
23/11/29 03:04:47 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000014_15: Committed. Elapsed time: 1 ms.
23/11/29 03:04:47 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:04:47 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (00c7599bf43e, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:47 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 20067 ms on 00c7599bf43e (executor driver) (14/23)
23/11/29 03:04:47 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:47 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:04:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000009
23/11/29 03:04:47 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000009_10: Committed. Elapsed time: 0 ms.
23/11/29 03:04:47 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:04:47 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (00c7599bf43e, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:04:47 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 22453 ms on 00c7599bf43e (executor driver) (15/23)
23/11/29 03:04:47 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:04:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000015
23/11/29 03:04:47 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000015_16: Committed. Elapsed time: 0 ms.
23/11/29 03:04:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:04:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:04:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:04:47 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:04:47 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:04:47 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 20137 ms on 00c7599bf43e (executor driver) (16/23)
23/11/29 03:05:03 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000022
23/11/29 03:05:03 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:05:03 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:05:03 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 15951 ms on 00c7599bf43e (executor driver) (17/23)
23/11/29 03:05:04 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000017
23/11/29 03:05:04 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:05:04 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:05:04 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 17866 ms on 00c7599bf43e (executor driver) (18/23)
23/11/29 03:05:04 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000016
23/11/29 03:05:04 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:05:04 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:05:04 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 18252 ms on 00c7599bf43e (executor driver) (19/23)
23/11/29 03:05:05 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000018
23/11/29 03:05:05 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:05:05 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000019
23/11/29 03:05:05 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:05:05 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:05:05 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 18435 ms on 00c7599bf43e (executor driver) (20/23)
23/11/29 03:05:05 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:05:05 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 18315 ms on 00c7599bf43e (executor driver) (21/23)
23/11/29 03:05:05 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000020
23/11/29 03:05:05 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000020_21: Committed. Elapsed time: 1 ms.
23/11/29 03:05:05 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:05:05 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 18439 ms on 00c7599bf43e (executor driver) (22/23)
23/11/29 03:05:06 INFO FileOutputCommitter: Saved output of task 'attempt_202311290304007108074795404279802_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290304007108074795404279802_0001_m_000021
23/11/29 03:05:06 INFO SparkHadoopMapRedUtil: attempt_202311290304007108074795404279802_0001_m_000021_22: Committed. Elapsed time: 0 ms.
23/11/29 03:05:06 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:05:06 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 18509 ms on 00c7599bf43e (executor driver) (23/23)
23/11/29 03:05:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:05:06 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 65.546 s
23/11/29 03:05:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:05:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:05:06 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 65.557208 s
23/11/29 03:05:06 INFO FileFormatWriter: Start to commit write Job 7206a73d-d7ff-4601-bf4b-028216152f85.
23/11/29 03:05:06 INFO FileFormatWriter: Write Job 7206a73d-d7ff-4601-bf4b-028216152f85 committed. Elapsed time: 31 ms.
23/11/29 03:05:06 INFO FileFormatWriter: Finished processing stats for write job 7206a73d-d7ff-4601-bf4b-028216152f85.
23/11/29 03:05:06 INFO SparkUI: Stopped Spark web UI at http://00c7599bf43e:4040
23/11/29 03:05:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:05:06 INFO MemoryStore: MemoryStore cleared
23/11/29 03:05:06 INFO BlockManager: BlockManager stopped
23/11/29 03:05:06 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:05:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:05:06 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 70
23/11/29 03:05:06 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:05:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-53589c42-7f23-4ad4-a696-fc7a3c1601bf
23/11/29 03:05:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff3d20d5-c019-4633-a93b-3278b5556fc4
Execution 5:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-97f614b6-d625-41ef-9f7d-e157a88e4a71;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (463ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (425ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (318ms)
:: resolution report :: resolve 7164ms :: artifacts dl 1222ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-97f614b6-d625-41ef-9f7d-e157a88e4a71
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/12ms)
23/11/29 03:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:05:19 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:05:19 INFO ResourceUtils: ==============================================================
23/11/29 03:05:19 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:05:19 INFO ResourceUtils: ==============================================================
23/11/29 03:05:19 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:05:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:05:19 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:05:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:05:19 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:05:19 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:05:19 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:05:19 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:05:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:05:20 INFO Utils: Successfully started service 'sparkDriver' on port 41813.
23/11/29 03:05:20 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:05:20 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:05:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:05:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:05:20 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:05:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-958c3c64-e282-43bc-b82b-66c55859e99d
23/11/29 03:05:20 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:05:20 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:05:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:05:20 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://2272e1c3a46f:41813/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227119451
23/11/29 03:05:20 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://2272e1c3a46f:41813/jars/org.tukaani_xz-1.9.jar with timestamp 1701227119451
23/11/29 03:05:20 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://2272e1c3a46f:41813/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227119451
23/11/29 03:05:20 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://2272e1c3a46f:41813/jars/spark.jar with timestamp 1701227119451
23/11/29 03:05:20 INFO Executor: Starting executor ID driver on host 2272e1c3a46f
23/11/29 03:05:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:05:20 INFO Executor: Fetching spark://2272e1c3a46f:41813/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227119451
23/11/29 03:05:20 INFO TransportClientFactory: Successfully created connection to 2272e1c3a46f/172.17.0.2:41813 after 33 ms (0 ms spent in bootstraps)
23/11/29 03:05:20 INFO Utils: Fetching spark://2272e1c3a46f:41813/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/fetchFileTemp12229506519845331331.tmp
23/11/29 03:05:21 INFO Executor: Adding file:/tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:05:21 INFO Executor: Fetching spark://2272e1c3a46f:41813/jars/org.tukaani_xz-1.9.jar with timestamp 1701227119451
23/11/29 03:05:21 INFO Utils: Fetching spark://2272e1c3a46f:41813/jars/org.tukaani_xz-1.9.jar to /tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/fetchFileTemp13105749656274726547.tmp
23/11/29 03:05:21 INFO Executor: Adding file:/tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:05:21 INFO Executor: Fetching spark://2272e1c3a46f:41813/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227119451
23/11/29 03:05:21 INFO Utils: Fetching spark://2272e1c3a46f:41813/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/fetchFileTemp2670742245589696367.tmp
23/11/29 03:05:21 INFO Executor: Adding file:/tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:05:21 INFO Executor: Fetching spark://2272e1c3a46f:41813/jars/spark.jar with timestamp 1701227119451
23/11/29 03:05:21 INFO Utils: Fetching spark://2272e1c3a46f:41813/jars/spark.jar to /tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/fetchFileTemp11683681764919502921.tmp
23/11/29 03:05:21 INFO Executor: Adding file:/tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b/userFiles-24f392df-aaf9-433f-97dc-8a2ae0eff6d2/spark.jar to class loader
23/11/29 03:05:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44163.
23/11/29 03:05:21 INFO NettyBlockTransferService: Server created on 2272e1c3a46f:44163
23/11/29 03:05:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:05:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2272e1c3a46f, 44163, None)
23/11/29 03:05:21 INFO BlockManagerMasterEndpoint: Registering block manager 2272e1c3a46f:44163 with 434.4 MiB RAM, BlockManagerId(driver, 2272e1c3a46f, 44163, None)
23/11/29 03:05:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2272e1c3a46f, 44163, None)
23/11/29 03:05:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2272e1c3a46f, 44163, None)
23/11/29 03:05:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:05:21 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:05:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:05:22 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
23/11/29 03:05:25 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:05:25 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:05:25 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:05:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:05:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:05:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2272e1c3a46f:44163 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:05:26 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:05:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:05:26 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:05:26 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:05:26 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:05:26 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:05:26 INFO DAGScheduler: Missing parents: List()
23/11/29 03:05:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:05:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:05:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:05:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2272e1c3a46f:44163 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:05:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:05:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:05:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:05:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2272e1c3a46f, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:05:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:05:26 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:05:27 INFO CodeGenerator: Code generated in 189.870668 ms
23/11/29 03:05:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 03:05:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 593 ms on 2272e1c3a46f (executor driver) (1/1)
23/11/29 03:05:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:05:27 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.706 s
23/11/29 03:05:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:05:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:05:27 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.752031 s
23/11/29 03:05:27 INFO CodeGenerator: Code generated in 21.478139 ms
23/11/29 03:05:27 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:05:27 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:05:27 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:05:27 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:05:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:05:27 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:05:27 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:05:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:27 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:05:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:05:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2272e1c3a46f:44163 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:05:27 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:05:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:05:27 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:05:27 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:05:27 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:05:27 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:05:27 INFO DAGScheduler: Missing parents: List()
23/11/29 03:05:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:05:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:05:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 03:05:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2272e1c3a46f:44163 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 03:05:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:05:27 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:05:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:05:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2272e1c3a46f, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (2272e1c3a46f, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (2272e1c3a46f, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (2272e1c3a46f, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (2272e1c3a46f, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (2272e1c3a46f, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (2272e1c3a46f, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (2272e1c3a46f, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:05:27 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:05:28 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:05:28 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:05:28 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:05:28 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:05:28 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:05:28 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:05:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2272e1c3a46f:44163 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:05:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2272e1c3a46f:44163 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:05:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:28 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:28 INFO CodeGenerator: Code generated in 109.543042 ms
23/11/29 03:05:28 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:05:51 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000005
23/11/29 03:05:51 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 03:05:51 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2541 bytes result sent to driver
23/11/29 03:05:51 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (2272e1c3a46f, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:51 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:05:51 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 23904 ms on 2272e1c3a46f (executor driver) (1/23)
23/11/29 03:05:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:51 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:05:52 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000001
23/11/29 03:05:52 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 03:05:52 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:05:52 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (2272e1c3a46f, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:52 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:05:52 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 24637 ms on 2272e1c3a46f (executor driver) (2/23)
23/11/29 03:05:52 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:52 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:52 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:52 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:05:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000006
23/11/29 03:05:53 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000006_7: Committed. Elapsed time: 2 ms.
23/11/29 03:05:53 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:05:53 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (2272e1c3a46f, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:53 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:05:53 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 25438 ms on 2272e1c3a46f (executor driver) (3/23)
23/11/29 03:05:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000004
23/11/29 03:05:53 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000004_5: Committed. Elapsed time: 2 ms.
23/11/29 03:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:53 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:53 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (2272e1c3a46f, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:05:53 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:05:53 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 25512 ms on 2272e1c3a46f (executor driver) (4/23)
23/11/29 03:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:05:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000007
23/11/29 03:05:53 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:05:53 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:05:53 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (2272e1c3a46f, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:53 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:05:53 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 25795 ms on 2272e1c3a46f (executor driver) (5/23)
23/11/29 03:05:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:05:54 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000002
23/11/29 03:05:54 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 03:05:54 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:05:54 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (2272e1c3a46f, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:54 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 26174 ms on 2272e1c3a46f (executor driver) (6/23)
23/11/29 03:05:54 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:05:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:54 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:05:54 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000000
23/11/29 03:05:54 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 03:05:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:05:54 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (2272e1c3a46f, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 26514 ms on 2272e1c3a46f (executor driver) (7/23)
23/11/29 03:05:54 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:05:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:54 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:05:54 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000003
23/11/29 03:05:54 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 03:05:54 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:05:54 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (2272e1c3a46f, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:05:54 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 26716 ms on 2272e1c3a46f (executor driver) (8/23)
23/11/29 03:05:54 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:05:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:05:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:05:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:05:54 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:06:12 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000011
23/11/29 03:06:12 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000011_12: Committed. Elapsed time: 1 ms.
23/11/29 03:06:12 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:06:12 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (2272e1c3a46f, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:12 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 19485 ms on 2272e1c3a46f (executor driver) (9/23)
23/11/29 03:06:13 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:06:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:06:13 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000012
23/11/29 03:06:13 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:06:13 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:06:13 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (2272e1c3a46f, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:13 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 19444 ms on 2272e1c3a46f (executor driver) (10/23)
23/11/29 03:06:13 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:06:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:06:13 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000008
23/11/29 03:06:13 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000008_9: Committed. Elapsed time: 1 ms.
23/11/29 03:06:13 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:06:13 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (2272e1c3a46f, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:13 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 21579 ms on 2272e1c3a46f (executor driver) (11/23)
23/11/29 03:06:13 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:06:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:06:13 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000013
23/11/29 03:06:13 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000013_14: Committed. Elapsed time: 2 ms.
23/11/29 03:06:13 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:06:13 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (2272e1c3a46f, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:13 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 19531 ms on 2272e1c3a46f (executor driver) (12/23)
23/11/29 03:06:13 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:06:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:13 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:06:14 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000014
23/11/29 03:06:14 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000014_15: Committed. Elapsed time: 5 ms.
23/11/29 03:06:14 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:06:14 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (2272e1c3a46f, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:14 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 19670 ms on 2272e1c3a46f (executor driver) (13/23)
23/11/29 03:06:14 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:06:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:14 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:06:14 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000010
23/11/29 03:06:14 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000010_11: Committed. Elapsed time: 0 ms.
23/11/29 03:06:14 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:06:14 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (2272e1c3a46f, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:14 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 20936 ms on 2272e1c3a46f (executor driver) (14/23)
23/11/29 03:06:14 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:06:14 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000009
23/11/29 03:06:14 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000009_10: Committed. Elapsed time: 0 ms.
23/11/29 03:06:14 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:06:14 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (2272e1c3a46f, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:14 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 21794 ms on 2272e1c3a46f (executor driver) (15/23)
23/11/29 03:06:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:14 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:06:14 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:06:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:14 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:06:14 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000015
23/11/29 03:06:14 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000015_16: Committed. Elapsed time: 0 ms.
23/11/29 03:06:14 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:06:14 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 20029 ms on 2272e1c3a46f (executor driver) (16/23)
23/11/29 03:06:31 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000022
23/11/29 03:06:31 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:06:31 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:06:31 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 17109 ms on 2272e1c3a46f (executor driver) (17/23)
23/11/29 03:06:32 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000017
23/11/29 03:06:32 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:06:32 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:06:32 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 19005 ms on 2272e1c3a46f (executor driver) (18/23)
23/11/29 03:06:32 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000016
23/11/29 03:06:32 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:06:32 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:06:32 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 19342 ms on 2272e1c3a46f (executor driver) (19/23)
23/11/29 03:06:32 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000018
23/11/29 03:06:32 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:06:32 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:06:32 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 19164 ms on 2272e1c3a46f (executor driver) (20/23)
23/11/29 03:06:33 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000019
23/11/29 03:06:33 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000019_20: Committed. Elapsed time: 1 ms.
23/11/29 03:06:33 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:06:33 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 19549 ms on 2272e1c3a46f (executor driver) (21/23)
23/11/29 03:06:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000021
23/11/29 03:06:34 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000021_22: Committed. Elapsed time: 1 ms.
23/11/29 03:06:34 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:06:34 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 19697 ms on 2272e1c3a46f (executor driver) (22/23)
23/11/29 03:06:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290305274913760544227224574_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290305274913760544227224574_0001_m_000020
23/11/29 03:06:34 INFO SparkHadoopMapRedUtil: attempt_202311290305274913760544227224574_0001_m_000020_21: Committed. Elapsed time: 1 ms.
23/11/29 03:06:34 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:06:34 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 20251 ms on 2272e1c3a46f (executor driver) (23/23)
23/11/29 03:06:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:06:34 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 66.512 s
23/11/29 03:06:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:06:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:06:34 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 66.520526 s
23/11/29 03:06:34 INFO FileFormatWriter: Start to commit write Job ef834221-2646-4985-87f6-1a7bd75887f3.
23/11/29 03:06:34 INFO FileFormatWriter: Write Job ef834221-2646-4985-87f6-1a7bd75887f3 committed. Elapsed time: 52 ms.
23/11/29 03:06:34 INFO FileFormatWriter: Finished processing stats for write job ef834221-2646-4985-87f6-1a7bd75887f3.
23/11/29 03:06:34 INFO SparkUI: Stopped Spark web UI at http://2272e1c3a46f:4040
23/11/29 03:06:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:06:34 INFO MemoryStore: MemoryStore cleared
23/11/29 03:06:34 INFO BlockManager: BlockManager stopped
23/11/29 03:06:34 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:06:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:06:34 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 71
23/11/29 03:06:34 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:06:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb8f4035-16cd-4fbd-bb56-8a57c0622db4
23/11/29 03:06:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-0cead82c-c6bd-49ca-a359-84ace937c69b
Execution 6:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-2ea8e785-653a-47b1-a3e9-ab1402171f79;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (602ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (394ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (342ms)
:: resolution report :: resolve 6489ms :: artifacts dl 1355ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-2ea8e785-653a-47b1-a3e9-ab1402171f79
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 03:06:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:06:47 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:06:47 INFO ResourceUtils: ==============================================================
23/11/29 03:06:47 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:06:47 INFO ResourceUtils: ==============================================================
23/11/29 03:06:47 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:06:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:06:47 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:06:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:06:47 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:06:47 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:06:47 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:06:47 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:06:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:06:47 INFO Utils: Successfully started service 'sparkDriver' on port 36781.
23/11/29 03:06:47 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:06:47 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:06:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:06:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:06:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:06:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-182aca81-2202-4c68-806b-f25622ad36e7
23/11/29 03:06:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:06:47 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:06:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:06:48 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://5cd7796cca48:36781/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://5cd7796cca48:36781/jars/org.tukaani_xz-1.9.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://5cd7796cca48:36781/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://5cd7796cca48:36781/jars/spark.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO Executor: Starting executor ID driver on host 5cd7796cca48
23/11/29 03:06:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:06:48 INFO Executor: Fetching spark://5cd7796cca48:36781/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO TransportClientFactory: Successfully created connection to 5cd7796cca48/172.17.0.2:36781 after 62 ms (0 ms spent in bootstraps)
23/11/29 03:06:48 INFO Utils: Fetching spark://5cd7796cca48:36781/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/fetchFileTemp10522917689755656220.tmp
23/11/29 03:06:48 INFO Executor: Adding file:/tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:06:48 INFO Executor: Fetching spark://5cd7796cca48:36781/jars/org.tukaani_xz-1.9.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO Utils: Fetching spark://5cd7796cca48:36781/jars/org.tukaani_xz-1.9.jar to /tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/fetchFileTemp16854606102467794870.tmp
23/11/29 03:06:48 INFO Executor: Adding file:/tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:06:48 INFO Executor: Fetching spark://5cd7796cca48:36781/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO Utils: Fetching spark://5cd7796cca48:36781/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/fetchFileTemp12010045580576192305.tmp
23/11/29 03:06:48 INFO Executor: Adding file:/tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:06:48 INFO Executor: Fetching spark://5cd7796cca48:36781/jars/spark.jar with timestamp 1701227207228
23/11/29 03:06:48 INFO Utils: Fetching spark://5cd7796cca48:36781/jars/spark.jar to /tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/fetchFileTemp16415606223008592061.tmp
23/11/29 03:06:49 INFO Executor: Adding file:/tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee/userFiles-0cd47d23-01c9-44b6-8a61-8b03c844db27/spark.jar to class loader
23/11/29 03:06:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44087.
23/11/29 03:06:49 INFO NettyBlockTransferService: Server created on 5cd7796cca48:44087
23/11/29 03:06:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:06:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5cd7796cca48, 44087, None)
23/11/29 03:06:49 INFO BlockManagerMasterEndpoint: Registering block manager 5cd7796cca48:44087 with 434.4 MiB RAM, BlockManagerId(driver, 5cd7796cca48, 44087, None)
23/11/29 03:06:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5cd7796cca48, 44087, None)
23/11/29 03:06:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5cd7796cca48, 44087, None)
23/11/29 03:06:49 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:06:49 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:06:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:06:51 INFO InMemoryFileIndex: It took 58 ms to list leaf files for 1 paths.
23/11/29 03:06:53 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:06:53 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:06:53 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:06:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:06:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:06:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5cd7796cca48:44087 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:06:53 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:06:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:06:54 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:06:54 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:06:54 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:06:54 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:06:54 INFO DAGScheduler: Missing parents: List()
23/11/29 03:06:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:06:54 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:06:54 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:06:54 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5cd7796cca48:44087 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:06:54 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:06:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:06:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:06:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5cd7796cca48, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:06:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:06:54 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:06:54 INFO CodeGenerator: Code generated in 173.940364 ms
23/11/29 03:06:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:06:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 488 ms on 5cd7796cca48 (executor driver) (1/1)
23/11/29 03:06:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:06:54 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.595 s
23/11/29 03:06:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:06:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:06:54 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.639705 s
23/11/29 03:06:54 INFO CodeGenerator: Code generated in 22.945231 ms
23/11/29 03:06:54 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:06:55 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:06:55 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:06:55 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:06:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:06:55 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:06:55 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:06:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:06:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5cd7796cca48:44087 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:06:55 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:06:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:06:55 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:06:55 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:06:55 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:06:55 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:06:55 INFO DAGScheduler: Missing parents: List()
23/11/29 03:06:55 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:06:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:06:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:06:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5cd7796cca48:44087 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:06:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:06:55 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:06:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:06:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (5cd7796cca48, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (5cd7796cca48, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (5cd7796cca48, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (5cd7796cca48, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (5cd7796cca48, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (5cd7796cca48, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (5cd7796cca48, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (5cd7796cca48, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:06:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:06:55 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:06:55 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:06:55 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:06:55 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:06:55 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:06:55 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:06:55 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:06:55 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5cd7796cca48:44087 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:06:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5cd7796cca48:44087 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:06:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:06:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:06:55 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:06:55 INFO CodeGenerator: Code generated in 124.60659 ms
23/11/29 03:07:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000004
23/11/29 03:07:19 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000004_5: Committed. Elapsed time: 2 ms.
23/11/29 03:07:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000006
23/11/29 03:07:19 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000006_7: Committed. Elapsed time: 2 ms.
23/11/29 03:07:19 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2541 bytes result sent to driver
23/11/29 03:07:19 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:07:19 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (5cd7796cca48, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:19 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (5cd7796cca48, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:19 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:07:19 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:07:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000002
23/11/29 03:07:19 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 03:07:19 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 24271 ms on 5cd7796cca48 (executor driver) (1/23)
23/11/29 03:07:19 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:07:19 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (5cd7796cca48, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:19 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:07:19 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 24299 ms on 5cd7796cca48 (executor driver) (2/23)
23/11/29 03:07:19 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 24307 ms on 5cd7796cca48 (executor driver) (3/23)
23/11/29 03:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:07:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000003
23/11/29 03:07:19 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 03:07:19 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:07:19 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (5cd7796cca48, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:19 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 24457 ms on 5cd7796cca48 (executor driver) (4/23)
23/11/29 03:07:19 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:07:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:07:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000005
23/11/29 03:07:20 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000005_6: Committed. Elapsed time: 3 ms.
23/11/29 03:07:20 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:07:20 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (5cd7796cca48, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:20 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 25239 ms on 5cd7796cca48 (executor driver) (5/23)
23/11/29 03:07:20 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:07:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000000
23/11/29 03:07:20 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000000_1: Committed. Elapsed time: 2 ms.
23/11/29 03:07:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 03:07:20 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (5cd7796cca48, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 25307 ms on 5cd7796cca48 (executor driver) (6/23)
23/11/29 03:07:20 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:07:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:07:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:07:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000007
23/11/29 03:07:21 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 03:07:21 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:07:21 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (5cd7796cca48, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:21 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:07:21 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 25873 ms on 5cd7796cca48 (executor driver) (7/23)
23/11/29 03:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:21 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:07:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000001
23/11/29 03:07:21 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 03:07:21 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:07:21 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (5cd7796cca48, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:21 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:07:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 25997 ms on 5cd7796cca48 (executor driver) (8/23)
23/11/29 03:07:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:21 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:07:35 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000011
23/11/29 03:07:35 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000011_12: Committed. Elapsed time: 10 ms.
23/11/29 03:07:35 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:07:35 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (5cd7796cca48, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:35 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:07:35 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 15478 ms on 5cd7796cca48 (executor driver) (9/23)
23/11/29 03:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:35 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:07:35 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000013
23/11/29 03:07:35 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000013_14: Committed. Elapsed time: 2 ms.
23/11/29 03:07:35 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:07:35 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (5cd7796cca48, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:35 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:07:35 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 15179 ms on 5cd7796cca48 (executor driver) (10/23)
23/11/29 03:07:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:35 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:07:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000010
23/11/29 03:07:36 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000010_11: Committed. Elapsed time: 1 ms.
23/11/29 03:07:36 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:07:36 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (5cd7796cca48, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:36 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:07:36 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 17090 ms on 5cd7796cca48 (executor driver) (11/23)
23/11/29 03:07:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:36 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:07:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000015
23/11/29 03:07:36 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000015_16: Committed. Elapsed time: 3 ms.
23/11/29 03:07:36 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:07:36 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (5cd7796cca48, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:36 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 15571 ms on 5cd7796cca48 (executor driver) (12/23)
23/11/29 03:07:36 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:07:37 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000014
23/11/29 03:07:37 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000014_15: Committed. Elapsed time: 3 ms.
23/11/29 03:07:37 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:07:37 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (5cd7796cca48, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:37 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:07:37 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 16606 ms on 5cd7796cca48 (executor driver) (13/23)
23/11/29 03:07:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:37 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:07:38 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000012
23/11/29 03:07:38 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:07:38 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:07:38 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (5cd7796cca48, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:38 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 17727 ms on 5cd7796cca48 (executor driver) (14/23)
23/11/29 03:07:38 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:07:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:07:39 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000008
23/11/29 03:07:39 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000008_9: Committed. Elapsed time: 2 ms.
23/11/29 03:07:39 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:07:39 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (5cd7796cca48, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:07:39 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 19375 ms on 5cd7796cca48 (executor driver) (15/23)
23/11/29 03:07:39 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:07:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:07:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:07:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:07:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:07:40 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000009
23/11/29 03:07:40 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000009_10: Committed. Elapsed time: 4 ms.
23/11/29 03:07:40 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:07:40 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 20680 ms on 5cd7796cca48 (executor driver) (16/23)
23/11/29 03:07:51 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000022
23/11/29 03:07:51 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000022_23: Committed. Elapsed time: 3 ms.
23/11/29 03:07:51 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:07:51 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 12806 ms on 5cd7796cca48 (executor driver) (17/23)
23/11/29 03:07:51 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000016
23/11/29 03:07:51 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:07:51 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:07:51 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 16621 ms on 5cd7796cca48 (executor driver) (18/23)
23/11/29 03:07:52 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000017
23/11/29 03:07:52 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:07:52 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:07:52 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 16112 ms on 5cd7796cca48 (executor driver) (19/23)
23/11/29 03:07:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000018
23/11/29 03:07:53 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000018_19: Committed. Elapsed time: 1 ms.
23/11/29 03:07:53 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:07:53 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 16230 ms on 5cd7796cca48 (executor driver) (20/23)
23/11/29 03:07:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000019
23/11/29 03:07:53 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000019_20: Committed. Elapsed time: 1 ms.
23/11/29 03:07:53 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:07:53 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 16135 ms on 5cd7796cca48 (executor driver) (21/23)
23/11/29 03:07:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000021
23/11/29 03:07:53 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000021_22: Committed. Elapsed time: 1 ms.
23/11/29 03:07:53 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:07:53 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 14969 ms on 5cd7796cca48 (executor driver) (22/23)
23/11/29 03:07:53 INFO FileOutputCommitter: Saved output of task 'attempt_202311290306557041108098470535832_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290306557041108098470535832_0001_m_000020
23/11/29 03:07:53 INFO SparkHadoopMapRedUtil: attempt_202311290306557041108098470535832_0001_m_000020_21: Committed. Elapsed time: 0 ms.
23/11/29 03:07:53 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:07:53 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 15814 ms on 5cd7796cca48 (executor driver) (23/23)
23/11/29 03:07:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:07:53 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 58.339 s
23/11/29 03:07:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:07:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:07:53 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 58.347715 s
23/11/29 03:07:53 INFO FileFormatWriter: Start to commit write Job a7de1ace-c3c1-433d-900c-d7a5a2ea5cae.
23/11/29 03:07:53 INFO FileFormatWriter: Write Job a7de1ace-c3c1-433d-900c-d7a5a2ea5cae committed. Elapsed time: 59 ms.
23/11/29 03:07:53 INFO FileFormatWriter: Finished processing stats for write job a7de1ace-c3c1-433d-900c-d7a5a2ea5cae.
23/11/29 03:07:53 INFO SparkUI: Stopped Spark web UI at http://5cd7796cca48:4040
23/11/29 03:07:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:07:53 INFO MemoryStore: MemoryStore cleared
23/11/29 03:07:53 INFO BlockManager: BlockManager stopped
23/11/29 03:07:53 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:07:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:07:53 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 63
23/11/29 03:07:53 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:07:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-ff0e0243-5643-47d3-a77b-62d089d810ee
23/11/29 03:07:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-18076bec-6a36-4293-845f-6cdd1b44bcae
Execution 7:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-6d30a37b-757b-429e-a62c-3bb5eec414f3;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1219ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (1102ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (419ms)
:: resolution report :: resolve 9182ms :: artifacts dl 2748ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-6d30a37b-757b-429e-a62c-3bb5eec414f3
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/10ms)
23/11/29 03:08:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:08:11 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:08:11 INFO ResourceUtils: ==============================================================
23/11/29 03:08:11 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:08:11 INFO ResourceUtils: ==============================================================
23/11/29 03:08:11 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:08:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:08:11 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:08:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:08:11 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:08:11 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:08:11 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:08:11 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:08:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:08:11 INFO Utils: Successfully started service 'sparkDriver' on port 43725.
23/11/29 03:08:11 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:08:11 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:08:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:08:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:08:11 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:08:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7db4085a-b8e3-4497-849c-02c6b8e01da8
23/11/29 03:08:11 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:08:12 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:08:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:08:12 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://b55f19816b88:43725/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://b55f19816b88:43725/jars/org.tukaani_xz-1.9.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://b55f19816b88:43725/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://b55f19816b88:43725/jars/spark.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO Executor: Starting executor ID driver on host b55f19816b88
23/11/29 03:08:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:08:12 INFO Executor: Fetching spark://b55f19816b88:43725/jars/org.tukaani_xz-1.9.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO TransportClientFactory: Successfully created connection to b55f19816b88/172.17.0.2:43725 after 31 ms (0 ms spent in bootstraps)
23/11/29 03:08:12 INFO Utils: Fetching spark://b55f19816b88:43725/jars/org.tukaani_xz-1.9.jar to /tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/fetchFileTemp17772305068338685777.tmp
23/11/29 03:08:12 INFO Executor: Adding file:/tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:08:12 INFO Executor: Fetching spark://b55f19816b88:43725/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO Utils: Fetching spark://b55f19816b88:43725/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/fetchFileTemp5835514053537020974.tmp
23/11/29 03:08:12 INFO Executor: Adding file:/tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:08:12 INFO Executor: Fetching spark://b55f19816b88:43725/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO Utils: Fetching spark://b55f19816b88:43725/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/fetchFileTemp1329414705496944963.tmp
23/11/29 03:08:12 INFO Executor: Adding file:/tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:08:12 INFO Executor: Fetching spark://b55f19816b88:43725/jars/spark.jar with timestamp 1701227291405
23/11/29 03:08:12 INFO Utils: Fetching spark://b55f19816b88:43725/jars/spark.jar to /tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/fetchFileTemp3835603328581435104.tmp
23/11/29 03:08:12 INFO Executor: Adding file:/tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770/userFiles-de2a265c-f03f-4e00-af76-b923cc16dc59/spark.jar to class loader
23/11/29 03:08:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36827.
23/11/29 03:08:12 INFO NettyBlockTransferService: Server created on b55f19816b88:36827
23/11/29 03:08:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:08:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b55f19816b88, 36827, None)
23/11/29 03:08:13 INFO BlockManagerMasterEndpoint: Registering block manager b55f19816b88:36827 with 434.4 MiB RAM, BlockManagerId(driver, b55f19816b88, 36827, None)
23/11/29 03:08:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b55f19816b88, 36827, None)
23/11/29 03:08:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b55f19816b88, 36827, None)
23/11/29 03:08:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:08:13 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:08:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:08:14 INFO InMemoryFileIndex: It took 54 ms to list leaf files for 1 paths.
23/11/29 03:08:16 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:08:16 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:08:16 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:08:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:08:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 03:08:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b55f19816b88:36827 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 03:08:17 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:08:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:08:17 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:08:17 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:08:17 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:08:17 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:08:17 INFO DAGScheduler: Missing parents: List()
23/11/29 03:08:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:08:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:08:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:08:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b55f19816b88:36827 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:08:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:08:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:08:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:08:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b55f19816b88, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:08:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:08:17 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:08:18 INFO CodeGenerator: Code generated in 170.192744 ms
23/11/29 03:08:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:08:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 505 ms on b55f19816b88 (executor driver) (1/1)
23/11/29 03:08:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:08:18 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.643 s
23/11/29 03:08:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:08:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:08:18 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.701436 s
23/11/29 03:08:18 INFO CodeGenerator: Code generated in 21.580893 ms
23/11/29 03:08:18 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:08:18 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:08:18 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:08:18 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:08:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:08:18 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:08:18 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:08:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:08:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b55f19816b88:36827 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:08:18 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:08:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:08:18 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:08:18 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:08:18 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:08:18 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:08:18 INFO DAGScheduler: Missing parents: List()
23/11/29 03:08:18 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:08:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:08:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 03:08:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b55f19816b88:36827 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 03:08:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:08:18 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:08:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:08:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b55f19816b88, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (b55f19816b88, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (b55f19816b88, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (b55f19816b88, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (b55f19816b88, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (b55f19816b88, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (b55f19816b88, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (b55f19816b88, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:08:18 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:08:18 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:08:18 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:08:18 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:08:18 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:08:18 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:08:18 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b55f19816b88:36827 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:08:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:08:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b55f19816b88:36827 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:08:18 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:08:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:08:19 INFO CodeGenerator: Code generated in 134.286616 ms
23/11/29 03:08:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000000
23/11/29 03:08:42 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000000_1: Committed. Elapsed time: 2 ms.
23/11/29 03:08:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
23/11/29 03:08:42 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (b55f19816b88, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:42 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:08:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 23883 ms on b55f19816b88 (executor driver) (1/23)
23/11/29 03:08:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000001
23/11/29 03:08:42 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 03:08:42 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:08:42 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (b55f19816b88, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:42 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:08:42 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 23913 ms on b55f19816b88 (executor driver) (2/23)
23/11/29 03:08:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:08:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:08:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000003
23/11/29 03:08:42 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 03:08:42 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:08:42 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (b55f19816b88, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:42 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:08:42 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 24059 ms on b55f19816b88 (executor driver) (3/23)
23/11/29 03:08:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:42 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:08:42 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000004
23/11/29 03:08:42 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 03:08:42 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:08:42 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (b55f19816b88, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:42 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 24242 ms on b55f19816b88 (executor driver) (4/23)
23/11/29 03:08:42 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:08:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:08:43 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000006
23/11/29 03:08:43 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000006_7: Committed. Elapsed time: 2 ms.
23/11/29 03:08:43 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:08:43 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (b55f19816b88, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:43 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:08:43 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 24690 ms on b55f19816b88 (executor driver) (5/23)
23/11/29 03:08:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:08:43 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000002
23/11/29 03:08:43 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 03:08:43 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:08:43 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (b55f19816b88, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:43 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:08:43 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 25069 ms on b55f19816b88 (executor driver) (6/23)
23/11/29 03:08:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:08:44 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000007
23/11/29 03:08:44 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000007_8: Committed. Elapsed time: 5 ms.
23/11/29 03:08:44 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:08:44 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (b55f19816b88, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:44 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 25342 ms on b55f19816b88 (executor driver) (7/23)
23/11/29 03:08:44 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:44 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:08:44 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000005
23/11/29 03:08:44 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 03:08:44 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:08:44 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (b55f19816b88, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:08:44 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:08:44 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 25676 ms on b55f19816b88 (executor driver) (8/23)
23/11/29 03:08:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:08:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:08:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:08:44 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:09:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000012
23/11/29 03:09:00 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000012_13: Committed. Elapsed time: 21 ms.
23/11/29 03:09:00 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:09:00 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (b55f19816b88, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:00 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:09:00 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 17158 ms on b55f19816b88 (executor driver) (9/23)
23/11/29 03:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:09:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000011
23/11/29 03:09:00 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000011_12: Committed. Elapsed time: 1 ms.
23/11/29 03:09:00 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:09:00 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (b55f19816b88, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:00 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 17877 ms on b55f19816b88 (executor driver) (10/23)
23/11/29 03:09:00 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:09:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:09:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000010
23/11/29 03:09:01 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000010_11: Committed. Elapsed time: 4 ms.
23/11/29 03:09:01 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:09:01 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (b55f19816b88, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:01 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:09:01 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 18284 ms on b55f19816b88 (executor driver) (11/23)
23/11/29 03:09:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:09:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000013
23/11/29 03:09:01 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000013_14: Committed. Elapsed time: 2 ms.
23/11/29 03:09:01 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:09:01 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (b55f19816b88, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:01 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:09:01 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 17384 ms on b55f19816b88 (executor driver) (12/23)
23/11/29 03:09:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:09:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000014
23/11/29 03:09:01 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000014_15: Committed. Elapsed time: 2 ms.
23/11/29 03:09:01 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:09:01 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (b55f19816b88, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:01 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 17750 ms on b55f19816b88 (executor driver) (13/23)
23/11/29 03:09:01 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:09:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000015
23/11/29 03:09:01 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000015_16: Committed. Elapsed time: 18 ms.
23/11/29 03:09:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000009
23/11/29 03:09:01 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000009_10: Committed. Elapsed time: 7 ms.
23/11/29 03:09:01 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:09:01 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:09:01 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (b55f19816b88, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:01 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:09:01 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (b55f19816b88, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:01 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:09:01 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 17481 ms on b55f19816b88 (executor driver) (14/23)
23/11/29 03:09:01 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 19251 ms on b55f19816b88 (executor driver) (15/23)
23/11/29 03:09:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:09:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:09:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:09:02 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000008
23/11/29 03:09:02 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000008_9: Committed. Elapsed time: 2 ms.
23/11/29 03:09:02 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:09:02 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 19429 ms on b55f19816b88 (executor driver) (16/23)
23/11/29 03:09:14 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000022
23/11/29 03:09:14 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:09:14 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:09:14 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 12467 ms on b55f19816b88 (executor driver) (17/23)
23/11/29 03:09:15 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000016
23/11/29 03:09:15 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000016_17: Committed. Elapsed time: 0 ms.
23/11/29 03:09:15 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:09:15 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 15078 ms on b55f19816b88 (executor driver) (18/23)
23/11/29 03:09:15 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000017
23/11/29 03:09:15 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000017_18: Committed. Elapsed time: 0 ms.
23/11/29 03:09:15 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:09:15 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 14863 ms on b55f19816b88 (executor driver) (19/23)
23/11/29 03:09:15 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000018
23/11/29 03:09:15 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000018_19: Committed. Elapsed time: 0 ms.
23/11/29 03:09:15 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:09:15 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 14814 ms on b55f19816b88 (executor driver) (20/23)
23/11/29 03:09:15 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000019
23/11/29 03:09:15 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:09:15 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:09:15 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 14786 ms on b55f19816b88 (executor driver) (21/23)
23/11/29 03:09:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000021
23/11/29 03:09:16 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000021_22: Committed. Elapsed time: 1 ms.
23/11/29 03:09:16 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:09:16 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 14668 ms on b55f19816b88 (executor driver) (22/23)
23/11/29 03:09:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290308181553997514846263602_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290308181553997514846263602_0001_m_000020
23/11/29 03:09:16 INFO SparkHadoopMapRedUtil: attempt_202311290308181553997514846263602_0001_m_000020_21: Committed. Elapsed time: 0 ms.
23/11/29 03:09:16 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:09:16 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 15073 ms on b55f19816b88 (executor driver) (23/23)
23/11/29 03:09:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:09:16 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 58.231 s
23/11/29 03:09:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:09:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:09:16 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 58.240581 s
23/11/29 03:09:16 INFO FileFormatWriter: Start to commit write Job ffe79fe6-3630-43be-8d01-5b59c8bdd3ce.
23/11/29 03:09:16 INFO FileFormatWriter: Write Job ffe79fe6-3630-43be-8d01-5b59c8bdd3ce committed. Elapsed time: 49 ms.
23/11/29 03:09:16 INFO FileFormatWriter: Finished processing stats for write job ffe79fe6-3630-43be-8d01-5b59c8bdd3ce.
23/11/29 03:09:16 INFO SparkUI: Stopped Spark web UI at http://b55f19816b88:4040
23/11/29 03:09:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:09:17 INFO MemoryStore: MemoryStore cleared
23/11/29 03:09:17 INFO BlockManager: BlockManager stopped
23/11/29 03:09:17 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:09:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:09:17 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 62
23/11/29 03:09:17 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:09:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-547de105-3f63-40b1-b361-173d85499f51
23/11/29 03:09:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-3799f4f8-d067-43c1-b1c1-4dd650b86770
Execution 8:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-61b4a7c3-9220-4000-a3f6-4e818f4189c2;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (528ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2176ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (402ms)
:: resolution report :: resolve 6513ms :: artifacts dl 3126ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-61b4a7c3-9220-4000-a3f6-4e818f4189c2
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/17ms)
23/11/29 03:09:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:09:31 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:09:31 INFO ResourceUtils: ==============================================================
23/11/29 03:09:31 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:09:31 INFO ResourceUtils: ==============================================================
23/11/29 03:09:31 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:09:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:09:31 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:09:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:09:31 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:09:31 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:09:31 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:09:31 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:09:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:09:32 INFO Utils: Successfully started service 'sparkDriver' on port 46561.
23/11/29 03:09:32 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:09:32 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:09:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:09:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:09:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:09:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5c8e30c2-4100-4a34-9150-b908334c54b4
23/11/29 03:09:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:09:32 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:09:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:09:32 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://3d38c0a10957:46561/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://3d38c0a10957:46561/jars/org.tukaani_xz-1.9.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://3d38c0a10957:46561/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://3d38c0a10957:46561/jars/spark.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO Executor: Starting executor ID driver on host 3d38c0a10957
23/11/29 03:09:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:09:32 INFO Executor: Fetching spark://3d38c0a10957:46561/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO TransportClientFactory: Successfully created connection to 3d38c0a10957/172.17.0.2:46561 after 28 ms (0 ms spent in bootstraps)
23/11/29 03:09:32 INFO Utils: Fetching spark://3d38c0a10957:46561/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/fetchFileTemp5627039355886009419.tmp
23/11/29 03:09:32 INFO Executor: Adding file:/tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:09:32 INFO Executor: Fetching spark://3d38c0a10957:46561/jars/spark.jar with timestamp 1701227371762
23/11/29 03:09:32 INFO Utils: Fetching spark://3d38c0a10957:46561/jars/spark.jar to /tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/fetchFileTemp11903022557114493790.tmp
23/11/29 03:09:33 INFO Executor: Adding file:/tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/spark.jar to class loader
23/11/29 03:09:33 INFO Executor: Fetching spark://3d38c0a10957:46561/jars/org.tukaani_xz-1.9.jar with timestamp 1701227371762
23/11/29 03:09:33 INFO Utils: Fetching spark://3d38c0a10957:46561/jars/org.tukaani_xz-1.9.jar to /tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/fetchFileTemp12225488868039871860.tmp
23/11/29 03:09:33 INFO Executor: Adding file:/tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:09:33 INFO Executor: Fetching spark://3d38c0a10957:46561/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227371762
23/11/29 03:09:33 INFO Utils: Fetching spark://3d38c0a10957:46561/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/fetchFileTemp2457282089476537215.tmp
23/11/29 03:09:33 INFO Executor: Adding file:/tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b/userFiles-013a0d82-7726-46ac-b61b-3fadfa461938/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:09:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37167.
23/11/29 03:09:33 INFO NettyBlockTransferService: Server created on 3d38c0a10957:37167
23/11/29 03:09:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:09:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3d38c0a10957, 37167, None)
23/11/29 03:09:33 INFO BlockManagerMasterEndpoint: Registering block manager 3d38c0a10957:37167 with 434.4 MiB RAM, BlockManagerId(driver, 3d38c0a10957, 37167, None)
23/11/29 03:09:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3d38c0a10957, 37167, None)
23/11/29 03:09:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3d38c0a10957, 37167, None)
23/11/29 03:09:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:09:33 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:09:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:09:34 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.
23/11/29 03:09:36 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:09:36 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:09:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:09:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:09:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:09:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3d38c0a10957:37167 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:09:37 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:09:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:09:37 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:09:37 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:09:37 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:09:37 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:09:37 INFO DAGScheduler: Missing parents: List()
23/11/29 03:09:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:09:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:09:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:09:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3d38c0a10957:37167 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:09:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:09:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:09:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:09:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3d38c0a10957, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:09:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:09:37 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:09:38 INFO CodeGenerator: Code generated in 168.736113 ms
23/11/29 03:09:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:09:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 481 ms on 3d38c0a10957 (executor driver) (1/1)
23/11/29 03:09:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:09:38 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.607 s
23/11/29 03:09:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:09:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:09:38 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.657200 s
23/11/29 03:09:38 INFO CodeGenerator: Code generated in 14.593515 ms
23/11/29 03:09:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:09:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:09:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:09:38 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:09:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:09:38 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:09:38 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:09:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:09:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 3d38c0a10957:37167 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:09:38 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:09:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:09:38 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:09:38 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:09:38 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:09:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:09:38 INFO DAGScheduler: Missing parents: List()
23/11/29 03:09:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:09:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:09:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:09:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 3d38c0a10957:37167 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:09:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:09:38 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:09:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:09:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (3d38c0a10957, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (3d38c0a10957, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (3d38c0a10957, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (3d38c0a10957, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (3d38c0a10957, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (3d38c0a10957, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (3d38c0a10957, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (3d38c0a10957, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:09:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:09:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:09:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:09:38 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:09:38 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:09:38 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:09:38 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:09:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 3d38c0a10957:37167 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:09:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:09:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 3d38c0a10957:37167 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:09:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:09:39 INFO CodeGenerator: Code generated in 113.258921 ms
23/11/29 03:09:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000004
23/11/29 03:09:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000000
23/11/29 03:09:59 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 03:09:59 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000000_1: Committed. Elapsed time: 2 ms.
23/11/29 03:09:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
23/11/29 03:09:59 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (3d38c0a10957, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:09:59 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:09:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 21295 ms on 3d38c0a10957 (executor driver) (1/23)
23/11/29 03:09:59 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2541 bytes result sent to driver
23/11/29 03:09:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000005
23/11/29 03:09:59 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000005_6: Committed. Elapsed time: 7 ms.
23/11/29 03:10:00 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:10:00 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (3d38c0a10957, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:00 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:10:00 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (3d38c0a10957, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:00 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:10:00 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 21332 ms on 3d38c0a10957 (executor driver) (2/23)
23/11/29 03:10:00 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 21346 ms on 3d38c0a10957 (executor driver) (3/23)
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:10:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000007
23/11/29 03:10:00 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000007_8: Committed. Elapsed time: 2 ms.
23/11/29 03:10:00 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:10:00 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (3d38c0a10957, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:00 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:10:00 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 21723 ms on 3d38c0a10957 (executor driver) (4/23)
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:10:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000002
23/11/29 03:10:00 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 03:10:00 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:10:00 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (3d38c0a10957, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:00 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:10:00 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 21982 ms on 3d38c0a10957 (executor driver) (5/23)
23/11/29 03:10:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000006
23/11/29 03:10:00 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 03:10:00 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:10:00 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (3d38c0a10957, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:00 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 21997 ms on 3d38c0a10957 (executor driver) (6/23)
23/11/29 03:10:00 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:10:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:10:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000003
23/11/29 03:10:01 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000003_4: Committed. Elapsed time: 2 ms.
23/11/29 03:10:01 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:10:01 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (3d38c0a10957, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:01 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 22743 ms on 3d38c0a10957 (executor driver) (7/23)
23/11/29 03:10:01 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:10:01 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000001
23/11/29 03:10:01 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 03:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:01 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:10:01 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (3d38c0a10957, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:10:01 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 22841 ms on 3d38c0a10957 (executor driver) (8/23)
23/11/29 03:10:01 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:10:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:01 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:01 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:10:19 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000011
23/11/29 03:10:19 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000011_12: Committed. Elapsed time: 2 ms.
23/11/29 03:10:19 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:10:19 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (3d38c0a10957, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:19 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 19279 ms on 3d38c0a10957 (executor driver) (9/23)
23/11/29 03:10:19 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:10:19 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:19 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:19 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:19 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:10:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000010
23/11/29 03:10:20 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000010_11: Committed. Elapsed time: 1 ms.
23/11/29 03:10:20 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:10:20 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (3d38c0a10957, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:20 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:10:20 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 20530 ms on 3d38c0a10957 (executor driver) (10/23)
23/11/29 03:10:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000015
23/11/29 03:10:20 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000015_16: Committed. Elapsed time: 1 ms.
23/11/29 03:10:20 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:10:20 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (3d38c0a10957, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:20 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 19035 ms on 3d38c0a10957 (executor driver) (11/23)
23/11/29 03:10:20 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:10:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000012
23/11/29 03:10:20 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:10:20 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:10:20 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (3d38c0a10957, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:20 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:10:20 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 20074 ms on 3d38c0a10957 (executor driver) (12/23)
23/11/29 03:10:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:10:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000014
23/11/29 03:10:21 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000014_15: Committed. Elapsed time: 2 ms.
23/11/29 03:10:21 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2541 bytes result sent to driver
23/11/29 03:10:21 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (3d38c0a10957, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:21 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:10:21 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 19816 ms on 3d38c0a10957 (executor driver) (13/23)
23/11/29 03:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:21 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:10:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000008
23/11/29 03:10:21 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000008_9: Committed. Elapsed time: 2 ms.
23/11/29 03:10:21 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:10:21 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (3d38c0a10957, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:21 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 21343 ms on 3d38c0a10957 (executor driver) (14/23)
23/11/29 03:10:21 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:21 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:10:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000013
23/11/29 03:10:21 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000013_14: Committed. Elapsed time: 2 ms.
23/11/29 03:10:21 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:10:21 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (3d38c0a10957, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:10:21 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 20976 ms on 3d38c0a10957 (executor driver) (15/23)
23/11/29 03:10:21 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:10:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:10:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:10:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:10:21 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:10:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000009
23/11/29 03:10:22 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000009_10: Committed. Elapsed time: 3 ms.
23/11/29 03:10:22 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:10:22 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 22210 ms on 3d38c0a10957 (executor driver) (16/23)
23/11/29 03:10:35 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000022
23/11/29 03:10:35 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:10:35 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:10:35 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 13540 ms on 3d38c0a10957 (executor driver) (17/23)
23/11/29 03:10:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000018
23/11/29 03:10:36 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000018_19: Committed. Elapsed time: 1 ms.
23/11/29 03:10:36 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:10:36 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 15727 ms on 3d38c0a10957 (executor driver) (18/23)
23/11/29 03:10:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000017
23/11/29 03:10:36 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000017_18: Committed. Elapsed time: 1 ms.
23/11/29 03:10:36 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:10:36 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 15888 ms on 3d38c0a10957 (executor driver) (19/23)
23/11/29 03:10:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000016
23/11/29 03:10:36 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000016_17: Committed. Elapsed time: 1 ms.
23/11/29 03:10:36 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:10:36 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 16847 ms on 3d38c0a10957 (executor driver) (20/23)
23/11/29 03:10:37 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000019
23/11/29 03:10:37 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000019_20: Committed. Elapsed time: 1 ms.
23/11/29 03:10:37 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:10:37 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 16267 ms on 3d38c0a10957 (executor driver) (21/23)
23/11/29 03:10:37 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000020
23/11/29 03:10:37 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000020_21: Committed. Elapsed time: 1 ms.
23/11/29 03:10:37 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:10:37 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 16268 ms on 3d38c0a10957 (executor driver) (22/23)
23/11/29 03:10:37 INFO FileOutputCommitter: Saved output of task 'attempt_202311290309385047999216795153872_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290309385047999216795153872_0001_m_000021
23/11/29 03:10:37 INFO SparkHadoopMapRedUtil: attempt_202311290309385047999216795153872_0001_m_000021_22: Committed. Elapsed time: 0 ms.
23/11/29 03:10:37 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:10:37 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 16490 ms on 3d38c0a10957 (executor driver) (23/23)
23/11/29 03:10:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:10:37 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 59.187 s
23/11/29 03:10:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:10:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:10:37 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 59.195013 s
23/11/29 03:10:37 INFO FileFormatWriter: Start to commit write Job c8cc3813-49c7-44a7-b419-a5532c4aab74.
23/11/29 03:10:37 INFO FileFormatWriter: Write Job c8cc3813-49c7-44a7-b419-a5532c4aab74 committed. Elapsed time: 53 ms.
23/11/29 03:10:37 INFO FileFormatWriter: Finished processing stats for write job c8cc3813-49c7-44a7-b419-a5532c4aab74.
23/11/29 03:10:37 INFO SparkUI: Stopped Spark web UI at http://3d38c0a10957:4040
23/11/29 03:10:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:10:37 INFO MemoryStore: MemoryStore cleared
23/11/29 03:10:37 INFO BlockManager: BlockManager stopped
23/11/29 03:10:37 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:10:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:10:37 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 63
23/11/29 03:10:37 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:10:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1db71ab-0c34-4919-bbe4-96b3536c8f1b
23/11/29 03:10:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-38ce5461-7850-4d34-b99f-d53cde0ee0d3
Execution 9:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-ef970905-bb2a-40b6-9ac9-22c906787a49;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (605ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2014ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (402ms)
:: resolution report :: resolve 7643ms :: artifacts dl 3040ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-ef970905-bb2a-40b6-9ac9-22c906787a49
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/14ms)
23/11/29 03:10:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:10:53 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:10:53 INFO ResourceUtils: ==============================================================
23/11/29 03:10:53 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:10:53 INFO ResourceUtils: ==============================================================
23/11/29 03:10:53 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:10:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:10:53 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:10:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:10:53 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:10:53 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:10:53 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:10:53 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:10:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:10:54 INFO Utils: Successfully started service 'sparkDriver' on port 46081.
23/11/29 03:10:54 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:10:54 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:10:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:10:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:10:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:10:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aaf355d0-f5e9-4b3e-a75e-17725eca3013
23/11/29 03:10:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:10:54 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:10:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:10:54 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://ffd902537427:46081/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://ffd902537427:46081/jars/org.tukaani_xz-1.9.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://ffd902537427:46081/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://ffd902537427:46081/jars/spark.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO Executor: Starting executor ID driver on host ffd902537427
23/11/29 03:10:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:10:54 INFO Executor: Fetching spark://ffd902537427:46081/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO TransportClientFactory: Successfully created connection to ffd902537427/172.17.0.2:46081 after 28 ms (0 ms spent in bootstraps)
23/11/29 03:10:54 INFO Utils: Fetching spark://ffd902537427:46081/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/fetchFileTemp4775089687426441191.tmp
23/11/29 03:10:54 INFO Executor: Adding file:/tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:10:54 INFO Executor: Fetching spark://ffd902537427:46081/jars/spark.jar with timestamp 1701227453584
23/11/29 03:10:54 INFO Utils: Fetching spark://ffd902537427:46081/jars/spark.jar to /tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/fetchFileTemp13301918753759827381.tmp
23/11/29 03:10:55 INFO Executor: Adding file:/tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/spark.jar to class loader
23/11/29 03:10:55 INFO Executor: Fetching spark://ffd902537427:46081/jars/org.tukaani_xz-1.9.jar with timestamp 1701227453584
23/11/29 03:10:55 INFO Utils: Fetching spark://ffd902537427:46081/jars/org.tukaani_xz-1.9.jar to /tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/fetchFileTemp1862310032932055123.tmp
23/11/29 03:10:55 INFO Executor: Adding file:/tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:10:55 INFO Executor: Fetching spark://ffd902537427:46081/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227453584
23/11/29 03:10:55 INFO Utils: Fetching spark://ffd902537427:46081/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/fetchFileTemp14614584047582416892.tmp
23/11/29 03:10:55 INFO Executor: Adding file:/tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce/userFiles-6c0f9af4-1189-41fb-a3d8-ebf7882462a4/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:10:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35987.
23/11/29 03:10:55 INFO NettyBlockTransferService: Server created on ffd902537427:35987
23/11/29 03:10:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:10:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ffd902537427, 35987, None)
23/11/29 03:10:55 INFO BlockManagerMasterEndpoint: Registering block manager ffd902537427:35987 with 434.4 MiB RAM, BlockManagerId(driver, ffd902537427, 35987, None)
23/11/29 03:10:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ffd902537427, 35987, None)
23/11/29 03:10:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ffd902537427, 35987, None)
23/11/29 03:10:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:10:55 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:10:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:10:56 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
23/11/29 03:10:58 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:10:58 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:10:58 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:10:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:10:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:10:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ffd902537427:35987 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:10:59 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:10:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:10:59 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:10:59 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:10:59 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:10:59 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:10:59 INFO DAGScheduler: Missing parents: List()
23/11/29 03:10:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:10:59 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:10:59 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:10:59 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ffd902537427:35987 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:10:59 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:10:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:10:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:10:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ffd902537427, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:10:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:10:59 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:10:59 INFO CodeGenerator: Code generated in 213.56598 ms
23/11/29 03:11:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:11:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 509 ms on ffd902537427 (executor driver) (1/1)
23/11/29 03:11:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:11:00 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.614 s
23/11/29 03:11:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:11:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:11:00 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.658455 s
23/11/29 03:11:00 INFO CodeGenerator: Code generated in 21.00114 ms
23/11/29 03:11:00 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 03:11:00 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:11:00 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:11:00 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:11:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:11:00 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:11:00 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:11:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:11:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ffd902537427:35987 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:11:00 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:11:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:11:00 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:11:00 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:11:00 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:11:00 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:11:00 INFO DAGScheduler: Missing parents: List()
23/11/29 03:11:00 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:11:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:11:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:11:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ffd902537427:35987 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:11:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:11:00 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:11:00 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:11:00 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ffd902537427, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ffd902537427, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (ffd902537427, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (ffd902537427, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (ffd902537427, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (ffd902537427, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (ffd902537427, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (ffd902537427, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:00 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:11:00 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:11:00 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:11:00 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:11:00 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:11:00 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:11:00 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:11:00 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:11:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:11:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:11:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ffd902537427:35987 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:11:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ffd902537427:35987 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:11:00 INFO CodeGenerator: Code generated in 115.767132 ms
23/11/29 03:11:00 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:11:23 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000000
23/11/29 03:11:23 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000000_1: Committed. Elapsed time: 7 ms.
23/11/29 03:11:23 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000005
23/11/29 03:11:23 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 03:11:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
23/11/29 03:11:23 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (ffd902537427, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:23 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:11:23 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:11:23 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (ffd902537427, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:23 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:11:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 23443 ms on ffd902537427 (executor driver) (1/23)
23/11/29 03:11:24 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000001
23/11/29 03:11:24 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 03:11:24 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:11:24 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (ffd902537427, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:24 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:11:24 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 23479 ms on ffd902537427 (executor driver) (2/23)
23/11/29 03:11:24 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 23478 ms on ffd902537427 (executor driver) (3/23)
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:11:24 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000007
23/11/29 03:11:24 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000007_8: Committed. Elapsed time: 10 ms.
23/11/29 03:11:24 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:11:24 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (ffd902537427, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:24 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:11:24 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 23933 ms on ffd902537427 (executor driver) (4/23)
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:11:24 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000006
23/11/29 03:11:24 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000006_7: Committed. Elapsed time: 4 ms.
23/11/29 03:11:24 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:11:24 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (ffd902537427, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:24 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:11:24 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 24183 ms on ffd902537427 (executor driver) (5/23)
23/11/29 03:11:24 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000004
23/11/29 03:11:24 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000004_5: Committed. Elapsed time: 2 ms.
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:11:24 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:11:24 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (ffd902537427, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:24 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 24306 ms on ffd902537427 (executor driver) (6/23)
23/11/29 03:11:24 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:11:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:11:25 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000002
23/11/29 03:11:25 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 03:11:25 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:11:25 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (ffd902537427, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:25 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 25193 ms on ffd902537427 (executor driver) (7/23)
23/11/29 03:11:25 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:11:25 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000003
23/11/29 03:11:25 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 03:11:25 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:11:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:25 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (ffd902537427, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:25 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 25281 ms on ffd902537427 (executor driver) (8/23)
23/11/29 03:11:25 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:11:25 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:11:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:25 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000010
23/11/29 03:11:45 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000010_11: Committed. Elapsed time: 6 ms.
23/11/29 03:11:45 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:11:45 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (ffd902537427, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:45 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 21105 ms on ffd902537427 (executor driver) (9/23)
23/11/29 03:11:45 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:11:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:45 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:11:45 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000012
23/11/29 03:11:45 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:11:45 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:11:45 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (ffd902537427, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:45 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:11:45 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 21257 ms on ffd902537427 (executor driver) (10/23)
23/11/29 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:11:46 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000013
23/11/29 03:11:46 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000013_14: Committed. Elapsed time: 1 ms.
23/11/29 03:11:46 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:11:46 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (ffd902537427, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:46 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:11:46 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 21193 ms on ffd902537427 (executor driver) (11/23)
23/11/29 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:11:46 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000011
23/11/29 03:11:46 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000011_12: Committed. Elapsed time: 2 ms.
23/11/29 03:11:46 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:11:46 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (ffd902537427, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:46 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:11:46 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 21810 ms on ffd902537427 (executor driver) (12/23)
23/11/29 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:11:46 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000008
23/11/29 03:11:46 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000008_9: Committed. Elapsed time: 1 ms.
23/11/29 03:11:46 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:11:46 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (ffd902537427, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:46 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 22538 ms on ffd902537427 (executor driver) (13/23)
23/11/29 03:11:46 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:11:46 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000014
23/11/29 03:11:46 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000014_15: Committed. Elapsed time: 1 ms.
23/11/29 03:11:46 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:11:46 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (ffd902537427, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:46 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 21109 ms on ffd902537427 (executor driver) (14/23)
23/11/29 03:11:46 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:11:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:46 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:46 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:11:47 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000015
23/11/29 03:11:47 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000015_16: Committed. Elapsed time: 1 ms.
23/11/29 03:11:47 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:11:47 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (ffd902537427, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:11:47 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 21916 ms on ffd902537427 (executor driver) (15/23)
23/11/29 03:11:47 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:11:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:11:47 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:11:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:11:47 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:11:48 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000009
23/11/29 03:11:48 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000009_10: Committed. Elapsed time: 3 ms.
23/11/29 03:11:48 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:11:48 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 24345 ms on ffd902537427 (executor driver) (16/23)
23/11/29 03:12:00 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000022
23/11/29 03:12:00 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000022_23: Committed. Elapsed time: 14 ms.
23/11/29 03:12:00 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:12:00 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 12787 ms on ffd902537427 (executor driver) (17/23)
23/11/29 03:12:01 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000016
23/11/29 03:12:01 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000016_17: Committed. Elapsed time: 1 ms.
23/11/29 03:12:01 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:12:01 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 16569 ms on ffd902537427 (executor driver) (18/23)
23/11/29 03:12:02 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000017
23/11/29 03:12:02 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000017_18: Committed. Elapsed time: 1 ms.
23/11/29 03:12:02 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:12:02 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 16314 ms on ffd902537427 (executor driver) (19/23)
23/11/29 03:12:02 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000019
23/11/29 03:12:02 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:12:02 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:12:02 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 16220 ms on ffd902537427 (executor driver) (20/23)
23/11/29 03:12:02 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000018
23/11/29 03:12:02 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000018_19: Committed. Elapsed time: 1 ms.
23/11/29 03:12:02 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:12:02 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 16485 ms on ffd902537427 (executor driver) (21/23)
23/11/29 03:12:03 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000020
23/11/29 03:12:03 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000020_21: Committed. Elapsed time: 1 ms.
23/11/29 03:12:03 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:12:03 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 17031 ms on ffd902537427 (executor driver) (22/23)
23/11/29 03:12:03 INFO FileOutputCommitter: Saved output of task 'attempt_20231129031100476466935069069173_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129031100476466935069069173_0001_m_000021
23/11/29 03:12:03 INFO SparkHadoopMapRedUtil: attempt_20231129031100476466935069069173_0001_m_000021_22: Committed. Elapsed time: 1 ms.
23/11/29 03:12:03 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:12:03 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 17014 ms on ffd902537427 (executor driver) (23/23)
23/11/29 03:12:03 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:12:03 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 63.370 s
23/11/29 03:12:03 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:12:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:12:03 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 63.379489 s
23/11/29 03:12:03 INFO FileFormatWriter: Start to commit write Job 95c54402-45b4-4a7f-932a-e3812d6d3a5f.
23/11/29 03:12:03 INFO FileFormatWriter: Write Job 95c54402-45b4-4a7f-932a-e3812d6d3a5f committed. Elapsed time: 50 ms.
23/11/29 03:12:03 INFO FileFormatWriter: Finished processing stats for write job 95c54402-45b4-4a7f-932a-e3812d6d3a5f.
23/11/29 03:12:03 INFO SparkUI: Stopped Spark web UI at http://ffd902537427:4040
23/11/29 03:12:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:12:03 INFO MemoryStore: MemoryStore cleared
23/11/29 03:12:03 INFO BlockManager: BlockManager stopped
23/11/29 03:12:03 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:12:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:12:04 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 67
23/11/29 03:12:04 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:12:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-adb2e8b3-6ffe-463d-9536-b6990d12c11f
23/11/29 03:12:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-23a64a00-7023-4bbc-a81b-c3b927831dce
Execution 10:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f69d67ea-0025-412e-a1a3-13fb921d419e;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (603ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2013ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (407ms)
:: resolution report :: resolve 6672ms :: artifacts dl 3036ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-f69d67ea-0025-412e-a1a3-13fb921d419e
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/14ms)
23/11/29 03:12:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 03:12:18 INFO SparkContext: Running Spark version 3.3.3
23/11/29 03:12:18 INFO ResourceUtils: ==============================================================
23/11/29 03:12:18 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 03:12:18 INFO ResourceUtils: ==============================================================
23/11/29 03:12:18 INFO SparkContext: Submitted application: AppSpark
23/11/29 03:12:18 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 03:12:18 INFO ResourceProfile: Limiting resource is cpu
23/11/29 03:12:18 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 03:12:18 INFO SecurityManager: Changing view acls to: spark
23/11/29 03:12:18 INFO SecurityManager: Changing modify acls to: spark
23/11/29 03:12:18 INFO SecurityManager: Changing view acls groups to: 
23/11/29 03:12:18 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 03:12:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 03:12:18 INFO Utils: Successfully started service 'sparkDriver' on port 44457.
23/11/29 03:12:18 INFO SparkEnv: Registering MapOutputTracker
23/11/29 03:12:18 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 03:12:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 03:12:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 03:12:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 03:12:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-03fa80d9-5931-451c-84fd-ccffcb2afc11
23/11/29 03:12:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 03:12:18 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 03:12:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 03:12:19 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://5f5d6638af6b:44457/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://5f5d6638af6b:44457/jars/org.tukaani_xz-1.9.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://5f5d6638af6b:44457/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://5f5d6638af6b:44457/jars/spark.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO Executor: Starting executor ID driver on host 5f5d6638af6b
23/11/29 03:12:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 03:12:19 INFO Executor: Fetching spark://5f5d6638af6b:44457/jars/org.tukaani_xz-1.9.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO TransportClientFactory: Successfully created connection to 5f5d6638af6b/172.17.0.2:44457 after 27 ms (0 ms spent in bootstraps)
23/11/29 03:12:19 INFO Utils: Fetching spark://5f5d6638af6b:44457/jars/org.tukaani_xz-1.9.jar to /tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/fetchFileTemp6029709930484743013.tmp
23/11/29 03:12:19 INFO Executor: Adding file:/tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/org.tukaani_xz-1.9.jar to class loader
23/11/29 03:12:19 INFO Executor: Fetching spark://5f5d6638af6b:44457/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO Utils: Fetching spark://5f5d6638af6b:44457/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/fetchFileTemp13150720555609441128.tmp
23/11/29 03:12:19 INFO Executor: Adding file:/tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 03:12:19 INFO Executor: Fetching spark://5f5d6638af6b:44457/jars/spark.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO Utils: Fetching spark://5f5d6638af6b:44457/jars/spark.jar to /tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/fetchFileTemp2537761092930263206.tmp
23/11/29 03:12:19 INFO Executor: Adding file:/tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/spark.jar to class loader
23/11/29 03:12:19 INFO Executor: Fetching spark://5f5d6638af6b:44457/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701227538252
23/11/29 03:12:19 INFO Utils: Fetching spark://5f5d6638af6b:44457/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/fetchFileTemp11566907170949795625.tmp
23/11/29 03:12:19 INFO Executor: Adding file:/tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878/userFiles-cdc308f3-65d9-446a-b10a-235b1a5014f1/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 03:12:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45009.
23/11/29 03:12:19 INFO NettyBlockTransferService: Server created on 5f5d6638af6b:45009
23/11/29 03:12:19 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 03:12:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5f5d6638af6b, 45009, None)
23/11/29 03:12:20 INFO BlockManagerMasterEndpoint: Registering block manager 5f5d6638af6b:45009 with 434.4 MiB RAM, BlockManagerId(driver, 5f5d6638af6b, 45009, None)
23/11/29 03:12:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5f5d6638af6b, 45009, None)
23/11/29 03:12:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5f5d6638af6b, 45009, None)
23/11/29 03:12:20 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 03:12:20 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 03:12:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 03:12:21 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
23/11/29 03:12:24 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:12:24 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:12:24 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 03:12:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 03:12:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 03:12:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5f5d6638af6b:45009 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 03:12:24 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 03:12:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:12:24 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 03:12:24 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 03:12:24 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 03:12:24 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:12:24 INFO DAGScheduler: Missing parents: List()
23/11/29 03:12:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 03:12:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 03:12:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 03:12:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5f5d6638af6b:45009 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 03:12:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 03:12:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 03:12:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 03:12:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5f5d6638af6b, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 03:12:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 03:12:25 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 03:12:25 INFO CodeGenerator: Code generated in 188.989505 ms
23/11/29 03:12:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 03:12:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 476 ms on 5f5d6638af6b (executor driver) (1/1)
23/11/29 03:12:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 03:12:25 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.591 s
23/11/29 03:12:25 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:12:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 03:12:25 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.635628 s
23/11/29 03:12:25 INFO CodeGenerator: Code generated in 16.169732 ms
23/11/29 03:12:25 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
23/11/29 03:12:25 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 03:12:25 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 03:12:25 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 03:12:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 03:12:25 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 03:12:25 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 03:12:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 03:12:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 03:12:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5f5d6638af6b:45009 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:12:25 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 03:12:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 03:12:25 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 03:12:25 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 23 output partitions
23/11/29 03:12:25 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 03:12:25 INFO DAGScheduler: Parents of final stage: List()
23/11/29 03:12:25 INFO DAGScheduler: Missing parents: List()
23/11/29 03:12:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 03:12:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 03:12:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 03:12:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5f5d6638af6b:45009 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 03:12:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 03:12:25 INFO DAGScheduler: Submitting 23 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
23/11/29 03:12:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 23 tasks resource profile 0
23/11/29 03:12:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (5f5d6638af6b, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (5f5d6638af6b, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (5f5d6638af6b, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (5f5d6638af6b, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (5f5d6638af6b, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (5f5d6638af6b, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (5f5d6638af6b, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (5f5d6638af6b, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 03:12:25 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 03:12:25 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 03:12:25 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 03:12:25 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 03:12:25 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 03:12:26 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 03:12:26 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 805306368-939524096, partition values: [empty row]
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 134217728-268435456, partition values: [empty row]
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 939524096-1073741824, partition values: [empty row]
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 671088640-805306368, partition values: [empty row]
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 402653184-536870912, partition values: [empty row]
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 536870912-671088640, partition values: [empty row]
23/11/29 03:12:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 268435456-402653184, partition values: [empty row]
23/11/29 03:12:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5f5d6638af6b:45009 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 03:12:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5f5d6638af6b:45009 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 03:12:26 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-134217728, partition values: [empty row]
23/11/29 03:12:26 INFO CodeGenerator: Code generated in 124.172719 ms
23/11/29 03:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000000
23/11/29 03:12:48 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000000_1: Committed. Elapsed time: 9 ms.
23/11/29 03:12:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
23/11/29 03:12:48 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9) (5f5d6638af6b, executor driver, partition 8, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:48 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)
23/11/29 03:12:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 22281 ms on 5f5d6638af6b (executor driver) (1/23)
23/11/29 03:12:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:48 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1073741824-1207959552, partition values: [empty row]
23/11/29 03:12:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000007
23/11/29 03:12:48 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000007_8: Committed. Elapsed time: 2 ms.
23/11/29 03:12:48 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2498 bytes result sent to driver
23/11/29 03:12:48 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10) (5f5d6638af6b, executor driver, partition 9, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:48 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 22717 ms on 5f5d6638af6b (executor driver) (2/23)
23/11/29 03:12:48 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)
23/11/29 03:12:48 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:48 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:48 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:48 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1207959552-1342177280, partition values: [empty row]
23/11/29 03:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000006
23/11/29 03:12:49 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000006_7: Committed. Elapsed time: 6 ms.
23/11/29 03:12:49 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 03:12:49 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11) (5f5d6638af6b, executor driver, partition 10, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:49 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)
23/11/29 03:12:49 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 23343 ms on 5f5d6638af6b (executor driver) (3/23)
23/11/29 03:12:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1342177280-1476395008, partition values: [empty row]
23/11/29 03:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000002
23/11/29 03:12:49 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 03:12:49 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 03:12:49 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12) (5f5d6638af6b, executor driver, partition 11, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:49 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 23621 ms on 5f5d6638af6b (executor driver) (4/23)
23/11/29 03:12:49 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)
23/11/29 03:12:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1476395008-1610612736, partition values: [empty row]
23/11/29 03:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000003
23/11/29 03:12:49 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 03:12:49 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 03:12:49 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 13) (5f5d6638af6b, executor driver, partition 12, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:49 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 23855 ms on 5f5d6638af6b (executor driver) (5/23)
23/11/29 03:12:49 INFO Executor: Running task 12.0 in stage 1.0 (TID 13)
23/11/29 03:12:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1610612736-1744830464, partition values: [empty row]
23/11/29 03:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000005
23/11/29 03:12:49 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000004
23/11/29 03:12:49 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000004_5: Committed. Elapsed time: 3 ms.
23/11/29 03:12:49 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000005_6: Committed. Elapsed time: 5 ms.
23/11/29 03:12:49 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 03:12:49 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 14) (5f5d6638af6b, executor driver, partition 13, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:49 INFO Executor: Running task 13.0 in stage 1.0 (TID 14)
23/11/29 03:12:49 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 23967 ms on 5f5d6638af6b (executor driver) (6/23)
23/11/29 03:12:49 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 03:12:49 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 15) (5f5d6638af6b, executor driver, partition 14, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:49 INFO Executor: Running task 14.0 in stage 1.0 (TID 15)
23/11/29 03:12:49 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 23981 ms on 5f5d6638af6b (executor driver) (7/23)
23/11/29 03:12:49 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:49 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:49 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:49 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1744830464-1879048192, partition values: [empty row]
23/11/29 03:12:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:50 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:50 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 1879048192-2013265920, partition values: [empty row]
23/11/29 03:12:51 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000001
23/11/29 03:12:51 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 03:12:51 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 03:12:51 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 16) (5f5d6638af6b, executor driver, partition 15, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:12:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 25154 ms on 5f5d6638af6b (executor driver) (8/23)
23/11/29 03:12:51 INFO Executor: Running task 15.0 in stage 1.0 (TID 16)
23/11/29 03:12:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:12:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:12:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:12:51 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2013265920-2147483648, partition values: [empty row]
23/11/29 03:13:07 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000010_11' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000010
23/11/29 03:13:07 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000010_11: Committed. Elapsed time: 4 ms.
23/11/29 03:13:07 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2498 bytes result sent to driver
23/11/29 03:13:07 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 17) (5f5d6638af6b, executor driver, partition 16, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:07 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 18553 ms on 5f5d6638af6b (executor driver) (9/23)
23/11/29 03:13:07 INFO Executor: Running task 16.0 in stage 1.0 (TID 17)
23/11/29 03:13:07 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000013_14' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000013
23/11/29 03:13:07 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000013_14: Committed. Elapsed time: 3 ms.
23/11/29 03:13:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:07 INFO Executor: Finished task 13.0 in stage 1.0 (TID 14). 2498 bytes result sent to driver
23/11/29 03:13:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2147483648-2281701376, partition values: [empty row]
23/11/29 03:13:07 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 18) (5f5d6638af6b, executor driver, partition 17, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:07 INFO Executor: Running task 17.0 in stage 1.0 (TID 18)
23/11/29 03:13:07 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 14) in 17978 ms on 5f5d6638af6b (executor driver) (10/23)
23/11/29 03:13:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2281701376-2415919104, partition values: [empty row]
23/11/29 03:13:08 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000012_13' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000012
23/11/29 03:13:08 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000012_13: Committed. Elapsed time: 1 ms.
23/11/29 03:13:08 INFO Executor: Finished task 12.0 in stage 1.0 (TID 13). 2498 bytes result sent to driver
23/11/29 03:13:08 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 19) (5f5d6638af6b, executor driver, partition 18, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:08 INFO Executor: Running task 18.0 in stage 1.0 (TID 19)
23/11/29 03:13:08 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 13) in 18449 ms on 5f5d6638af6b (executor driver) (11/23)
23/11/29 03:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2415919104-2550136832, partition values: [empty row]
23/11/29 03:13:08 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000014_15' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000014
23/11/29 03:13:08 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000014_15: Committed. Elapsed time: 2 ms.
23/11/29 03:13:08 INFO Executor: Finished task 14.0 in stage 1.0 (TID 15). 2498 bytes result sent to driver
23/11/29 03:13:08 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 20) (5f5d6638af6b, executor driver, partition 19, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:08 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 15) in 18536 ms on 5f5d6638af6b (executor driver) (12/23)
23/11/29 03:13:08 INFO Executor: Running task 19.0 in stage 1.0 (TID 20)
23/11/29 03:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2550136832-2684354560, partition values: [empty row]
23/11/29 03:13:08 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000011_12' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000011
23/11/29 03:13:08 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000011_12: Committed. Elapsed time: 1 ms.
23/11/29 03:13:08 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2498 bytes result sent to driver
23/11/29 03:13:08 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 21) (5f5d6638af6b, executor driver, partition 20, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:08 INFO Executor: Running task 20.0 in stage 1.0 (TID 21)
23/11/29 03:13:08 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 19154 ms on 5f5d6638af6b (executor driver) (13/23)
23/11/29 03:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2684354560-2818572288, partition values: [empty row]
23/11/29 03:13:08 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000008_9' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000008
23/11/29 03:13:08 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000008_9: Committed. Elapsed time: 1 ms.
23/11/29 03:13:08 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2498 bytes result sent to driver
23/11/29 03:13:08 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 22) (5f5d6638af6b, executor driver, partition 21, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:08 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 20694 ms on 5f5d6638af6b (executor driver) (14/23)
23/11/29 03:13:08 INFO Executor: Running task 21.0 in stage 1.0 (TID 22)
23/11/29 03:13:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2818572288-2952790016, partition values: [empty row]
23/11/29 03:13:09 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000015_16' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000015
23/11/29 03:13:09 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000015_16: Committed. Elapsed time: 14 ms.
23/11/29 03:13:09 INFO Executor: Finished task 15.0 in stage 1.0 (TID 16). 2498 bytes result sent to driver
23/11/29 03:13:09 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 23) (5f5d6638af6b, executor driver, partition 22, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 03:13:09 INFO Executor: Running task 22.0 in stage 1.0 (TID 23)
23/11/29 03:13:09 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 16) in 18102 ms on 5f5d6638af6b (executor driver) (15/23)
23/11/29 03:13:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 03:13:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 03:13:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 03:13:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 2952790016-3058183216, partition values: [empty row]
23/11/29 03:13:09 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000009_10' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000009
23/11/29 03:13:09 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000009_10: Committed. Elapsed time: 0 ms.
23/11/29 03:13:09 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2498 bytes result sent to driver
23/11/29 03:13:09 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 21031 ms on 5f5d6638af6b (executor driver) (16/23)
23/11/29 03:13:20 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000022_23' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000022
23/11/29 03:13:20 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000022_23: Committed. Elapsed time: 0 ms.
23/11/29 03:13:20 INFO Executor: Finished task 22.0 in stage 1.0 (TID 23). 2498 bytes result sent to driver
23/11/29 03:13:20 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 23) in 11504 ms on 5f5d6638af6b (executor driver) (17/23)
23/11/29 03:13:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000016_17' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000016
23/11/29 03:13:21 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000016_17: Committed. Elapsed time: 2 ms.
23/11/29 03:13:21 INFO Executor: Finished task 16.0 in stage 1.0 (TID 17). 2498 bytes result sent to driver
23/11/29 03:13:21 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 17) in 14126 ms on 5f5d6638af6b (executor driver) (18/23)
23/11/29 03:13:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000017_18' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000017
23/11/29 03:13:22 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000017_18: Committed. Elapsed time: 1 ms.
23/11/29 03:13:22 INFO Executor: Finished task 17.0 in stage 1.0 (TID 18). 2498 bytes result sent to driver
23/11/29 03:13:22 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 18) in 14433 ms on 5f5d6638af6b (executor driver) (19/23)
23/11/29 03:13:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000018_19' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000018
23/11/29 03:13:23 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000018_19: Committed. Elapsed time: 1 ms.
23/11/29 03:13:23 INFO Executor: Finished task 18.0 in stage 1.0 (TID 19). 2498 bytes result sent to driver
23/11/29 03:13:23 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 19) in 14756 ms on 5f5d6638af6b (executor driver) (20/23)
23/11/29 03:13:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000020_21' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000020
23/11/29 03:13:23 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000020_21: Committed. Elapsed time: 1 ms.
23/11/29 03:13:23 INFO Executor: Finished task 20.0 in stage 1.0 (TID 21). 2498 bytes result sent to driver
23/11/29 03:13:23 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 21) in 14685 ms on 5f5d6638af6b (executor driver) (21/23)
23/11/29 03:13:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000021_22' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000021
23/11/29 03:13:23 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000021_22: Committed. Elapsed time: 1 ms.
23/11/29 03:13:23 INFO Executor: Finished task 21.0 in stage 1.0 (TID 22). 2498 bytes result sent to driver
23/11/29 03:13:23 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 22) in 14810 ms on 5f5d6638af6b (executor driver) (22/23)
23/11/29 03:13:23 INFO FileOutputCommitter: Saved output of task 'attempt_202311290312257253767472882637574_0001_m_000019_20' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290312257253767472882637574_0001_m_000019
23/11/29 03:13:23 INFO SparkHadoopMapRedUtil: attempt_202311290312257253767472882637574_0001_m_000019_20: Committed. Elapsed time: 0 ms.
23/11/29 03:13:23 INFO Executor: Finished task 19.0 in stage 1.0 (TID 20). 2498 bytes result sent to driver
23/11/29 03:13:23 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 20) in 15306 ms on 5f5d6638af6b (executor driver) (23/23)
23/11/29 03:13:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 03:13:23 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 57.875 s
23/11/29 03:13:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 03:13:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 03:13:23 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 57.884620 s
23/11/29 03:13:23 INFO FileFormatWriter: Start to commit write Job 05c8e5da-e603-4fe0-83f2-7be1a7661d29.
23/11/29 03:13:23 INFO FileFormatWriter: Write Job 05c8e5da-e603-4fe0-83f2-7be1a7661d29 committed. Elapsed time: 63 ms.
23/11/29 03:13:23 INFO FileFormatWriter: Finished processing stats for write job 05c8e5da-e603-4fe0-83f2-7be1a7661d29.
23/11/29 03:13:23 INFO SparkUI: Stopped Spark web UI at http://5f5d6638af6b:4040
23/11/29 03:13:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 03:13:23 INFO MemoryStore: MemoryStore cleared
23/11/29 03:13:23 INFO BlockManager: BlockManager stopped
23/11/29 03:13:23 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 03:13:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 03:13:23 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 62
23/11/29 03:13:23 INFO ShutdownHookManager: Shutdown hook called
23/11/29 03:13:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-b3524029-40c9-4f2b-aa0d-da68c5a20878
23/11/29 03:13:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d4df76d8-97db-44e0-b18f-7725b98bc2f3
