Execution 1:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-c1a4c86b-4649-4a02-907f-5d4dbb08349b;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (668ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (587ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (340ms)
:: resolution report :: resolve 6152ms :: artifacts dl 1613ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-c1a4c86b-4649-4a02-907f-5d4dbb08349b
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 02:53:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:53:32 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:53:32 INFO ResourceUtils: ==============================================================
23/11/29 02:53:32 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:53:32 INFO ResourceUtils: ==============================================================
23/11/29 02:53:32 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:53:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:53:32 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:53:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:53:32 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:53:32 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:53:32 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:53:32 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:53:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:53:33 INFO Utils: Successfully started service 'sparkDriver' on port 42359.
23/11/29 02:53:33 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:53:33 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:53:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:53:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:53:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:53:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-15e270a6-ebf7-4bba-9ddf-843802499150
23/11/29 02:53:33 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:53:33 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:53:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:53:33 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://525beffbfcb4:42359/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226412680
23/11/29 02:53:33 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://525beffbfcb4:42359/jars/org.tukaani_xz-1.9.jar with timestamp 1701226412680
23/11/29 02:53:33 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://525beffbfcb4:42359/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226412680
23/11/29 02:53:33 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://525beffbfcb4:42359/jars/spark.jar with timestamp 1701226412680
23/11/29 02:53:33 INFO Executor: Starting executor ID driver on host 525beffbfcb4
23/11/29 02:53:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:53:33 INFO Executor: Fetching spark://525beffbfcb4:42359/jars/spark.jar with timestamp 1701226412680
23/11/29 02:53:33 INFO TransportClientFactory: Successfully created connection to 525beffbfcb4/172.17.0.2:42359 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:53:33 INFO Utils: Fetching spark://525beffbfcb4:42359/jars/spark.jar to /tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/fetchFileTemp3893364501318410029.tmp
23/11/29 02:53:34 INFO Executor: Adding file:/tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/spark.jar to class loader
23/11/29 02:53:34 INFO Executor: Fetching spark://525beffbfcb4:42359/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226412680
23/11/29 02:53:34 INFO Utils: Fetching spark://525beffbfcb4:42359/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/fetchFileTemp10332159870361652094.tmp
23/11/29 02:53:34 INFO Executor: Adding file:/tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:53:34 INFO Executor: Fetching spark://525beffbfcb4:42359/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226412680
23/11/29 02:53:34 INFO Utils: Fetching spark://525beffbfcb4:42359/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/fetchFileTemp11384221268208811919.tmp
23/11/29 02:53:34 INFO Executor: Adding file:/tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:53:34 INFO Executor: Fetching spark://525beffbfcb4:42359/jars/org.tukaani_xz-1.9.jar with timestamp 1701226412680
23/11/29 02:53:34 INFO Utils: Fetching spark://525beffbfcb4:42359/jars/org.tukaani_xz-1.9.jar to /tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/fetchFileTemp8902617565517172484.tmp
23/11/29 02:53:34 INFO Executor: Adding file:/tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7/userFiles-1708c46a-775a-415f-89b9-bd292a7911d5/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:53:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41195.
23/11/29 02:53:34 INFO NettyBlockTransferService: Server created on 525beffbfcb4:41195
23/11/29 02:53:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:53:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 525beffbfcb4, 41195, None)
23/11/29 02:53:34 INFO BlockManagerMasterEndpoint: Registering block manager 525beffbfcb4:41195 with 434.4 MiB RAM, BlockManagerId(driver, 525beffbfcb4, 41195, None)
23/11/29 02:53:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 525beffbfcb4, 41195, None)
23/11/29 02:53:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 525beffbfcb4, 41195, None)
23/11/29 02:53:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:53:34 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:53:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:53:35 INFO InMemoryFileIndex: It took 47 ms to list leaf files for 1 paths.
23/11/29 02:53:37 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:53:37 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:53:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:53:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:53:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:53:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 525beffbfcb4:41195 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:53:37 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:53:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:53:38 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:53:38 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:53:38 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:53:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:53:38 INFO DAGScheduler: Missing parents: List()
23/11/29 02:53:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:53:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:53:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:53:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 525beffbfcb4:41195 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:53:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:53:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:53:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:53:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (525beffbfcb4, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:53:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:53:38 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:53:38 INFO CodeGenerator: Code generated in 157.024796 ms
23/11/29 02:53:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:53:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 438 ms on 525beffbfcb4 (executor driver) (1/1)
23/11/29 02:53:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:53:38 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.548 s
23/11/29 02:53:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:53:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:53:38 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.590823 s
23/11/29 02:53:38 INFO CodeGenerator: Code generated in 18.535356 ms
23/11/29 02:53:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:53:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:53:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:53:38 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:53:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:53:39 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:53:39 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:53:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:53:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 525beffbfcb4:41195 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:53:39 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:53:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:53:39 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:53:39 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:53:39 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:53:39 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:53:39 INFO DAGScheduler: Missing parents: List()
23/11/29 02:53:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:53:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:53:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:53:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 525beffbfcb4:41195 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:53:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:53:39 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:53:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:53:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (525beffbfcb4, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (525beffbfcb4, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (525beffbfcb4, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (525beffbfcb4, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (525beffbfcb4, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (525beffbfcb4, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (525beffbfcb4, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (525beffbfcb4, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:53:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:53:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:53:39 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:53:39 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:53:39 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:53:39 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:53:39 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:53:39 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:53:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:53:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 525beffbfcb4:41195 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:53:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:53:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 525beffbfcb4:41195 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:53:39 INFO CodeGenerator: Code generated in 84.615692 ms
23/11/29 02:53:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:53:44 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000007
23/11/29 02:53:44 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000007_8: Committed. Elapsed time: 18 ms.
23/11/29 02:53:44 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:53:44 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 5754 ms on 525beffbfcb4 (executor driver) (1/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000003
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000003_4: Committed. Elapsed time: 5 ms.
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000002
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 02:53:45 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 5852 ms on 525beffbfcb4 (executor driver) (2/8)
23/11/29 02:53:45 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 5857 ms on 525beffbfcb4 (executor driver) (3/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000001
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000001_2: Committed. Elapsed time: 10 ms.
23/11/29 02:53:45 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 5906 ms on 525beffbfcb4 (executor driver) (4/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000000
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:53:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5979 ms on 525beffbfcb4 (executor driver) (5/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000006
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:53:45 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 6040 ms on 525beffbfcb4 (executor driver) (6/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000004
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000004_5: Committed. Elapsed time: 5 ms.
23/11/29 02:53:45 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 6202 ms on 525beffbfcb4 (executor driver) (7/8)
23/11/29 02:53:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290253392742083580336221492_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290253392742083580336221492_0001_m_000005
23/11/29 02:53:45 INFO SparkHadoopMapRedUtil: attempt_202311290253392742083580336221492_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:53:45 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:53:45 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 6259 ms on 525beffbfcb4 (executor driver) (8/8)
23/11/29 02:53:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:53:45 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 6.309 s
23/11/29 02:53:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:53:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:53:45 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 6.316220 s
23/11/29 02:53:45 INFO FileFormatWriter: Start to commit write Job d432219f-58db-49f2-a2fb-0ecf233a2959.
23/11/29 02:53:45 INFO FileFormatWriter: Write Job d432219f-58db-49f2-a2fb-0ecf233a2959 committed. Elapsed time: 32 ms.
23/11/29 02:53:45 INFO FileFormatWriter: Finished processing stats for write job d432219f-58db-49f2-a2fb-0ecf233a2959.
23/11/29 02:53:45 INFO SparkUI: Stopped Spark web UI at http://525beffbfcb4:4040
23/11/29 02:53:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:53:45 INFO MemoryStore: MemoryStore cleared
23/11/29 02:53:45 INFO BlockManager: BlockManager stopped
23/11/29 02:53:45 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:53:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:53:45 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 10
23/11/29 02:53:45 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:53:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-e27924e3-fee0-410b-b688-a67d6050b22b
23/11/29 02:53:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-61d3e144-1ea4-4f96-8f96-91ca84db87f7
Execution 2:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-71c3388a-6681-434e-8569-be64599e61a7;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (656ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2096ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (453ms)
:: resolution report :: resolve 7362ms :: artifacts dl 3229ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-71c3388a-6681-434e-8569-be64599e61a7
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 02:54:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:54:00 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:54:00 INFO ResourceUtils: ==============================================================
23/11/29 02:54:00 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:54:00 INFO ResourceUtils: ==============================================================
23/11/29 02:54:00 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:54:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:54:00 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:54:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:54:00 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:54:00 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:54:00 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:54:00 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:54:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:54:00 INFO Utils: Successfully started service 'sparkDriver' on port 40747.
23/11/29 02:54:00 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:54:00 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:54:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:54:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:54:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:54:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c523fd13-a75d-49c3-84fe-16f5d79d247e
23/11/29 02:54:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:54:01 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:54:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:54:01 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://dc7a1595998a:40747/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://dc7a1595998a:40747/jars/org.tukaani_xz-1.9.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://dc7a1595998a:40747/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://dc7a1595998a:40747/jars/spark.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO Executor: Starting executor ID driver on host dc7a1595998a
23/11/29 02:54:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:54:01 INFO Executor: Fetching spark://dc7a1595998a:40747/jars/spark.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO TransportClientFactory: Successfully created connection to dc7a1595998a/172.17.0.2:40747 after 30 ms (0 ms spent in bootstraps)
23/11/29 02:54:01 INFO Utils: Fetching spark://dc7a1595998a:40747/jars/spark.jar to /tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/fetchFileTemp3563084839765980297.tmp
23/11/29 02:54:01 INFO Executor: Adding file:/tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/spark.jar to class loader
23/11/29 02:54:01 INFO Executor: Fetching spark://dc7a1595998a:40747/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO Utils: Fetching spark://dc7a1595998a:40747/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/fetchFileTemp7321532502262964061.tmp
23/11/29 02:54:01 INFO Executor: Adding file:/tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:54:01 INFO Executor: Fetching spark://dc7a1595998a:40747/jars/org.tukaani_xz-1.9.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO Utils: Fetching spark://dc7a1595998a:40747/jars/org.tukaani_xz-1.9.jar to /tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/fetchFileTemp8897425825088625354.tmp
23/11/29 02:54:01 INFO Executor: Adding file:/tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:54:01 INFO Executor: Fetching spark://dc7a1595998a:40747/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226440437
23/11/29 02:54:01 INFO Utils: Fetching spark://dc7a1595998a:40747/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/fetchFileTemp6993876047661132418.tmp
23/11/29 02:54:01 INFO Executor: Adding file:/tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216/userFiles-c7f5efbf-4df9-44d5-8773-6f9ad7b28401/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:54:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42769.
23/11/29 02:54:01 INFO NettyBlockTransferService: Server created on dc7a1595998a:42769
23/11/29 02:54:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:54:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, dc7a1595998a, 42769, None)
23/11/29 02:54:02 INFO BlockManagerMasterEndpoint: Registering block manager dc7a1595998a:42769 with 434.4 MiB RAM, BlockManagerId(driver, dc7a1595998a, 42769, None)
23/11/29 02:54:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, dc7a1595998a, 42769, None)
23/11/29 02:54:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, dc7a1595998a, 42769, None)
23/11/29 02:54:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:54:02 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:54:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:54:03 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
23/11/29 02:54:07 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:54:07 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:54:07 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:54:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:54:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:54:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on dc7a1595998a:42769 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:54:07 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:54:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:54:07 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:54:07 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:54:07 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:54:07 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:54:07 INFO DAGScheduler: Missing parents: List()
23/11/29 02:54:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:54:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:54:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:54:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on dc7a1595998a:42769 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:54:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:54:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:54:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:54:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (dc7a1595998a, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:54:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:54:08 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:54:08 INFO CodeGenerator: Code generated in 209.236055 ms
23/11/29 02:54:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:54:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 671 ms on dc7a1595998a (executor driver) (1/1)
23/11/29 02:54:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:54:08 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.826 s
23/11/29 02:54:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:54:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:54:08 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.873036 s
23/11/29 02:54:08 INFO CodeGenerator: Code generated in 21.286919 ms
23/11/29 02:54:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:54:09 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:54:09 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:54:09 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:54:09 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:54:09 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:54:09 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:54:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:54:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on dc7a1595998a:42769 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:54:09 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:54:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:54:09 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:54:09 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:54:09 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:54:09 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:54:09 INFO DAGScheduler: Missing parents: List()
23/11/29 02:54:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:54:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:54:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:54:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on dc7a1595998a:42769 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:54:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:54:09 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:54:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:54:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (dc7a1595998a, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (dc7a1595998a, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (dc7a1595998a, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (dc7a1595998a, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (dc7a1595998a, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (dc7a1595998a, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (dc7a1595998a, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (dc7a1595998a, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:54:09 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:54:09 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:54:09 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:54:09 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:54:09 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:54:09 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:54:09 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:54:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:54:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on dc7a1595998a:42769 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:54:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on dc7a1595998a:42769 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:54:09 INFO CodeGenerator: Code generated in 111.656589 ms
23/11/29 02:54:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000007
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000007_8: Committed. Elapsed time: 2 ms.
23/11/29 02:54:16 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000002
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 02:54:16 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7096 ms on dc7a1595998a (executor driver) (1/8)
23/11/29 02:54:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 7103 ms on dc7a1595998a (executor driver) (2/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000004
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000004_5: Committed. Elapsed time: 2 ms.
23/11/29 02:54:16 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 7219 ms on dc7a1595998a (executor driver) (3/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000001
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000001_2: Committed. Elapsed time: 3 ms.
23/11/29 02:54:16 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 7237 ms on dc7a1595998a (executor driver) (4/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000005
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 02:54:16 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 7267 ms on dc7a1595998a (executor driver) (5/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000003
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:54:16 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 7372 ms on dc7a1595998a (executor driver) (6/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000000
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000000_1: Committed. Elapsed time: 9 ms.
23/11/29 02:54:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7431 ms on dc7a1595998a (executor driver) (7/8)
23/11/29 02:54:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254098298008529724164412_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254098298008529724164412_0001_m_000006
23/11/29 02:54:16 INFO SparkHadoopMapRedUtil: attempt_202311290254098298008529724164412_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:54:16 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:54:16 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 7466 ms on dc7a1595998a (executor driver) (8/8)
23/11/29 02:54:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:54:16 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 7.518 s
23/11/29 02:54:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:54:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:54:16 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 7.525059 s
23/11/29 02:54:16 INFO FileFormatWriter: Start to commit write Job 9021fb79-d0c0-4417-bc03-149848c031ab.
23/11/29 02:54:16 INFO FileFormatWriter: Write Job 9021fb79-d0c0-4417-bc03-149848c031ab committed. Elapsed time: 29 ms.
23/11/29 02:54:16 INFO FileFormatWriter: Finished processing stats for write job 9021fb79-d0c0-4417-bc03-149848c031ab.
23/11/29 02:54:16 INFO SparkUI: Stopped Spark web UI at http://dc7a1595998a:4040
23/11/29 02:54:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:54:16 INFO MemoryStore: MemoryStore cleared
23/11/29 02:54:16 INFO BlockManager: BlockManager stopped
23/11/29 02:54:16 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:54:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:54:16 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 13
23/11/29 02:54:16 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:54:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-4c3b2ee2-fc13-4811-a598-ebca9c9f1353
23/11/29 02:54:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-9387b5e8-d91c-4c17-8999-2a29f55e7216
Execution 3:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-80ac7ebc-16d2-411d-9d9e-d91ebb4f6f1e;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (606ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (1992ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (413ms)
:: resolution report :: resolve 6786ms :: artifacts dl 3029ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-80ac7ebc-16d2-411d-9d9e-d91ebb4f6f1e
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/14ms)
23/11/29 02:54:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:54:31 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:54:31 INFO ResourceUtils: ==============================================================
23/11/29 02:54:31 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:54:31 INFO ResourceUtils: ==============================================================
23/11/29 02:54:31 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:54:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:54:31 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:54:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:54:31 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:54:31 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:54:31 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:54:31 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:54:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:54:31 INFO Utils: Successfully started service 'sparkDriver' on port 40119.
23/11/29 02:54:31 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:54:31 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:54:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:54:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:54:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:54:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a68105b6-e817-423f-a9aa-6730f1098ea0
23/11/29 02:54:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:54:31 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:54:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:54:31 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://700501af1b48:40119/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226470993
23/11/29 02:54:31 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://700501af1b48:40119/jars/org.tukaani_xz-1.9.jar with timestamp 1701226470993
23/11/29 02:54:31 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://700501af1b48:40119/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226470993
23/11/29 02:54:31 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://700501af1b48:40119/jars/spark.jar with timestamp 1701226470993
23/11/29 02:54:32 INFO Executor: Starting executor ID driver on host 700501af1b48
23/11/29 02:54:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:54:32 INFO Executor: Fetching spark://700501af1b48:40119/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226470993
23/11/29 02:54:32 INFO TransportClientFactory: Successfully created connection to 700501af1b48/172.17.0.2:40119 after 30 ms (0 ms spent in bootstraps)
23/11/29 02:54:32 INFO Utils: Fetching spark://700501af1b48:40119/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/fetchFileTemp722457266677500304.tmp
23/11/29 02:54:32 INFO Executor: Adding file:/tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:54:32 INFO Executor: Fetching spark://700501af1b48:40119/jars/org.tukaani_xz-1.9.jar with timestamp 1701226470993
23/11/29 02:54:32 INFO Utils: Fetching spark://700501af1b48:40119/jars/org.tukaani_xz-1.9.jar to /tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/fetchFileTemp15636060402556376844.tmp
23/11/29 02:54:32 INFO Executor: Adding file:/tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:54:32 INFO Executor: Fetching spark://700501af1b48:40119/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226470993
23/11/29 02:54:32 INFO Utils: Fetching spark://700501af1b48:40119/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/fetchFileTemp10478736675817624001.tmp
23/11/29 02:54:32 INFO Executor: Adding file:/tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:54:32 INFO Executor: Fetching spark://700501af1b48:40119/jars/spark.jar with timestamp 1701226470993
23/11/29 02:54:32 INFO Utils: Fetching spark://700501af1b48:40119/jars/spark.jar to /tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/fetchFileTemp693319323059892161.tmp
23/11/29 02:54:32 INFO Executor: Adding file:/tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35/userFiles-c89b0128-dd4d-48be-b0a0-ddac1185c1e3/spark.jar to class loader
23/11/29 02:54:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37643.
23/11/29 02:54:32 INFO NettyBlockTransferService: Server created on 700501af1b48:37643
23/11/29 02:54:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:54:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 700501af1b48, 37643, None)
23/11/29 02:54:32 INFO BlockManagerMasterEndpoint: Registering block manager 700501af1b48:37643 with 434.4 MiB RAM, BlockManagerId(driver, 700501af1b48, 37643, None)
23/11/29 02:54:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 700501af1b48, 37643, None)
23/11/29 02:54:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 700501af1b48, 37643, None)
23/11/29 02:54:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:54:33 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:54:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:54:34 INFO InMemoryFileIndex: It took 65 ms to list leaf files for 1 paths.
23/11/29 02:54:37 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:54:37 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:54:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:54:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:54:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:54:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 700501af1b48:37643 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:54:37 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:54:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:54:37 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:54:37 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:54:37 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:54:37 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:54:37 INFO DAGScheduler: Missing parents: List()
23/11/29 02:54:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:54:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:54:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:54:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 700501af1b48:37643 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:54:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:54:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:54:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:54:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (700501af1b48, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:54:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:54:38 INFO CodeGenerator: Code generated in 165.2373 ms
23/11/29 02:54:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:54:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 412 ms on 700501af1b48 (executor driver) (1/1)
23/11/29 02:54:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:54:38 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.513 s
23/11/29 02:54:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:54:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:54:38 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.559910 s
23/11/29 02:54:38 INFO CodeGenerator: Code generated in 15.144797 ms
23/11/29 02:54:38 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
23/11/29 02:54:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:54:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:54:38 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:54:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:54:38 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:54:38 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:54:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:54:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 700501af1b48:37643 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:54:38 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:54:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:54:38 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:54:38 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:54:38 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:54:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:54:38 INFO DAGScheduler: Missing parents: List()
23/11/29 02:54:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:54:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:54:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:54:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 700501af1b48:37643 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:54:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:54:38 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:54:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:54:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (700501af1b48, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (700501af1b48, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (700501af1b48, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (700501af1b48, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (700501af1b48, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (700501af1b48, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (700501af1b48, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (700501af1b48, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:54:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:54:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:54:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:54:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:54:38 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:54:38 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:54:38 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:54:38 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:54:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:54:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:54:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:54:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:54:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 700501af1b48:37643 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:54:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 700501af1b48:37643 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:54:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:54:39 INFO CodeGenerator: Code generated in 90.974317 ms
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000007
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 02:54:46 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7729 ms on 700501af1b48 (executor driver) (1/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000000
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:54:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2541 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 7915 ms on 700501af1b48 (executor driver) (2/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000006
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:54:46 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 7971 ms on 700501af1b48 (executor driver) (3/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000003
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:54:46 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 7992 ms on 700501af1b48 (executor driver) (4/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000001
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:54:46 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8039 ms on 700501af1b48 (executor driver) (5/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000004
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:54:46 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8044 ms on 700501af1b48 (executor driver) (6/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000002
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000002_3: Committed. Elapsed time: 5 ms.
23/11/29 02:54:46 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8053 ms on 700501af1b48 (executor driver) (7/8)
23/11/29 02:54:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290254386444480101960623820_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290254386444480101960623820_0001_m_000005
23/11/29 02:54:46 INFO SparkHadoopMapRedUtil: attempt_202311290254386444480101960623820_0001_m_000005_6: Committed. Elapsed time: 0 ms.
23/11/29 02:54:46 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:54:46 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8128 ms on 700501af1b48 (executor driver) (8/8)
23/11/29 02:54:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:54:46 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.175 s
23/11/29 02:54:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:54:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:54:46 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.181488 s
23/11/29 02:54:46 INFO FileFormatWriter: Start to commit write Job 5d3732e4-8a5a-494a-81d8-5998f860e8fe.
23/11/29 02:54:46 INFO FileFormatWriter: Write Job 5d3732e4-8a5a-494a-81d8-5998f860e8fe committed. Elapsed time: 16 ms.
23/11/29 02:54:46 INFO FileFormatWriter: Finished processing stats for write job 5d3732e4-8a5a-494a-81d8-5998f860e8fe.
23/11/29 02:54:46 INFO SparkUI: Stopped Spark web UI at http://700501af1b48:4040
23/11/29 02:54:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:54:47 INFO MemoryStore: MemoryStore cleared
23/11/29 02:54:47 INFO BlockManager: BlockManager stopped
23/11/29 02:54:47 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:54:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:54:47 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 12
23/11/29 02:54:47 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:54:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb373186-4f00-48de-b321-d630f16c9ff6
23/11/29 02:54:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-bb6bec9e-d376-45a6-8536-a3a575afaa35
Execution 4:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-1c45d000-afba-468d-a33a-83cbf3e30be5;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (653ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2036ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (512ms)
:: resolution report :: resolve 7238ms :: artifacts dl 3215ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-1c45d000-afba-468d-a33a-83cbf3e30be5
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 02:55:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:55:01 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:55:01 INFO ResourceUtils: ==============================================================
23/11/29 02:55:01 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:55:01 INFO ResourceUtils: ==============================================================
23/11/29 02:55:01 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:55:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:55:01 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:55:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:55:01 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:55:01 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:55:01 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:55:01 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:55:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:55:02 INFO Utils: Successfully started service 'sparkDriver' on port 35299.
23/11/29 02:55:02 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:55:02 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:55:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:55:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:55:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:55:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ee57003f-9e3c-4c19-a893-1e0d1ba909e9
23/11/29 02:55:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:55:02 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:55:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:55:02 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://7de6bceff024:35299/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://7de6bceff024:35299/jars/org.tukaani_xz-1.9.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://7de6bceff024:35299/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://7de6bceff024:35299/jars/spark.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO Executor: Starting executor ID driver on host 7de6bceff024
23/11/29 02:55:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:55:02 INFO Executor: Fetching spark://7de6bceff024:35299/jars/org.tukaani_xz-1.9.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO TransportClientFactory: Successfully created connection to 7de6bceff024/172.17.0.2:35299 after 30 ms (0 ms spent in bootstraps)
23/11/29 02:55:02 INFO Utils: Fetching spark://7de6bceff024:35299/jars/org.tukaani_xz-1.9.jar to /tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/fetchFileTemp1757670430671967517.tmp
23/11/29 02:55:02 INFO Executor: Adding file:/tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:55:02 INFO Executor: Fetching spark://7de6bceff024:35299/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO Utils: Fetching spark://7de6bceff024:35299/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/fetchFileTemp3333050913114405139.tmp
23/11/29 02:55:02 INFO Executor: Adding file:/tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:55:02 INFO Executor: Fetching spark://7de6bceff024:35299/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO Utils: Fetching spark://7de6bceff024:35299/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/fetchFileTemp8610867543980565212.tmp
23/11/29 02:55:02 INFO Executor: Adding file:/tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:55:02 INFO Executor: Fetching spark://7de6bceff024:35299/jars/spark.jar with timestamp 1701226501657
23/11/29 02:55:02 INFO Utils: Fetching spark://7de6bceff024:35299/jars/spark.jar to /tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/fetchFileTemp11385712055711082803.tmp
23/11/29 02:55:03 INFO Executor: Adding file:/tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac/userFiles-cf6cbb5e-de61-4ce8-a7bd-35056c227ea9/spark.jar to class loader
23/11/29 02:55:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36357.
23/11/29 02:55:03 INFO NettyBlockTransferService: Server created on 7de6bceff024:36357
23/11/29 02:55:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:55:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7de6bceff024, 36357, None)
23/11/29 02:55:03 INFO BlockManagerMasterEndpoint: Registering block manager 7de6bceff024:36357 with 434.4 MiB RAM, BlockManagerId(driver, 7de6bceff024, 36357, None)
23/11/29 02:55:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7de6bceff024, 36357, None)
23/11/29 02:55:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7de6bceff024, 36357, None)
23/11/29 02:55:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:55:03 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:55:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:55:04 INFO InMemoryFileIndex: It took 59 ms to list leaf files for 1 paths.
23/11/29 02:55:07 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:55:07 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:55:07 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:55:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:55:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:55:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7de6bceff024:36357 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:55:07 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:55:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:55:08 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:55:08 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:55:08 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:55:08 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:55:08 INFO DAGScheduler: Missing parents: List()
23/11/29 02:55:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:55:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:55:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:55:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7de6bceff024:36357 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:55:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:55:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:55:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:55:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (7de6bceff024, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:55:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:55:08 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:55:08 INFO CodeGenerator: Code generated in 184.932473 ms
23/11/29 02:55:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:55:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 445 ms on 7de6bceff024 (executor driver) (1/1)
23/11/29 02:55:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:55:08 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.554 s
23/11/29 02:55:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:55:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:55:08 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.604430 s
23/11/29 02:55:08 INFO CodeGenerator: Code generated in 14.381931 ms
23/11/29 02:55:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:55:08 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:55:08 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:55:08 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:55:09 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:55:09 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:55:09 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:55:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:55:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7de6bceff024:36357 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:55:09 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:55:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:55:09 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:55:09 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:55:09 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:55:09 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:55:09 INFO DAGScheduler: Missing parents: List()
23/11/29 02:55:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:55:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:55:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:55:09 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7de6bceff024:36357 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:55:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:55:09 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:55:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:55:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (7de6bceff024, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (7de6bceff024, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (7de6bceff024, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (7de6bceff024, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (7de6bceff024, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (7de6bceff024, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (7de6bceff024, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (7de6bceff024, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:55:09 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:55:09 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:55:09 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:55:09 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:55:09 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:55:09 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:55:09 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:55:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:55:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7de6bceff024:36357 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:55:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7de6bceff024:36357 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:55:09 INFO CodeGenerator: Code generated in 124.501442 ms
23/11/29 02:55:09 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000007
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 02:55:18 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 8823 ms on 7de6bceff024 (executor driver) (1/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000006
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:55:18 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8900 ms on 7de6bceff024 (executor driver) (2/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000005
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:55:18 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 9113 ms on 7de6bceff024 (executor driver) (3/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000003
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:55:18 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 9127 ms on 7de6bceff024 (executor driver) (4/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000001
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:55:18 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 9174 ms on 7de6bceff024 (executor driver) (5/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000002
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000002_3: Committed. Elapsed time: 0 ms.
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000000
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:55:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 9218 ms on 7de6bceff024 (executor driver) (6/8)
23/11/29 02:55:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 9218 ms on 7de6bceff024 (executor driver) (7/8)
23/11/29 02:55:18 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025509990604137125852379_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025509990604137125852379_0001_m_000004
23/11/29 02:55:18 INFO SparkHadoopMapRedUtil: attempt_20231129025509990604137125852379_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:55:18 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:55:18 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 9256 ms on 7de6bceff024 (executor driver) (8/8)
23/11/29 02:55:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:55:18 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 9.308 s
23/11/29 02:55:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:55:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:55:18 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 9.315106 s
23/11/29 02:55:18 INFO FileFormatWriter: Start to commit write Job 9c8b4312-28d5-498f-a3e1-e235cc40cbd0.
23/11/29 02:55:18 INFO FileFormatWriter: Write Job 9c8b4312-28d5-498f-a3e1-e235cc40cbd0 committed. Elapsed time: 16 ms.
23/11/29 02:55:18 INFO FileFormatWriter: Finished processing stats for write job 9c8b4312-28d5-498f-a3e1-e235cc40cbd0.
23/11/29 02:55:18 INFO SparkUI: Stopped Spark web UI at http://7de6bceff024:4040
23/11/29 02:55:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:55:18 INFO MemoryStore: MemoryStore cleared
23/11/29 02:55:18 INFO BlockManager: BlockManager stopped
23/11/29 02:55:18 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:55:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:55:18 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 13
23/11/29 02:55:18 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-ca5173ed-d735-4e38-88ee-35d0f41fadf0
23/11/29 02:55:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-aae6115d-3be5-4f04-9887-8800ed5392ac
Execution 5:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-af2aadf6-a1a5-4c3d-a1cd-2370c6a07d85;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (660ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (454ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (313ms)
:: resolution report :: resolve 7035ms :: artifacts dl 1441ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-af2aadf6-a1a5-4c3d-a1cd-2370c6a07d85
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/9ms)
23/11/29 02:55:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:55:31 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:55:31 INFO ResourceUtils: ==============================================================
23/11/29 02:55:31 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:55:31 INFO ResourceUtils: ==============================================================
23/11/29 02:55:31 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:55:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:55:31 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:55:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:55:31 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:55:31 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:55:31 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:55:31 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:55:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:55:31 INFO Utils: Successfully started service 'sparkDriver' on port 33203.
23/11/29 02:55:31 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:55:31 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:55:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:55:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:55:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:55:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c613a10b-4ff4-4e1f-9702-a29a102f0e23
23/11/29 02:55:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:55:31 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:55:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:55:32 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://b2b989d3239a:33203/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://b2b989d3239a:33203/jars/org.tukaani_xz-1.9.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://b2b989d3239a:33203/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://b2b989d3239a:33203/jars/spark.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO Executor: Starting executor ID driver on host b2b989d3239a
23/11/29 02:55:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:55:32 INFO Executor: Fetching spark://b2b989d3239a:33203/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO TransportClientFactory: Successfully created connection to b2b989d3239a/172.17.0.2:33203 after 26 ms (0 ms spent in bootstraps)
23/11/29 02:55:32 INFO Utils: Fetching spark://b2b989d3239a:33203/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/fetchFileTemp12746964868178864960.tmp
23/11/29 02:55:32 INFO Executor: Adding file:/tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:55:32 INFO Executor: Fetching spark://b2b989d3239a:33203/jars/org.tukaani_xz-1.9.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO Utils: Fetching spark://b2b989d3239a:33203/jars/org.tukaani_xz-1.9.jar to /tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/fetchFileTemp2261534015291000005.tmp
23/11/29 02:55:32 INFO Executor: Adding file:/tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:55:32 INFO Executor: Fetching spark://b2b989d3239a:33203/jars/spark.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO Utils: Fetching spark://b2b989d3239a:33203/jars/spark.jar to /tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/fetchFileTemp2201022126559670405.tmp
23/11/29 02:55:32 INFO Executor: Adding file:/tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/spark.jar to class loader
23/11/29 02:55:32 INFO Executor: Fetching spark://b2b989d3239a:33203/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226531170
23/11/29 02:55:32 INFO Utils: Fetching spark://b2b989d3239a:33203/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/fetchFileTemp15108557352235308265.tmp
23/11/29 02:55:32 INFO Executor: Adding file:/tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab/userFiles-b0b882ee-e31f-49d4-acc9-ea5e8af30cd5/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:55:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41713.
23/11/29 02:55:32 INFO NettyBlockTransferService: Server created on b2b989d3239a:41713
23/11/29 02:55:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:55:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b2b989d3239a, 41713, None)
23/11/29 02:55:32 INFO BlockManagerMasterEndpoint: Registering block manager b2b989d3239a:41713 with 434.4 MiB RAM, BlockManagerId(driver, b2b989d3239a, 41713, None)
23/11/29 02:55:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b2b989d3239a, 41713, None)
23/11/29 02:55:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b2b989d3239a, 41713, None)
23/11/29 02:55:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:55:33 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:55:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:55:34 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.
23/11/29 02:55:36 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:55:36 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:55:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:55:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:55:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:55:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b2b989d3239a:41713 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:55:37 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:55:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:55:37 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:55:37 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:55:37 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:55:37 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:55:37 INFO DAGScheduler: Missing parents: List()
23/11/29 02:55:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:55:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:55:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:55:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b2b989d3239a:41713 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:55:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:55:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:55:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:55:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b2b989d3239a, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:55:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:55:38 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:55:38 INFO CodeGenerator: Code generated in 198.192324 ms
23/11/29 02:55:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:55:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 509 ms on b2b989d3239a (executor driver) (1/1)
23/11/29 02:55:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:55:38 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.631 s
23/11/29 02:55:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:55:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:55:38 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.678165 s
23/11/29 02:55:38 INFO CodeGenerator: Code generated in 16.481411 ms
23/11/29 02:55:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:55:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:55:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:55:38 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:55:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:55:38 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:55:38 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:55:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:55:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:55:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b2b989d3239a:41713 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:55:38 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:55:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:55:38 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:55:38 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:55:38 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:55:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:55:38 INFO DAGScheduler: Missing parents: List()
23/11/29 02:55:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:55:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:55:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:55:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on b2b989d3239a:41713 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:55:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:55:38 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:55:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:55:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b2b989d3239a, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (b2b989d3239a, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (b2b989d3239a, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (b2b989d3239a, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (b2b989d3239a, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (b2b989d3239a, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (b2b989d3239a, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (b2b989d3239a, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:55:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:55:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:55:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:55:38 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:55:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:55:38 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:55:38 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:55:38 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:55:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:55:39 INFO CodeGenerator: Code generated in 98.238431 ms
23/11/29 02:55:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on b2b989d3239a:41713 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:55:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on b2b989d3239a:41713 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000007
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 02:55:47 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 8251 ms on b2b989d3239a (executor driver) (1/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000002
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:55:47 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8442 ms on b2b989d3239a (executor driver) (2/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000000
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000000_1: Committed. Elapsed time: 0 ms.
23/11/29 02:55:47 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8461 ms on b2b989d3239a (executor driver) (3/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000004
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:55:47 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8477 ms on b2b989d3239a (executor driver) (4/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000005
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:55:47 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8505 ms on b2b989d3239a (executor driver) (5/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000006
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:55:47 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8557 ms on b2b989d3239a (executor driver) (6/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000003
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:55:47 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 8593 ms on b2b989d3239a (executor driver) (7/8)
23/11/29 02:55:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290255383669026236774175347_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290255383669026236774175347_0001_m_000001
23/11/29 02:55:47 INFO SparkHadoopMapRedUtil: attempt_202311290255383669026236774175347_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:55:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:55:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8612 ms on b2b989d3239a (executor driver) (8/8)
23/11/29 02:55:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:55:47 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.668 s
23/11/29 02:55:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:55:47 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.674694 s
23/11/29 02:55:47 INFO FileFormatWriter: Start to commit write Job 705ca922-73b9-4250-b2d9-4dabcdf05183.
23/11/29 02:55:47 INFO FileFormatWriter: Write Job 705ca922-73b9-4250-b2d9-4dabcdf05183 committed. Elapsed time: 15 ms.
23/11/29 02:55:47 INFO FileFormatWriter: Finished processing stats for write job 705ca922-73b9-4250-b2d9-4dabcdf05183.
23/11/29 02:55:47 INFO SparkUI: Stopped Spark web UI at http://b2b989d3239a:4040
23/11/29 02:55:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:55:47 INFO MemoryStore: MemoryStore cleared
23/11/29 02:55:47 INFO BlockManager: BlockManager stopped
23/11/29 02:55:47 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:55:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:55:47 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 13
23/11/29 02:55:47 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:55:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-e64ab970-5136-41f6-8165-5be17b624f8d
23/11/29 02:55:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-55b51980-1a57-4cd2-85f1-e4168f0f34ab
Execution 6:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f7c36fd5-6b6f-4117-8219-ccab0e6ecf1c;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (851ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2344ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (401ms)
:: resolution report :: resolve 7813ms :: artifacts dl 3615ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-f7c36fd5-6b6f-4117-8219-ccab0e6ecf1c
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/12ms)
23/11/29 02:56:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:56:02 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:56:02 INFO ResourceUtils: ==============================================================
23/11/29 02:56:02 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:56:02 INFO ResourceUtils: ==============================================================
23/11/29 02:56:02 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:56:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:56:02 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:56:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:56:02 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:56:02 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:56:02 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:56:02 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:56:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:56:03 INFO Utils: Successfully started service 'sparkDriver' on port 46515.
23/11/29 02:56:03 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:56:03 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:56:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:56:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:56:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:56:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9128d300-5e5a-4de8-8cb1-653f5171a230
23/11/29 02:56:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:56:03 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:56:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:56:03 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://1a28f6111de1:46515/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226562720
23/11/29 02:56:03 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://1a28f6111de1:46515/jars/org.tukaani_xz-1.9.jar with timestamp 1701226562720
23/11/29 02:56:03 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://1a28f6111de1:46515/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226562720
23/11/29 02:56:03 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://1a28f6111de1:46515/jars/spark.jar with timestamp 1701226562720
23/11/29 02:56:03 INFO Executor: Starting executor ID driver on host 1a28f6111de1
23/11/29 02:56:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:56:03 INFO Executor: Fetching spark://1a28f6111de1:46515/jars/spark.jar with timestamp 1701226562720
23/11/29 02:56:03 INFO TransportClientFactory: Successfully created connection to 1a28f6111de1/172.17.0.2:46515 after 28 ms (0 ms spent in bootstraps)
23/11/29 02:56:03 INFO Utils: Fetching spark://1a28f6111de1:46515/jars/spark.jar to /tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/fetchFileTemp14964835868668027978.tmp
23/11/29 02:56:04 INFO Executor: Adding file:/tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/spark.jar to class loader
23/11/29 02:56:04 INFO Executor: Fetching spark://1a28f6111de1:46515/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226562720
23/11/29 02:56:04 INFO Utils: Fetching spark://1a28f6111de1:46515/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/fetchFileTemp9537178947578594443.tmp
23/11/29 02:56:04 INFO Executor: Adding file:/tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:56:04 INFO Executor: Fetching spark://1a28f6111de1:46515/jars/org.tukaani_xz-1.9.jar with timestamp 1701226562720
23/11/29 02:56:04 INFO Utils: Fetching spark://1a28f6111de1:46515/jars/org.tukaani_xz-1.9.jar to /tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/fetchFileTemp5177714837121445913.tmp
23/11/29 02:56:04 INFO Executor: Adding file:/tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:56:04 INFO Executor: Fetching spark://1a28f6111de1:46515/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226562720
23/11/29 02:56:04 INFO Utils: Fetching spark://1a28f6111de1:46515/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/fetchFileTemp2892077305682400703.tmp
23/11/29 02:56:04 INFO Executor: Adding file:/tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f/userFiles-6f91d5ab-404b-4835-9541-bbc6c7daffae/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:56:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34515.
23/11/29 02:56:04 INFO NettyBlockTransferService: Server created on 1a28f6111de1:34515
23/11/29 02:56:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:56:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1a28f6111de1, 34515, None)
23/11/29 02:56:04 INFO BlockManagerMasterEndpoint: Registering block manager 1a28f6111de1:34515 with 434.4 MiB RAM, BlockManagerId(driver, 1a28f6111de1, 34515, None)
23/11/29 02:56:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1a28f6111de1, 34515, None)
23/11/29 02:56:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1a28f6111de1, 34515, None)
23/11/29 02:56:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:56:04 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:56:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:56:05 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
23/11/29 02:56:08 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:56:08 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:56:08 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:56:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:56:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:56:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1a28f6111de1:34515 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:56:08 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:56:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:56:09 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:56:09 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:56:09 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:56:09 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:56:09 INFO DAGScheduler: Missing parents: List()
23/11/29 02:56:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:56:09 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:56:09 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:56:09 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1a28f6111de1:34515 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:56:09 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:56:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:56:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:56:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1a28f6111de1, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:56:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:56:09 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:56:09 INFO CodeGenerator: Code generated in 142.182604 ms
23/11/29 02:56:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:56:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 455 ms on 1a28f6111de1 (executor driver) (1/1)
23/11/29 02:56:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:56:09 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.569 s
23/11/29 02:56:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:56:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:56:09 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.619190 s
23/11/29 02:56:09 INFO CodeGenerator: Code generated in 21.479746 ms
23/11/29 02:56:09 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:56:09 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:56:09 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:56:09 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:56:09 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:56:09 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:56:09 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:56:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:56:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:56:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1a28f6111de1:34515 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:56:10 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:56:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:56:10 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:56:10 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:56:10 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:56:10 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:56:10 INFO DAGScheduler: Missing parents: List()
23/11/29 02:56:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:56:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:56:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:56:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1a28f6111de1:34515 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:56:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:56:10 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:56:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:56:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (1a28f6111de1, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (1a28f6111de1, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (1a28f6111de1, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (1a28f6111de1, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (1a28f6111de1, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (1a28f6111de1, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (1a28f6111de1, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (1a28f6111de1, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:56:10 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:56:10 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:56:10 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:56:10 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:56:10 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:56:10 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:56:10 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:56:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1a28f6111de1:34515 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:56:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1a28f6111de1:34515 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:56:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:10 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:10 INFO CodeGenerator: Code generated in 103.129624 ms
23/11/29 02:56:10 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:56:17 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000007
23/11/29 02:56:17 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 02:56:17 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:56:17 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7701 ms on 1a28f6111de1 (executor driver) (1/8)
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000003
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:56:18 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 7945 ms on 1a28f6111de1 (executor driver) (2/8)
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000001
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:56:18 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000000
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:56:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8010 ms on 1a28f6111de1 (executor driver) (3/8)
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000005
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000005_6: Committed. Elapsed time: 0 ms.
23/11/29 02:56:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8013 ms on 1a28f6111de1 (executor driver) (4/8)
23/11/29 02:56:18 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8011 ms on 1a28f6111de1 (executor driver) (5/8)
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000006
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:56:18 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8068 ms on 1a28f6111de1 (executor driver) (6/8)
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000004
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:56:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256108679023326999222512_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256108679023326999222512_0001_m_000002
23/11/29 02:56:18 INFO SparkHadoopMapRedUtil: attempt_202311290256108679023326999222512_0001_m_000002_3: Committed. Elapsed time: 0 ms.
23/11/29 02:56:18 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:56:18 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8133 ms on 1a28f6111de1 (executor driver) (7/8)
23/11/29 02:56:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8136 ms on 1a28f6111de1 (executor driver) (8/8)
23/11/29 02:56:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:56:18 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.205 s
23/11/29 02:56:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:56:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:56:18 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.213630 s
23/11/29 02:56:18 INFO FileFormatWriter: Start to commit write Job 09b678d3-3cb3-4973-888c-10634b10ca39.
23/11/29 02:56:18 INFO FileFormatWriter: Write Job 09b678d3-3cb3-4973-888c-10634b10ca39 committed. Elapsed time: 15 ms.
23/11/29 02:56:18 INFO FileFormatWriter: Finished processing stats for write job 09b678d3-3cb3-4973-888c-10634b10ca39.
23/11/29 02:56:18 INFO SparkUI: Stopped Spark web UI at http://1a28f6111de1:4040
23/11/29 02:56:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:56:18 INFO MemoryStore: MemoryStore cleared
23/11/29 02:56:18 INFO BlockManager: BlockManager stopped
23/11/29 02:56:18 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:56:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:56:18 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 12
23/11/29 02:56:18 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:56:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-12c74cc6-a02d-49c9-bae6-e3ffd800404f
23/11/29 02:56:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-c96aa9e0-e002-4717-88e2-5f151df05877
Execution 7:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-3ab04946-4b82-4016-9903-0ea377f52204;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1229ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (674ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (379ms)
:: resolution report :: resolve 7709ms :: artifacts dl 2302ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-3ab04946-4b82-4016-9903-0ea377f52204
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/10ms)
23/11/29 02:56:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:56:32 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:56:32 INFO ResourceUtils: ==============================================================
23/11/29 02:56:32 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:56:32 INFO ResourceUtils: ==============================================================
23/11/29 02:56:32 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:56:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:56:32 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:56:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:56:32 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:56:32 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:56:32 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:56:32 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:56:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:56:32 INFO Utils: Successfully started service 'sparkDriver' on port 45715.
23/11/29 02:56:32 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:56:32 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:56:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:56:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:56:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:56:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3ea0f810-6684-462d-b20c-1cc6d4da3e0f
23/11/29 02:56:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:56:32 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:56:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:56:33 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://0ebbb64cd068:45715/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://0ebbb64cd068:45715/jars/org.tukaani_xz-1.9.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://0ebbb64cd068:45715/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://0ebbb64cd068:45715/jars/spark.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO Executor: Starting executor ID driver on host 0ebbb64cd068
23/11/29 02:56:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:56:33 INFO Executor: Fetching spark://0ebbb64cd068:45715/jars/spark.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO TransportClientFactory: Successfully created connection to 0ebbb64cd068/172.17.0.2:45715 after 36 ms (0 ms spent in bootstraps)
23/11/29 02:56:33 INFO Utils: Fetching spark://0ebbb64cd068:45715/jars/spark.jar to /tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/fetchFileTemp3888354983640446930.tmp
23/11/29 02:56:33 INFO Executor: Adding file:/tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/spark.jar to class loader
23/11/29 02:56:33 INFO Executor: Fetching spark://0ebbb64cd068:45715/jars/org.tukaani_xz-1.9.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO Utils: Fetching spark://0ebbb64cd068:45715/jars/org.tukaani_xz-1.9.jar to /tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/fetchFileTemp3342869628770924683.tmp
23/11/29 02:56:33 INFO Executor: Adding file:/tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:56:33 INFO Executor: Fetching spark://0ebbb64cd068:45715/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO Utils: Fetching spark://0ebbb64cd068:45715/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/fetchFileTemp14936305645416388551.tmp
23/11/29 02:56:33 INFO Executor: Adding file:/tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:56:33 INFO Executor: Fetching spark://0ebbb64cd068:45715/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226592224
23/11/29 02:56:33 INFO Utils: Fetching spark://0ebbb64cd068:45715/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/fetchFileTemp4161216185570070980.tmp
23/11/29 02:56:33 INFO Executor: Adding file:/tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa/userFiles-62bbb46e-5d9d-476e-ac2e-d19af047769b/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:56:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45007.
23/11/29 02:56:33 INFO NettyBlockTransferService: Server created on 0ebbb64cd068:45007
23/11/29 02:56:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:56:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 0ebbb64cd068, 45007, None)
23/11/29 02:56:33 INFO BlockManagerMasterEndpoint: Registering block manager 0ebbb64cd068:45007 with 434.4 MiB RAM, BlockManagerId(driver, 0ebbb64cd068, 45007, None)
23/11/29 02:56:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 0ebbb64cd068, 45007, None)
23/11/29 02:56:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 0ebbb64cd068, 45007, None)
23/11/29 02:56:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:56:34 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:56:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:56:35 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
23/11/29 02:56:37 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:56:37 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:56:37 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:56:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:56:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:56:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 0ebbb64cd068:45007 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:56:38 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:56:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:56:38 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:56:38 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:56:38 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:56:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:56:38 INFO DAGScheduler: Missing parents: List()
23/11/29 02:56:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:56:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:56:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:56:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 0ebbb64cd068:45007 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:56:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:56:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:56:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:56:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (0ebbb64cd068, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:56:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:56:38 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:56:38 INFO CodeGenerator: Code generated in 141.791083 ms
23/11/29 02:56:39 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:56:39 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 430 ms on 0ebbb64cd068 (executor driver) (1/1)
23/11/29 02:56:39 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:56:39 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.533 s
23/11/29 02:56:39 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:56:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:56:39 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.574611 s
23/11/29 02:56:39 INFO CodeGenerator: Code generated in 15.34199 ms
23/11/29 02:56:39 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:56:39 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:56:39 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:56:39 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:56:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:56:39 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:56:39 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:56:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:56:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 0ebbb64cd068:45007 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:56:39 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:56:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:56:39 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:56:39 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:56:39 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:56:39 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:56:39 INFO DAGScheduler: Missing parents: List()
23/11/29 02:56:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:56:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:56:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 0ebbb64cd068:45007 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:56:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:56:39 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:56:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:56:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (0ebbb64cd068, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (0ebbb64cd068, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (0ebbb64cd068, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (0ebbb64cd068, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (0ebbb64cd068, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (0ebbb64cd068, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (0ebbb64cd068, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (0ebbb64cd068, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:56:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:56:39 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:56:39 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:56:39 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:56:39 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:56:39 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:56:39 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:56:39 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:56:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:56:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 0ebbb64cd068:45007 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:56:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 0ebbb64cd068:45007 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:56:39 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:56:39 INFO CodeGenerator: Code generated in 86.461288 ms
23/11/29 02:56:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000007
23/11/29 02:56:47 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000007_8: Committed. Elapsed time: 1 ms.
23/11/29 02:56:47 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:56:47 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 8308 ms on 0ebbb64cd068 (executor driver) (1/8)
23/11/29 02:56:47 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000001
23/11/29 02:56:47 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:56:47 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:56:47 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8542 ms on 0ebbb64cd068 (executor driver) (2/8)
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000004
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000004_5: Committed. Elapsed time: 17 ms.
23/11/29 02:56:48 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8634 ms on 0ebbb64cd068 (executor driver) (3/8)
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000002
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:56:48 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8640 ms on 0ebbb64cd068 (executor driver) (4/8)
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000005
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000005_6: Committed. Elapsed time: 0 ms.
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000006
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000006_7: Committed. Elapsed time: 0 ms.
23/11/29 02:56:48 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8664 ms on 0ebbb64cd068 (executor driver) (5/8)
23/11/29 02:56:48 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8665 ms on 0ebbb64cd068 (executor driver) (6/8)
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000000
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:56:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8684 ms on 0ebbb64cd068 (executor driver) (7/8)
23/11/29 02:56:48 INFO FileOutputCommitter: Saved output of task 'attempt_202311290256395033255841284783146_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290256395033255841284783146_0001_m_000003
23/11/29 02:56:48 INFO SparkHadoopMapRedUtil: attempt_202311290256395033255841284783146_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:56:48 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:56:48 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 8733 ms on 0ebbb64cd068 (executor driver) (8/8)
23/11/29 02:56:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:56:48 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.786 s
23/11/29 02:56:48 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:56:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:56:48 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.792238 s
23/11/29 02:56:48 INFO FileFormatWriter: Start to commit write Job 68491e0c-eb86-4fc3-9df5-4a78b7e75a8b.
23/11/29 02:56:48 INFO FileFormatWriter: Write Job 68491e0c-eb86-4fc3-9df5-4a78b7e75a8b committed. Elapsed time: 24 ms.
23/11/29 02:56:48 INFO FileFormatWriter: Finished processing stats for write job 68491e0c-eb86-4fc3-9df5-4a78b7e75a8b.
23/11/29 02:56:48 INFO SparkUI: Stopped Spark web UI at http://0ebbb64cd068:4040
23/11/29 02:56:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:56:48 INFO MemoryStore: MemoryStore cleared
23/11/29 02:56:48 INFO BlockManager: BlockManager stopped
23/11/29 02:56:48 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:56:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:56:48 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 13
23/11/29 02:56:48 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:56:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-64a41270-bac1-4be5-b769-720fe23434a0
23/11/29 02:56:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-60b0ae0e-9768-4a56-99c3-277d1fcaa1fa
Execution 8:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f065e3b3-f413-4419-8ef6-d479699b09fc;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1132ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (830ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (544ms)
:: resolution report :: resolve 7458ms :: artifacts dl 2521ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-f065e3b3-f413-4419-8ef6-d479699b09fc
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/14ms)
23/11/29 02:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:57:02 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:57:02 INFO ResourceUtils: ==============================================================
23/11/29 02:57:02 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:57:02 INFO ResourceUtils: ==============================================================
23/11/29 02:57:02 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:57:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:57:02 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:57:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:57:02 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:57:02 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:57:02 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:57:02 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:57:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:57:02 INFO Utils: Successfully started service 'sparkDriver' on port 43655.
23/11/29 02:57:02 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:57:02 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:57:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:57:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:57:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:57:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-021181fe-8847-4635-a27a-ebd18dfd42bf
23/11/29 02:57:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:57:02 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:57:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:57:02 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://4408a1d44c3e:43655/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226622087
23/11/29 02:57:02 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://4408a1d44c3e:43655/jars/org.tukaani_xz-1.9.jar with timestamp 1701226622087
23/11/29 02:57:02 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://4408a1d44c3e:43655/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226622087
23/11/29 02:57:02 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://4408a1d44c3e:43655/jars/spark.jar with timestamp 1701226622087
23/11/29 02:57:03 INFO Executor: Starting executor ID driver on host 4408a1d44c3e
23/11/29 02:57:03 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:57:03 INFO Executor: Fetching spark://4408a1d44c3e:43655/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226622087
23/11/29 02:57:03 INFO TransportClientFactory: Successfully created connection to 4408a1d44c3e/172.17.0.2:43655 after 25 ms (0 ms spent in bootstraps)
23/11/29 02:57:03 INFO Utils: Fetching spark://4408a1d44c3e:43655/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/fetchFileTemp17497611456889202158.tmp
23/11/29 02:57:03 INFO Executor: Adding file:/tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:57:03 INFO Executor: Fetching spark://4408a1d44c3e:43655/jars/spark.jar with timestamp 1701226622087
23/11/29 02:57:03 INFO Utils: Fetching spark://4408a1d44c3e:43655/jars/spark.jar to /tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/fetchFileTemp10031316303017509717.tmp
23/11/29 02:57:03 INFO Executor: Adding file:/tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/spark.jar to class loader
23/11/29 02:57:03 INFO Executor: Fetching spark://4408a1d44c3e:43655/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226622087
23/11/29 02:57:03 INFO Utils: Fetching spark://4408a1d44c3e:43655/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/fetchFileTemp13357125011442704402.tmp
23/11/29 02:57:03 INFO Executor: Adding file:/tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:57:03 INFO Executor: Fetching spark://4408a1d44c3e:43655/jars/org.tukaani_xz-1.9.jar with timestamp 1701226622087
23/11/29 02:57:03 INFO Utils: Fetching spark://4408a1d44c3e:43655/jars/org.tukaani_xz-1.9.jar to /tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/fetchFileTemp7095385359464256070.tmp
23/11/29 02:57:03 INFO Executor: Adding file:/tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2/userFiles-6b91c4b8-fbeb-499b-81ee-328ed12900b7/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:57:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35985.
23/11/29 02:57:03 INFO NettyBlockTransferService: Server created on 4408a1d44c3e:35985
23/11/29 02:57:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:57:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4408a1d44c3e, 35985, None)
23/11/29 02:57:03 INFO BlockManagerMasterEndpoint: Registering block manager 4408a1d44c3e:35985 with 434.4 MiB RAM, BlockManagerId(driver, 4408a1d44c3e, 35985, None)
23/11/29 02:57:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4408a1d44c3e, 35985, None)
23/11/29 02:57:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4408a1d44c3e, 35985, None)
23/11/29 02:57:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:57:03 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:57:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:57:04 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.
23/11/29 02:57:06 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:57:06 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:57:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:57:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:57:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:57:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4408a1d44c3e:35985 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:57:07 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:57:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:57:07 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:57:07 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:57:07 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:57:07 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:57:07 INFO DAGScheduler: Missing parents: List()
23/11/29 02:57:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:57:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:57:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:57:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4408a1d44c3e:35985 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:57:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:57:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:57:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:57:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4408a1d44c3e, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:57:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:57:07 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:57:08 INFO CodeGenerator: Code generated in 161.263029 ms
23/11/29 02:57:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:57:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 435 ms on 4408a1d44c3e (executor driver) (1/1)
23/11/29 02:57:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:57:08 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.535 s
23/11/29 02:57:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:57:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:57:08 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.578499 s
23/11/29 02:57:08 INFO CodeGenerator: Code generated in 19.716516 ms
23/11/29 02:57:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:57:08 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:57:08 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:57:08 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:57:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:57:08 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:57:08 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:57:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:57:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4408a1d44c3e:35985 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:57:08 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:57:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:57:08 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:57:08 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:57:08 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:57:08 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:57:08 INFO DAGScheduler: Missing parents: List()
23/11/29 02:57:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:57:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:57:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:57:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4408a1d44c3e:35985 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:57:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:57:08 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:57:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:57:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4408a1d44c3e, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (4408a1d44c3e, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (4408a1d44c3e, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (4408a1d44c3e, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (4408a1d44c3e, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (4408a1d44c3e, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (4408a1d44c3e, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (4408a1d44c3e, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:57:08 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:57:08 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:57:08 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:57:08 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:57:08 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:57:08 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:57:08 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:57:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4408a1d44c3e:35985 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4408a1d44c3e:35985 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:57:08 INFO CodeGenerator: Code generated in 96.858444 ms
23/11/29 02:57:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000007
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000007_8: Committed. Elapsed time: 7 ms.
23/11/29 02:57:16 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7694 ms on 4408a1d44c3e (executor driver) (1/8)
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000005
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:57:16 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8098 ms on 4408a1d44c3e (executor driver) (2/8)
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000000
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:57:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8169 ms on 4408a1d44c3e (executor driver) (3/8)
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000001
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:57:16 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8218 ms on 4408a1d44c3e (executor driver) (4/8)
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000002
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:57:16 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8227 ms on 4408a1d44c3e (executor driver) (5/8)
23/11/29 02:57:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000006
23/11/29 02:57:16 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000006_7: Committed. Elapsed time: 0 ms.
23/11/29 02:57:16 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:57:16 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8306 ms on 4408a1d44c3e (executor driver) (6/8)
23/11/29 02:57:17 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000004
23/11/29 02:57:17 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:57:17 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:57:17 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8353 ms on 4408a1d44c3e (executor driver) (7/8)
23/11/29 02:57:17 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257088675663903957456685_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257088675663903957456685_0001_m_000003
23/11/29 02:57:17 INFO SparkHadoopMapRedUtil: attempt_202311290257088675663903957456685_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:57:17 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:57:17 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 8420 ms on 4408a1d44c3e (executor driver) (8/8)
23/11/29 02:57:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:57:17 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.477 s
23/11/29 02:57:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:57:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:57:17 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.483199 s
23/11/29 02:57:17 INFO FileFormatWriter: Start to commit write Job 502407ea-c576-412a-b0bb-31579b48e846.
23/11/29 02:57:17 INFO FileFormatWriter: Write Job 502407ea-c576-412a-b0bb-31579b48e846 committed. Elapsed time: 14 ms.
23/11/29 02:57:17 INFO FileFormatWriter: Finished processing stats for write job 502407ea-c576-412a-b0bb-31579b48e846.
23/11/29 02:57:17 INFO SparkUI: Stopped Spark web UI at http://4408a1d44c3e:4040
23/11/29 02:57:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:57:17 INFO MemoryStore: MemoryStore cleared
23/11/29 02:57:17 INFO BlockManager: BlockManager stopped
23/11/29 02:57:17 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:57:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:57:17 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 12
23/11/29 02:57:17 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:57:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-da85380e-f231-4f9a-9fc9-a0bb461c11c2
23/11/29 02:57:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed0bb06f-d8da-45a4-b707-36b466883d93
Execution 9:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-26608c7e-230f-4d7f-9c79-cabd56a20feb;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (925ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (422ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (600ms)
:: resolution report :: resolve 9124ms :: artifacts dl 1966ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-26608c7e-230f-4d7f-9c79-cabd56a20feb
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 02:57:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:57:31 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:57:32 INFO ResourceUtils: ==============================================================
23/11/29 02:57:32 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:57:32 INFO ResourceUtils: ==============================================================
23/11/29 02:57:32 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:57:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:57:32 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:57:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:57:32 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:57:32 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:57:32 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:57:32 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:57:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:57:32 INFO Utils: Successfully started service 'sparkDriver' on port 46539.
23/11/29 02:57:32 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:57:32 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:57:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:57:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:57:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:57:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3a7137c9-5179-4506-ac6f-000659d58955
23/11/29 02:57:32 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:57:32 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:57:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:57:32 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://9a95eef8a216:46539/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226651980
23/11/29 02:57:32 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://9a95eef8a216:46539/jars/org.tukaani_xz-1.9.jar with timestamp 1701226651980
23/11/29 02:57:32 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://9a95eef8a216:46539/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226651980
23/11/29 02:57:32 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://9a95eef8a216:46539/jars/spark.jar with timestamp 1701226651980
23/11/29 02:57:32 INFO Executor: Starting executor ID driver on host 9a95eef8a216
23/11/29 02:57:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:57:32 INFO Executor: Fetching spark://9a95eef8a216:46539/jars/spark.jar with timestamp 1701226651980
23/11/29 02:57:33 INFO TransportClientFactory: Successfully created connection to 9a95eef8a216/172.17.0.2:46539 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:57:33 INFO Utils: Fetching spark://9a95eef8a216:46539/jars/spark.jar to /tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/fetchFileTemp9570037088415029428.tmp
23/11/29 02:57:33 INFO Executor: Adding file:/tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/spark.jar to class loader
23/11/29 02:57:33 INFO Executor: Fetching spark://9a95eef8a216:46539/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226651980
23/11/29 02:57:33 INFO Utils: Fetching spark://9a95eef8a216:46539/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/fetchFileTemp16103242825311695001.tmp
23/11/29 02:57:33 INFO Executor: Adding file:/tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:57:33 INFO Executor: Fetching spark://9a95eef8a216:46539/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226651980
23/11/29 02:57:33 INFO Utils: Fetching spark://9a95eef8a216:46539/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/fetchFileTemp16332137258995735576.tmp
23/11/29 02:57:33 INFO Executor: Adding file:/tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:57:33 INFO Executor: Fetching spark://9a95eef8a216:46539/jars/org.tukaani_xz-1.9.jar with timestamp 1701226651980
23/11/29 02:57:33 INFO Utils: Fetching spark://9a95eef8a216:46539/jars/org.tukaani_xz-1.9.jar to /tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/fetchFileTemp16477823895768188113.tmp
23/11/29 02:57:33 INFO Executor: Adding file:/tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308/userFiles-f7f8c9a1-6ebf-4de4-86c7-da5cedcfc9f3/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:57:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44027.
23/11/29 02:57:33 INFO NettyBlockTransferService: Server created on 9a95eef8a216:44027
23/11/29 02:57:33 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:57:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 9a95eef8a216, 44027, None)
23/11/29 02:57:33 INFO BlockManagerMasterEndpoint: Registering block manager 9a95eef8a216:44027 with 434.4 MiB RAM, BlockManagerId(driver, 9a95eef8a216, 44027, None)
23/11/29 02:57:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 9a95eef8a216, 44027, None)
23/11/29 02:57:33 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 9a95eef8a216, 44027, None)
23/11/29 02:57:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:57:33 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:57:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:57:34 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.
23/11/29 02:57:36 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:57:36 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:57:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:57:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:57:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:57:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 9a95eef8a216:44027 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:57:37 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:57:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:57:37 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:57:37 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:57:37 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:57:37 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:57:37 INFO DAGScheduler: Missing parents: List()
23/11/29 02:57:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:57:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:57:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:57:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 9a95eef8a216:44027 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:57:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:57:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:57:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:57:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (9a95eef8a216, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:57:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:57:37 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:57:37 INFO CodeGenerator: Code generated in 130.463698 ms
23/11/29 02:57:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:57:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 438 ms on 9a95eef8a216 (executor driver) (1/1)
23/11/29 02:57:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:57:37 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.533 s
23/11/29 02:57:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:57:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:57:37 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.576688 s
23/11/29 02:57:37 INFO CodeGenerator: Code generated in 16.166052 ms
23/11/29 02:57:38 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:57:38 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:57:38 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:57:38 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:57:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:57:38 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:57:38 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:57:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:57:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 9a95eef8a216:44027 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:57:38 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:57:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:57:38 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:57:38 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:57:38 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:57:38 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:57:38 INFO DAGScheduler: Missing parents: List()
23/11/29 02:57:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:57:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:57:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:57:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 9a95eef8a216:44027 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:57:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:57:38 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:57:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:57:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (9a95eef8a216, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (9a95eef8a216, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (9a95eef8a216, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (9a95eef8a216, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (9a95eef8a216, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (9a95eef8a216, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (9a95eef8a216, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (9a95eef8a216, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:57:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:57:38 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:57:38 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:57:38 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:57:38 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:57:38 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:57:38 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:57:38 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:57:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 9a95eef8a216:44027 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:57:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 9a95eef8a216:44027 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:57:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:57:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:57:38 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:57:38 INFO CodeGenerator: Code generated in 130.921383 ms
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000007
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000007_8: Committed. Elapsed time: 3 ms.
23/11/29 02:57:46 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7673 ms on 9a95eef8a216 (executor driver) (1/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000002
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000002_3: Committed. Elapsed time: 10 ms.
23/11/29 02:57:46 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 7932 ms on 9a95eef8a216 (executor driver) (2/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000006
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:57:46 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8013 ms on 9a95eef8a216 (executor driver) (3/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000004
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:57:46 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8094 ms on 9a95eef8a216 (executor driver) (4/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000000
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000000_1: Committed. Elapsed time: 0 ms.
23/11/29 02:57:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8130 ms on 9a95eef8a216 (executor driver) (5/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000003
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:57:46 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 8134 ms on 9a95eef8a216 (executor driver) (6/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000001
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:57:46 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8163 ms on 9a95eef8a216 (executor driver) (7/8)
23/11/29 02:57:46 INFO FileOutputCommitter: Saved output of task 'attempt_202311290257386839934980727020459_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290257386839934980727020459_0001_m_000005
23/11/29 02:57:46 INFO SparkHadoopMapRedUtil: attempt_202311290257386839934980727020459_0001_m_000005_6: Committed. Elapsed time: 0 ms.
23/11/29 02:57:46 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:57:46 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8241 ms on 9a95eef8a216 (executor driver) (8/8)
23/11/29 02:57:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:57:46 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.290 s
23/11/29 02:57:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:57:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:57:46 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.295572 s
23/11/29 02:57:46 INFO FileFormatWriter: Start to commit write Job fd846d55-191f-4cae-9d1b-1d31e4f43188.
23/11/29 02:57:46 INFO FileFormatWriter: Write Job fd846d55-191f-4cae-9d1b-1d31e4f43188 committed. Elapsed time: 13 ms.
23/11/29 02:57:46 INFO FileFormatWriter: Finished processing stats for write job fd846d55-191f-4cae-9d1b-1d31e4f43188.
23/11/29 02:57:46 INFO SparkUI: Stopped Spark web UI at http://9a95eef8a216:4040
23/11/29 02:57:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:57:46 INFO MemoryStore: MemoryStore cleared
23/11/29 02:57:46 INFO BlockManager: BlockManager stopped
23/11/29 02:57:46 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:57:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:57:46 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 12
23/11/29 02:57:46 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:57:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-16eaeade-a8a3-4d8c-8195-6b5fc5924515
23/11/29 02:57:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed64ad71-76d0-401f-8041-607e0461f308
Execution 10:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-b85dc18e-deae-4d67-bb1a-97b94d772f7f;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (651ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (1907ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (324ms)
:: resolution report :: resolve 7355ms :: artifacts dl 2890ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-b85dc18e-deae-4d67-bb1a-97b94d772f7f
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/12ms)
23/11/29 02:58:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:58:00 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:58:00 INFO ResourceUtils: ==============================================================
23/11/29 02:58:00 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:58:00 INFO ResourceUtils: ==============================================================
23/11/29 02:58:00 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:58:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:58:00 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:58:00 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:58:00 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:58:00 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:58:00 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:58:00 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:58:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:58:01 INFO Utils: Successfully started service 'sparkDriver' on port 34211.
23/11/29 02:58:01 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:58:01 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:58:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:58:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:58:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:58:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f53504f8-8472-4af1-814f-fa53ab5428c0
23/11/29 02:58:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:58:01 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:58:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:58:01 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://a9cad0e294b6:34211/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226680782
23/11/29 02:58:01 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://a9cad0e294b6:34211/jars/org.tukaani_xz-1.9.jar with timestamp 1701226680782
23/11/29 02:58:01 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a9cad0e294b6:34211/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226680782
23/11/29 02:58:01 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://a9cad0e294b6:34211/jars/spark.jar with timestamp 1701226680782
23/11/29 02:58:01 INFO Executor: Starting executor ID driver on host a9cad0e294b6
23/11/29 02:58:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:58:01 INFO Executor: Fetching spark://a9cad0e294b6:34211/jars/spark.jar with timestamp 1701226680782
23/11/29 02:58:01 INFO TransportClientFactory: Successfully created connection to a9cad0e294b6/172.17.0.2:34211 after 33 ms (0 ms spent in bootstraps)
23/11/29 02:58:01 INFO Utils: Fetching spark://a9cad0e294b6:34211/jars/spark.jar to /tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/fetchFileTemp4391795037506237519.tmp
23/11/29 02:58:02 INFO Executor: Adding file:/tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/spark.jar to class loader
23/11/29 02:58:02 INFO Executor: Fetching spark://a9cad0e294b6:34211/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226680782
23/11/29 02:58:02 INFO Utils: Fetching spark://a9cad0e294b6:34211/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/fetchFileTemp11804648639350229417.tmp
23/11/29 02:58:02 INFO Executor: Adding file:/tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:58:02 INFO Executor: Fetching spark://a9cad0e294b6:34211/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226680782
23/11/29 02:58:02 INFO Utils: Fetching spark://a9cad0e294b6:34211/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/fetchFileTemp4370278272679465589.tmp
23/11/29 02:58:02 INFO Executor: Adding file:/tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:58:02 INFO Executor: Fetching spark://a9cad0e294b6:34211/jars/org.tukaani_xz-1.9.jar with timestamp 1701226680782
23/11/29 02:58:02 INFO Utils: Fetching spark://a9cad0e294b6:34211/jars/org.tukaani_xz-1.9.jar to /tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/fetchFileTemp6920799608194151809.tmp
23/11/29 02:58:02 INFO Executor: Adding file:/tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728/userFiles-ce97a476-78ca-409d-b18e-367a2c5f82bd/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:58:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33747.
23/11/29 02:58:02 INFO NettyBlockTransferService: Server created on a9cad0e294b6:33747
23/11/29 02:58:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:58:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a9cad0e294b6, 33747, None)
23/11/29 02:58:02 INFO BlockManagerMasterEndpoint: Registering block manager a9cad0e294b6:33747 with 434.4 MiB RAM, BlockManagerId(driver, a9cad0e294b6, 33747, None)
23/11/29 02:58:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a9cad0e294b6, 33747, None)
23/11/29 02:58:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a9cad0e294b6, 33747, None)
23/11/29 02:58:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:58:02 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:58:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:58:03 INFO InMemoryFileIndex: It took 45 ms to list leaf files for 1 paths.
23/11/29 02:58:06 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:58:06 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:58:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:58:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:58:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:58:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a9cad0e294b6:33747 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:58:06 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:58:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:58:06 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:58:06 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:58:06 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:58:06 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:58:06 INFO DAGScheduler: Missing parents: List()
23/11/29 02:58:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:58:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:58:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:58:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a9cad0e294b6:33747 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:58:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:58:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:58:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:58:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a9cad0e294b6, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:58:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:58:07 INFO CodeGenerator: Code generated in 170.028745 ms
23/11/29 02:58:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:58:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 445 ms on a9cad0e294b6 (executor driver) (1/1)
23/11/29 02:58:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:58:07 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.554 s
23/11/29 02:58:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:58:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:58:07 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.598501 s
23/11/29 02:58:07 INFO CodeGenerator: Code generated in 15.901184 ms
23/11/29 02:58:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:58:07 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:58:07 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:58:07 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:58:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:58:07 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:58:07 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:58:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:58:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on a9cad0e294b6:33747 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:58:07 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:58:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 33306316 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:58:07 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:58:07 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 8 output partitions
23/11/29 02:58:07 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:58:07 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:58:07 INFO DAGScheduler: Missing parents: List()
23/11/29 02:58:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:58:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:58:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:58:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on a9cad0e294b6:33747 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:58:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:58:07 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
23/11/29 02:58:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
23/11/29 02:58:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (a9cad0e294b6, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (a9cad0e294b6, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (a9cad0e294b6, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (a9cad0e294b6, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (a9cad0e294b6, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (a9cad0e294b6, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (a9cad0e294b6, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (a9cad0e294b6, executor driver, partition 7, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:58:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:58:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:58:07 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:58:07 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:58:07 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:58:07 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
23/11/29 02:58:07 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:58:07 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 99918948-133225264, partition values: [empty row]
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 166531580-199837896, partition values: [empty row]
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 233144212-262256224, partition values: [empty row]
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 133225264-166531580, partition values: [empty row]
23/11/29 02:58:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:58:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 199837896-233144212, partition values: [empty row]
23/11/29 02:58:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on a9cad0e294b6:33747 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 66612632-99918948, partition values: [empty row]
23/11/29 02:58:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:58:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 33306316-66612632, partition values: [empty row]
23/11/29 02:58:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a9cad0e294b6:33747 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:58:08 INFO CodeGenerator: Code generated in 96.07366 ms
23/11/29 02:58:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-33306316, partition values: [empty row]
23/11/29 02:58:15 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000007_8' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000007
23/11/29 02:58:15 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000007_8: Committed. Elapsed time: 3 ms.
23/11/29 02:58:15 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2541 bytes result sent to driver
23/11/29 02:58:15 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 7915 ms on a9cad0e294b6 (executor driver) (1/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000002
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000002_3: Committed. Elapsed time: 6 ms.
23/11/29 02:58:16 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 8246 ms on a9cad0e294b6 (executor driver) (2/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000000
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:58:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8275 ms on a9cad0e294b6 (executor driver) (3/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000005
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:58:16 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 8328 ms on a9cad0e294b6 (executor driver) (4/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000003
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:58:16 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 8371 ms on a9cad0e294b6 (executor driver) (5/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000004
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:58:16 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 8374 ms on a9cad0e294b6 (executor driver) (6/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000001
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:58:16 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 8459 ms on a9cad0e294b6 (executor driver) (7/8)
23/11/29 02:58:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311290258071752740177829827904_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290258071752740177829827904_0001_m_000006
23/11/29 02:58:16 INFO SparkHadoopMapRedUtil: attempt_202311290258071752740177829827904_0001_m_000006_7: Committed. Elapsed time: 0 ms.
23/11/29 02:58:16 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2498 bytes result sent to driver
23/11/29 02:58:16 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 8505 ms on a9cad0e294b6 (executor driver) (8/8)
23/11/29 02:58:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:58:16 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 8.555 s
23/11/29 02:58:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:58:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:58:16 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 8.561299 s
23/11/29 02:58:16 INFO FileFormatWriter: Start to commit write Job 984a4728-ee0a-440c-ba2b-7414459c21ff.
23/11/29 02:58:16 INFO FileFormatWriter: Write Job 984a4728-ee0a-440c-ba2b-7414459c21ff committed. Elapsed time: 38 ms.
23/11/29 02:58:16 INFO FileFormatWriter: Finished processing stats for write job 984a4728-ee0a-440c-ba2b-7414459c21ff.
23/11/29 02:58:16 INFO SparkUI: Stopped Spark web UI at http://a9cad0e294b6:4040
23/11/29 02:58:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:58:16 INFO MemoryStore: MemoryStore cleared
23/11/29 02:58:16 INFO BlockManager: BlockManager stopped
23/11/29 02:58:16 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:58:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:58:16 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 12
23/11/29 02:58:16 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:58:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-aec51edf-5a89-4d75-bb32-b63584114728
23/11/29 02:58:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-703e28b5-9813-4545-9d4a-7d12903c3c05
