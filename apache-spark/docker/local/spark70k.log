Execution 1:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-c6439d3b-9e1c-444d-a629-0faa007edba0;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (508ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (356ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (330ms)
:: resolution report :: resolve 7369ms :: artifacts dl 1208ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-c6439d3b-9e1c-444d-a629-0faa007edba0
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 02:47:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:47:27 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:47:27 INFO ResourceUtils: ==============================================================
23/11/29 02:47:27 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:47:27 INFO ResourceUtils: ==============================================================
23/11/29 02:47:27 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:47:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:47:27 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:47:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:47:27 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:47:27 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:47:27 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:47:27 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:47:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:47:27 INFO Utils: Successfully started service 'sparkDriver' on port 36961.
23/11/29 02:47:27 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:47:27 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:47:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:47:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:47:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:47:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c93ec018-b146-4b58-8f30-93f9e5b70f70
23/11/29 02:47:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:47:27 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:47:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:47:28 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://7f72b748a2f4:36961/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://7f72b748a2f4:36961/jars/org.tukaani_xz-1.9.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://7f72b748a2f4:36961/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://7f72b748a2f4:36961/jars/spark.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO Executor: Starting executor ID driver on host 7f72b748a2f4
23/11/29 02:47:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:47:28 INFO Executor: Fetching spark://7f72b748a2f4:36961/jars/spark.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO TransportClientFactory: Successfully created connection to 7f72b748a2f4/172.17.0.2:36961 after 30 ms (0 ms spent in bootstraps)
23/11/29 02:47:28 INFO Utils: Fetching spark://7f72b748a2f4:36961/jars/spark.jar to /tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/fetchFileTemp8336392608736018771.tmp
23/11/29 02:47:28 INFO Executor: Adding file:/tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/spark.jar to class loader
23/11/29 02:47:28 INFO Executor: Fetching spark://7f72b748a2f4:36961/jars/org.tukaani_xz-1.9.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO Utils: Fetching spark://7f72b748a2f4:36961/jars/org.tukaani_xz-1.9.jar to /tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/fetchFileTemp11527482542340029227.tmp
23/11/29 02:47:28 INFO Executor: Adding file:/tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:47:28 INFO Executor: Fetching spark://7f72b748a2f4:36961/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO Utils: Fetching spark://7f72b748a2f4:36961/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/fetchFileTemp8613636775437635540.tmp
23/11/29 02:47:28 INFO Executor: Adding file:/tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:47:28 INFO Executor: Fetching spark://7f72b748a2f4:36961/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226047017
23/11/29 02:47:28 INFO Utils: Fetching spark://7f72b748a2f4:36961/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/fetchFileTemp13310546438247343434.tmp
23/11/29 02:47:28 INFO Executor: Adding file:/tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c/userFiles-a0c6a95b-ab82-4129-9ce6-40f5db0af57f/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:47:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39877.
23/11/29 02:47:28 INFO NettyBlockTransferService: Server created on 7f72b748a2f4:39877
23/11/29 02:47:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:47:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7f72b748a2f4, 39877, None)
23/11/29 02:47:28 INFO BlockManagerMasterEndpoint: Registering block manager 7f72b748a2f4:39877 with 434.4 MiB RAM, BlockManagerId(driver, 7f72b748a2f4, 39877, None)
23/11/29 02:47:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7f72b748a2f4, 39877, None)
23/11/29 02:47:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7f72b748a2f4, 39877, None)
23/11/29 02:47:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:47:29 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:47:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:47:30 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
23/11/29 02:47:32 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:47:32 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:47:32 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:47:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:47:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:47:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7f72b748a2f4:39877 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:47:32 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:47:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:47:33 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:47:33 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:47:33 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:47:33 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:47:33 INFO DAGScheduler: Missing parents: List()
23/11/29 02:47:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:47:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:47:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:47:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7f72b748a2f4:39877 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:47:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:47:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:47:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:47:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (7f72b748a2f4, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:47:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:47:33 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:47:33 INFO CodeGenerator: Code generated in 159.00539 ms
23/11/29 02:47:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:47:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 422 ms on 7f72b748a2f4 (executor driver) (1/1)
23/11/29 02:47:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:47:33 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.518 s
23/11/29 02:47:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:47:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:47:33 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.565663 s
23/11/29 02:47:33 INFO CodeGenerator: Code generated in 13.069722 ms
23/11/29 02:47:33 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/11/29 02:47:33 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:47:33 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:47:33 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:47:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:47:33 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:47:33 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:47:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:47:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:47:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7f72b748a2f4:39877 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:47:34 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:47:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:47:34 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:47:34 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:47:34 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:47:34 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:47:34 INFO DAGScheduler: Missing parents: List()
23/11/29 02:47:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:47:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:47:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:47:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7f72b748a2f4:39877 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:47:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:47:34 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:47:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:47:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (7f72b748a2f4, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (7f72b748a2f4, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (7f72b748a2f4, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (7f72b748a2f4, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (7f72b748a2f4, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (7f72b748a2f4, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (7f72b748a2f4, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:47:34 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:47:34 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:47:34 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:47:34 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:47:34 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:47:34 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:47:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:47:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7f72b748a2f4:39877 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:47:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7f72b748a2f4:39877 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:47:34 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:47:34 INFO CodeGenerator: Code generated in 89.616642 ms
23/11/29 02:47:35 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000006
23/11/29 02:47:35 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:47:35 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:47:35 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1269 ms on 7f72b748a2f4 (executor driver) (1/7)
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000005
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000005_6: Committed. Elapsed time: 2 ms.
23/11/29 02:47:36 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000000
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000000_1: Committed. Elapsed time: 3 ms.
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000004
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000003
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:47:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2335 ms on 7f72b748a2f4 (executor driver) (2/7)
23/11/29 02:47:36 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2343 ms on 7f72b748a2f4 (executor driver) (3/7)
23/11/29 02:47:36 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2341 ms on 7f72b748a2f4 (executor driver) (4/7)
23/11/29 02:47:36 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2342 ms on 7f72b748a2f4 (executor driver) (5/7)
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000001
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:47:36 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247346594026401540289116_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247346594026401540289116_0001_m_000002
23/11/29 02:47:36 INFO SparkHadoopMapRedUtil: attempt_202311290247346594026401540289116_0001_m_000002_3: Committed. Elapsed time: 0 ms.
23/11/29 02:47:36 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2357 ms on 7f72b748a2f4 (executor driver) (6/7)
23/11/29 02:47:36 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:47:36 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2359 ms on 7f72b748a2f4 (executor driver) (7/7)
23/11/29 02:47:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:47:36 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.414 s
23/11/29 02:47:36 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:47:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:47:36 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.419571 s
23/11/29 02:47:36 INFO FileFormatWriter: Start to commit write Job eae1cfa8-5381-41f4-9435-a0679ff97dd5.
23/11/29 02:47:36 INFO FileFormatWriter: Write Job eae1cfa8-5381-41f4-9435-a0679ff97dd5 committed. Elapsed time: 15 ms.
23/11/29 02:47:36 INFO FileFormatWriter: Finished processing stats for write job eae1cfa8-5381-41f4-9435-a0679ff97dd5.
23/11/29 02:47:36 INFO SparkUI: Stopped Spark web UI at http://7f72b748a2f4:4040
23/11/29 02:47:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:47:36 INFO MemoryStore: MemoryStore cleared
23/11/29 02:47:36 INFO BlockManager: BlockManager stopped
23/11/29 02:47:36 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:47:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:47:36 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:47:36 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:47:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-58921631-dade-4e6a-9baa-d49eff9bd02c
23/11/29 02:47:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-8bd52ab2-63e0-4dd8-bd2b-374563db3a54
Execution 2:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e639397c-e552-4f32-abff-b9914f9fdaa0;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (724ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (347ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (330ms)
:: resolution report :: resolve 7076ms :: artifacts dl 1424ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e639397c-e552-4f32-abff-b9914f9fdaa0
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/15ms)
23/11/29 02:47:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:47:48 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:47:48 INFO ResourceUtils: ==============================================================
23/11/29 02:47:48 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:47:48 INFO ResourceUtils: ==============================================================
23/11/29 02:47:48 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:47:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:47:48 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:47:48 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:47:48 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:47:48 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:47:48 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:47:48 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:47:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:47:49 INFO Utils: Successfully started service 'sparkDriver' on port 45351.
23/11/29 02:47:49 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:47:49 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:47:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:47:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:47:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:47:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a10c3341-c8e1-4888-a893-b76b403c6bae
23/11/29 02:47:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:47:49 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:47:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:47:50 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://acaf36cd34cc:45351/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://acaf36cd34cc:45351/jars/org.tukaani_xz-1.9.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://acaf36cd34cc:45351/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://acaf36cd34cc:45351/jars/spark.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO Executor: Starting executor ID driver on host acaf36cd34cc
23/11/29 02:47:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:47:50 INFO Executor: Fetching spark://acaf36cd34cc:45351/jars/spark.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO TransportClientFactory: Successfully created connection to acaf36cd34cc/172.17.0.2:45351 after 28 ms (0 ms spent in bootstraps)
23/11/29 02:47:50 INFO Utils: Fetching spark://acaf36cd34cc:45351/jars/spark.jar to /tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/fetchFileTemp7744653072833586896.tmp
23/11/29 02:47:50 INFO Executor: Adding file:/tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/spark.jar to class loader
23/11/29 02:47:50 INFO Executor: Fetching spark://acaf36cd34cc:45351/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO Utils: Fetching spark://acaf36cd34cc:45351/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/fetchFileTemp17827892178473640314.tmp
23/11/29 02:47:50 INFO Executor: Adding file:/tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:47:50 INFO Executor: Fetching spark://acaf36cd34cc:45351/jars/org.tukaani_xz-1.9.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO Utils: Fetching spark://acaf36cd34cc:45351/jars/org.tukaani_xz-1.9.jar to /tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/fetchFileTemp17278445021624272520.tmp
23/11/29 02:47:50 INFO Executor: Adding file:/tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:47:50 INFO Executor: Fetching spark://acaf36cd34cc:45351/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226068740
23/11/29 02:47:50 INFO Utils: Fetching spark://acaf36cd34cc:45351/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/fetchFileTemp3244077722020207756.tmp
23/11/29 02:47:50 INFO Executor: Adding file:/tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c/userFiles-b81a1abd-473f-4374-8acf-4ad36f063b7a/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:47:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45335.
23/11/29 02:47:50 INFO NettyBlockTransferService: Server created on acaf36cd34cc:45335
23/11/29 02:47:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:47:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, acaf36cd34cc, 45335, None)
23/11/29 02:47:50 INFO BlockManagerMasterEndpoint: Registering block manager acaf36cd34cc:45335 with 434.4 MiB RAM, BlockManagerId(driver, acaf36cd34cc, 45335, None)
23/11/29 02:47:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, acaf36cd34cc, 45335, None)
23/11/29 02:47:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, acaf36cd34cc, 45335, None)
23/11/29 02:47:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:47:51 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:47:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:47:51 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
23/11/29 02:47:55 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:47:55 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:47:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:47:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:47:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:47:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on acaf36cd34cc:45335 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:47:56 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:47:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:47:56 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:47:56 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:47:56 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:47:56 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:47:56 INFO DAGScheduler: Missing parents: List()
23/11/29 02:47:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:47:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:47:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:47:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on acaf36cd34cc:45335 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:47:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:47:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:47:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:47:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (acaf36cd34cc, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:47:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:47:56 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:47:57 INFO CodeGenerator: Code generated in 256.462659 ms
23/11/29 02:47:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:47:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 555 ms on acaf36cd34cc (executor driver) (1/1)
23/11/29 02:47:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:47:57 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.660 s
23/11/29 02:47:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:47:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:47:57 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.706558 s
23/11/29 02:47:57 INFO CodeGenerator: Code generated in 15.030975 ms
23/11/29 02:47:57 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:47:57 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:47:57 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:47:57 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:47:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:47:57 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:47:57 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:47:57 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:47:57 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on acaf36cd34cc:45335 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:47:57 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:47:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:47:57 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:47:57 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:47:57 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:47:57 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:47:57 INFO DAGScheduler: Missing parents: List()
23/11/29 02:47:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:47:57 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:47:57 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:47:57 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on acaf36cd34cc:45335 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:47:57 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:47:57 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:47:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:47:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (acaf36cd34cc, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (acaf36cd34cc, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (acaf36cd34cc, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (acaf36cd34cc, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (acaf36cd34cc, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (acaf36cd34cc, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (acaf36cd34cc, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:47:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:47:57 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:47:57 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:47:57 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:47:57 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:47:57 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:47:57 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:47:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:47:57 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:47:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:47:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:47:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on acaf36cd34cc:45335 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:47:57 INFO BlockManagerInfo: Removed broadcast_1_piece0 on acaf36cd34cc:45335 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:47:58 INFO CodeGenerator: Code generated in 119.399658 ms
23/11/29 02:47:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000006
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000006_7: Committed. Elapsed time: 9 ms.
23/11/29 02:47:59 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1457 ms on acaf36cd34cc (executor driver) (1/7)
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000003
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000003_4: Committed. Elapsed time: 2 ms.
23/11/29 02:47:59 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000000
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:47:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2132 ms on acaf36cd34cc (executor driver) (2/7)
23/11/29 02:47:59 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2143 ms on acaf36cd34cc (executor driver) (3/7)
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000002
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000004
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:47:59 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2173 ms on acaf36cd34cc (executor driver) (4/7)
23/11/29 02:47:59 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2175 ms on acaf36cd34cc (executor driver) (5/7)
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000001
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:47:59 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2198 ms on acaf36cd34cc (executor driver) (6/7)
23/11/29 02:47:59 INFO FileOutputCommitter: Saved output of task 'attempt_202311290247575433191082925498192_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290247575433191082925498192_0001_m_000005
23/11/29 02:47:59 INFO SparkHadoopMapRedUtil: attempt_202311290247575433191082925498192_0001_m_000005_6: Committed. Elapsed time: 0 ms.
23/11/29 02:47:59 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:47:59 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2197 ms on acaf36cd34cc (executor driver) (7/7)
23/11/29 02:47:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:47:59 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.245 s
23/11/29 02:47:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:47:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:47:59 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.250953 s
23/11/29 02:47:59 INFO FileFormatWriter: Start to commit write Job 15021bd0-34f7-4f3a-a819-328f1446ba5b.
23/11/29 02:47:59 INFO FileFormatWriter: Write Job 15021bd0-34f7-4f3a-a819-328f1446ba5b committed. Elapsed time: 17 ms.
23/11/29 02:47:59 INFO FileFormatWriter: Finished processing stats for write job 15021bd0-34f7-4f3a-a819-328f1446ba5b.
23/11/29 02:47:59 INFO SparkUI: Stopped Spark web UI at http://acaf36cd34cc:4040
23/11/29 02:47:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:47:59 INFO MemoryStore: MemoryStore cleared
23/11/29 02:47:59 INFO BlockManager: BlockManager stopped
23/11/29 02:47:59 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:47:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:47:59 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 8
23/11/29 02:47:59 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:47:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-0f199891-2529-439b-92c9-2bd0e22efc9f
23/11/29 02:47:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5bf4e76-786c-4cb1-9470-dd4b95b4644c
Execution 3:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e7475aac-ddcf-4305-a445-660f5a830856;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1361ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (3191ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (634ms)
:: resolution report :: resolve 8492ms :: artifacts dl 5200ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e7475aac-ddcf-4305-a445-660f5a830856
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/12ms)
23/11/29 02:48:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:48:17 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:48:17 INFO ResourceUtils: ==============================================================
23/11/29 02:48:17 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:48:17 INFO ResourceUtils: ==============================================================
23/11/29 02:48:17 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:48:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:48:17 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:48:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:48:17 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:48:17 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:48:17 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:48:17 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:48:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:48:17 INFO Utils: Successfully started service 'sparkDriver' on port 45739.
23/11/29 02:48:17 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:48:17 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:48:17 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:48:17 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:48:17 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:48:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-64a4f689-f8b8-4c4e-93b6-c57b3c5b2003
23/11/29 02:48:17 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:48:17 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:48:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:48:18 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://92a64e169653:45739/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://92a64e169653:45739/jars/org.tukaani_xz-1.9.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://92a64e169653:45739/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://92a64e169653:45739/jars/spark.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO Executor: Starting executor ID driver on host 92a64e169653
23/11/29 02:48:18 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:48:18 INFO Executor: Fetching spark://92a64e169653:45739/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO TransportClientFactory: Successfully created connection to 92a64e169653/172.17.0.2:45739 after 31 ms (0 ms spent in bootstraps)
23/11/29 02:48:18 INFO Utils: Fetching spark://92a64e169653:45739/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/fetchFileTemp10143049075999342712.tmp
23/11/29 02:48:18 INFO Executor: Adding file:/tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:48:18 INFO Executor: Fetching spark://92a64e169653:45739/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO Utils: Fetching spark://92a64e169653:45739/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/fetchFileTemp14606958750745558227.tmp
23/11/29 02:48:18 INFO Executor: Adding file:/tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:48:18 INFO Executor: Fetching spark://92a64e169653:45739/jars/spark.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO Utils: Fetching spark://92a64e169653:45739/jars/spark.jar to /tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/fetchFileTemp95073654607679724.tmp
23/11/29 02:48:18 INFO Executor: Adding file:/tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/spark.jar to class loader
23/11/29 02:48:18 INFO Executor: Fetching spark://92a64e169653:45739/jars/org.tukaani_xz-1.9.jar with timestamp 1701226097266
23/11/29 02:48:18 INFO Utils: Fetching spark://92a64e169653:45739/jars/org.tukaani_xz-1.9.jar to /tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/fetchFileTemp12771385032768309300.tmp
23/11/29 02:48:18 INFO Executor: Adding file:/tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3/userFiles-f4a9234f-e317-4026-90df-f475e031900b/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:48:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43567.
23/11/29 02:48:18 INFO NettyBlockTransferService: Server created on 92a64e169653:43567
23/11/29 02:48:18 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:48:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 92a64e169653, 43567, None)
23/11/29 02:48:18 INFO BlockManagerMasterEndpoint: Registering block manager 92a64e169653:43567 with 434.4 MiB RAM, BlockManagerId(driver, 92a64e169653, 43567, None)
23/11/29 02:48:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 92a64e169653, 43567, None)
23/11/29 02:48:18 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 92a64e169653, 43567, None)
23/11/29 02:48:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:48:19 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:48:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:48:20 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
23/11/29 02:48:22 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:48:22 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:48:22 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:48:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:48:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:48:23 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 92a64e169653:43567 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:48:23 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:48:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:48:23 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:48:23 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:48:23 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:48:23 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:48:23 INFO DAGScheduler: Missing parents: List()
23/11/29 02:48:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:48:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:48:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:48:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 92a64e169653:43567 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:48:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:48:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:48:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:48:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (92a64e169653, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:48:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:48:23 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:48:23 INFO CodeGenerator: Code generated in 147.673213 ms
23/11/29 02:48:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:48:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 423 ms on 92a64e169653 (executor driver) (1/1)
23/11/29 02:48:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:48:23 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.533 s
23/11/29 02:48:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:48:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:48:23 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.581539 s
23/11/29 02:48:23 INFO CodeGenerator: Code generated in 18.762507 ms
23/11/29 02:48:23 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
23/11/29 02:48:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 92a64e169653:43567 in memory (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:48:24 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:48:24 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:48:24 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:48:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:48:24 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:48:24 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:48:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:48:24 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 92a64e169653:43567 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:48:24 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:48:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:48:24 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:48:24 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:48:24 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:48:24 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:48:24 INFO DAGScheduler: Missing parents: List()
23/11/29 02:48:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:48:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:48:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.7 MiB)
23/11/29 02:48:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 92a64e169653:43567 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:48:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:48:24 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:48:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:48:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (92a64e169653, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (92a64e169653, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (92a64e169653, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (92a64e169653, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (92a64e169653, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (92a64e169653, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (92a64e169653, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:48:24 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:48:24 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:48:24 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:48:24 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:48:24 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:48:24 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 92a64e169653:43567 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:48:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:48:24 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:48:24 INFO CodeGenerator: Code generated in 116.061076 ms
23/11/29 02:48:25 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000006
23/11/29 02:48:25 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:48:25 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:48:25 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1178 ms on 92a64e169653 (executor driver) (1/7)
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000003
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000003_4: Committed. Elapsed time: 2 ms.
23/11/29 02:48:26 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000001
23/11/29 02:48:26 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2183 ms on 92a64e169653 (executor driver) (2/7)
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:48:26 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000004
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000005
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000005_6: Committed. Elapsed time: 11 ms.
23/11/29 02:48:26 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2200 ms on 92a64e169653 (executor driver) (3/7)
23/11/29 02:48:26 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2200 ms on 92a64e169653 (executor driver) (4/7)
23/11/29 02:48:26 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2203 ms on 92a64e169653 (executor driver) (5/7)
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000002
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000002_3: Committed. Elapsed time: 0 ms.
23/11/29 02:48:26 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2250 ms on 92a64e169653 (executor driver) (6/7)
23/11/29 02:48:26 INFO FileOutputCommitter: Saved output of task 'attempt_202311290248247082191410512868473_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290248247082191410512868473_0001_m_000000
23/11/29 02:48:26 INFO SparkHadoopMapRedUtil: attempt_202311290248247082191410512868473_0001_m_000000_1: Committed. Elapsed time: 0 ms.
23/11/29 02:48:26 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:48:26 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2264 ms on 92a64e169653 (executor driver) (7/7)
23/11/29 02:48:26 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:48:26 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.328 s
23/11/29 02:48:26 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:48:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:48:26 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.333535 s
23/11/29 02:48:26 INFO FileFormatWriter: Start to commit write Job 4bb62d4a-3381-42a6-b407-d67e248199e0.
23/11/29 02:48:26 INFO FileFormatWriter: Write Job 4bb62d4a-3381-42a6-b407-d67e248199e0 committed. Elapsed time: 17 ms.
23/11/29 02:48:26 INFO FileFormatWriter: Finished processing stats for write job 4bb62d4a-3381-42a6-b407-d67e248199e0.
23/11/29 02:48:26 INFO SparkUI: Stopped Spark web UI at http://92a64e169653:4040
23/11/29 02:48:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:48:26 INFO MemoryStore: MemoryStore cleared
23/11/29 02:48:26 INFO BlockManager: BlockManager stopped
23/11/29 02:48:26 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:48:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:48:26 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:48:26 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:48:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d89ca94-ce7c-4af1-bb31-c083a7cfdbc3
23/11/29 02:48:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-249ea7d9-8c6c-482c-bcc5-2c14fa895ad3
Execution 4:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-96791c44-dbc4-4c75-b6fb-39b6785f75ba;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (2228ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (1700ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (678ms)
:: resolution report :: resolve 11733ms :: artifacts dl 4626ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-96791c44-dbc4-4c75-b6fb-39b6785f75ba
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/18ms)
23/11/29 02:48:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:48:46 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:48:46 INFO ResourceUtils: ==============================================================
23/11/29 02:48:46 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:48:46 INFO ResourceUtils: ==============================================================
23/11/29 02:48:46 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:48:46 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:48:46 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:48:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:48:47 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:48:47 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:48:47 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:48:47 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:48:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:48:47 INFO Utils: Successfully started service 'sparkDriver' on port 40745.
23/11/29 02:48:47 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:48:47 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:48:47 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:48:47 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:48:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:48:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fea59db6-44c6-4536-a03f-3b53832dce34
23/11/29 02:48:47 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:48:47 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:48:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:48:47 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://c53ad7f01cd8:40745/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://c53ad7f01cd8:40745/jars/org.tukaani_xz-1.9.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://c53ad7f01cd8:40745/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://c53ad7f01cd8:40745/jars/spark.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO Executor: Starting executor ID driver on host c53ad7f01cd8
23/11/29 02:48:47 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:48:47 INFO Executor: Fetching spark://c53ad7f01cd8:40745/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO TransportClientFactory: Successfully created connection to c53ad7f01cd8/172.17.0.2:40745 after 25 ms (0 ms spent in bootstraps)
23/11/29 02:48:47 INFO Utils: Fetching spark://c53ad7f01cd8:40745/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/fetchFileTemp11429536159114079811.tmp
23/11/29 02:48:47 INFO Executor: Adding file:/tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:48:47 INFO Executor: Fetching spark://c53ad7f01cd8:40745/jars/spark.jar with timestamp 1701226126846
23/11/29 02:48:47 INFO Utils: Fetching spark://c53ad7f01cd8:40745/jars/spark.jar to /tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/fetchFileTemp11315525702197297290.tmp
23/11/29 02:48:48 INFO Executor: Adding file:/tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/spark.jar to class loader
23/11/29 02:48:48 INFO Executor: Fetching spark://c53ad7f01cd8:40745/jars/org.tukaani_xz-1.9.jar with timestamp 1701226126846
23/11/29 02:48:48 INFO Utils: Fetching spark://c53ad7f01cd8:40745/jars/org.tukaani_xz-1.9.jar to /tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/fetchFileTemp6326271588341794898.tmp
23/11/29 02:48:48 INFO Executor: Adding file:/tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:48:48 INFO Executor: Fetching spark://c53ad7f01cd8:40745/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226126846
23/11/29 02:48:48 INFO Utils: Fetching spark://c53ad7f01cd8:40745/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/fetchFileTemp7112868246346295245.tmp
23/11/29 02:48:48 INFO Executor: Adding file:/tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3/userFiles-4eae852e-9eaa-401c-b491-4172c3d874bd/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:48:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45273.
23/11/29 02:48:48 INFO NettyBlockTransferService: Server created on c53ad7f01cd8:45273
23/11/29 02:48:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:48:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c53ad7f01cd8, 45273, None)
23/11/29 02:48:48 INFO BlockManagerMasterEndpoint: Registering block manager c53ad7f01cd8:45273 with 434.4 MiB RAM, BlockManagerId(driver, c53ad7f01cd8, 45273, None)
23/11/29 02:48:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c53ad7f01cd8, 45273, None)
23/11/29 02:48:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c53ad7f01cd8, 45273, None)
23/11/29 02:48:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:48:48 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:48:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:48:49 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.
23/11/29 02:48:51 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:48:51 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:48:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:48:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:48:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:48:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c53ad7f01cd8:45273 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:48:52 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:48:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:48:52 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:48:52 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:48:52 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:48:52 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:48:52 INFO DAGScheduler: Missing parents: List()
23/11/29 02:48:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:48:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:48:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:48:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c53ad7f01cd8:45273 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:48:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:48:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:48:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:48:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c53ad7f01cd8, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:48:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:48:52 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:48:52 INFO CodeGenerator: Code generated in 149.815742 ms
23/11/29 02:48:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:48:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 440 ms on c53ad7f01cd8 (executor driver) (1/1)
23/11/29 02:48:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:48:53 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.546 s
23/11/29 02:48:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:48:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:48:53 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.588349 s
23/11/29 02:48:53 INFO CodeGenerator: Code generated in 14.253359 ms
23/11/29 02:48:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:48:53 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:48:53 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:48:53 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:48:53 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:48:53 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:48:53 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:48:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:48:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c53ad7f01cd8:45273 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:48:53 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:48:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:48:53 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:48:53 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:48:53 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:48:53 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:48:53 INFO DAGScheduler: Missing parents: List()
23/11/29 02:48:53 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:48:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:48:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:48:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c53ad7f01cd8:45273 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:48:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:48:53 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:48:53 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:48:53 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c53ad7f01cd8, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (c53ad7f01cd8, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (c53ad7f01cd8, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (c53ad7f01cd8, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (c53ad7f01cd8, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (c53ad7f01cd8, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (c53ad7f01cd8, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:48:53 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:48:53 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:48:53 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:48:53 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:48:53 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:48:53 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:48:53 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:48:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c53ad7f01cd8:45273 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:48:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:48:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:48:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:48:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c53ad7f01cd8:45273 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:48:53 INFO CodeGenerator: Code generated in 96.590288 ms
23/11/29 02:48:53 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:48:54 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000006
23/11/29 02:48:54 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:48:54 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:48:54 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1256 ms on c53ad7f01cd8 (executor driver) (1/7)
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000005
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:48:55 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1944 ms on c53ad7f01cd8 (executor driver) (2/7)
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000003
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:48:55 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1970 ms on c53ad7f01cd8 (executor driver) (3/7)
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000002
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:48:55 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000004
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:48:55 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2025 ms on c53ad7f01cd8 (executor driver) (4/7)
23/11/29 02:48:55 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2025 ms on c53ad7f01cd8 (executor driver) (5/7)
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000000
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:48:55 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2037 ms on c53ad7f01cd8 (executor driver) (6/7)
23/11/29 02:48:55 INFO FileOutputCommitter: Saved output of task 'attempt_20231129024853653672878004275809_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129024853653672878004275809_0001_m_000001
23/11/29 02:48:55 INFO SparkHadoopMapRedUtil: attempt_20231129024853653672878004275809_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:48:55 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:48:55 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2044 ms on c53ad7f01cd8 (executor driver) (7/7)
23/11/29 02:48:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:48:55 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.100 s
23/11/29 02:48:55 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:48:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:48:55 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.105890 s
23/11/29 02:48:55 INFO FileFormatWriter: Start to commit write Job 616818e1-53a0-4a09-aa19-0148df508837.
23/11/29 02:48:55 INFO FileFormatWriter: Write Job 616818e1-53a0-4a09-aa19-0148df508837 committed. Elapsed time: 32 ms.
23/11/29 02:48:55 INFO FileFormatWriter: Finished processing stats for write job 616818e1-53a0-4a09-aa19-0148df508837.
23/11/29 02:48:55 INFO SparkUI: Stopped Spark web UI at http://c53ad7f01cd8:4040
23/11/29 02:48:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:48:55 INFO MemoryStore: MemoryStore cleared
23/11/29 02:48:55 INFO BlockManager: BlockManager stopped
23/11/29 02:48:55 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:48:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:48:55 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:48:55 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:48:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-ba67ff78-2cb4-43fb-8b92-fa2a048798a8
23/11/29 02:48:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-dfe1cbbc-0597-4f7f-a896-7ef6fd9995e3
Execution 5:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-22804612-f3f0-44a0-a744-4b6e5433e226;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1097ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (628ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (468ms)
:: resolution report :: resolve 8631ms :: artifacts dl 2210ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-22804612-f3f0-44a0-a744-4b6e5433e226
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/15ms)
23/11/29 02:49:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:49:09 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:49:09 INFO ResourceUtils: ==============================================================
23/11/29 02:49:09 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:49:09 INFO ResourceUtils: ==============================================================
23/11/29 02:49:09 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:49:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:49:10 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:49:10 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:49:10 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:49:10 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:49:10 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:49:10 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:49:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:49:10 INFO Utils: Successfully started service 'sparkDriver' on port 43611.
23/11/29 02:49:10 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:49:10 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:49:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:49:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:49:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:49:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3920749-b335-41da-ac4b-4d3091f98844
23/11/29 02:49:10 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:49:10 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:49:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:49:10 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://7e9607ed34df:43611/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226149931
23/11/29 02:49:10 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://7e9607ed34df:43611/jars/org.tukaani_xz-1.9.jar with timestamp 1701226149931
23/11/29 02:49:10 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://7e9607ed34df:43611/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226149931
23/11/29 02:49:10 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://7e9607ed34df:43611/jars/spark.jar with timestamp 1701226149931
23/11/29 02:49:10 INFO Executor: Starting executor ID driver on host 7e9607ed34df
23/11/29 02:49:10 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:49:10 INFO Executor: Fetching spark://7e9607ed34df:43611/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226149931
23/11/29 02:49:11 INFO TransportClientFactory: Successfully created connection to 7e9607ed34df/172.17.0.2:43611 after 31 ms (0 ms spent in bootstraps)
23/11/29 02:49:11 INFO Utils: Fetching spark://7e9607ed34df:43611/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/fetchFileTemp16917663314718921988.tmp
23/11/29 02:49:11 INFO Executor: Adding file:/tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:49:11 INFO Executor: Fetching spark://7e9607ed34df:43611/jars/org.tukaani_xz-1.9.jar with timestamp 1701226149931
23/11/29 02:49:11 INFO Utils: Fetching spark://7e9607ed34df:43611/jars/org.tukaani_xz-1.9.jar to /tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/fetchFileTemp10825609791141483698.tmp
23/11/29 02:49:11 INFO Executor: Adding file:/tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:49:11 INFO Executor: Fetching spark://7e9607ed34df:43611/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226149931
23/11/29 02:49:11 INFO Utils: Fetching spark://7e9607ed34df:43611/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/fetchFileTemp16944193190870336126.tmp
23/11/29 02:49:11 INFO Executor: Adding file:/tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:49:11 INFO Executor: Fetching spark://7e9607ed34df:43611/jars/spark.jar with timestamp 1701226149931
23/11/29 02:49:11 INFO Utils: Fetching spark://7e9607ed34df:43611/jars/spark.jar to /tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/fetchFileTemp17258805198291866276.tmp
23/11/29 02:49:11 INFO Executor: Adding file:/tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d/userFiles-ceef2363-8d38-48fd-8dbf-c3e22ce425b1/spark.jar to class loader
23/11/29 02:49:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37271.
23/11/29 02:49:11 INFO NettyBlockTransferService: Server created on 7e9607ed34df:37271
23/11/29 02:49:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:49:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7e9607ed34df, 37271, None)
23/11/29 02:49:11 INFO BlockManagerMasterEndpoint: Registering block manager 7e9607ed34df:37271 with 434.4 MiB RAM, BlockManagerId(driver, 7e9607ed34df, 37271, None)
23/11/29 02:49:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7e9607ed34df, 37271, None)
23/11/29 02:49:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7e9607ed34df, 37271, None)
23/11/29 02:49:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:49:11 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:49:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:49:12 INFO InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.
23/11/29 02:49:14 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:49:14 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:49:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:49:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:49:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:49:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7e9607ed34df:37271 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:49:15 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:49:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:49:15 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:49:15 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:49:15 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:49:15 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:49:15 INFO DAGScheduler: Missing parents: List()
23/11/29 02:49:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:49:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:49:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:49:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7e9607ed34df:37271 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:49:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:49:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:49:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:49:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (7e9607ed34df, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:49:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:49:15 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:49:16 INFO CodeGenerator: Code generated in 140.716682 ms
23/11/29 02:49:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:49:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 446 ms on 7e9607ed34df (executor driver) (1/1)
23/11/29 02:49:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:49:16 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.557 s
23/11/29 02:49:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:49:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:49:16 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.600032 s
23/11/29 02:49:16 INFO CodeGenerator: Code generated in 19.96765 ms
23/11/29 02:49:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:49:16 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:49:16 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:49:16 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:49:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:49:16 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:49:16 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:49:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:49:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7e9607ed34df:37271 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:49:16 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:49:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:49:16 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:49:16 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:49:16 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:49:16 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:49:16 INFO DAGScheduler: Missing parents: List()
23/11/29 02:49:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:49:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:49:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 02:49:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7e9607ed34df:37271 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 02:49:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:49:16 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:49:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:49:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (7e9607ed34df, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (7e9607ed34df, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (7e9607ed34df, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (7e9607ed34df, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (7e9607ed34df, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (7e9607ed34df, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (7e9607ed34df, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:49:16 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:49:16 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:49:16 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:49:16 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:49:16 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:49:16 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7e9607ed34df:37271 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:49:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:16 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:49:16 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:49:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7e9607ed34df:37271 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:49:17 INFO CodeGenerator: Code generated in 128.371531 ms
23/11/29 02:49:17 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000006
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000006_7: Committed. Elapsed time: 7 ms.
23/11/29 02:49:18 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:49:18 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1341 ms on 7e9607ed34df (executor driver) (1/7)
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000004
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000004_5: Committed. Elapsed time: 3 ms.
23/11/29 02:49:18 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1798 ms on 7e9607ed34df (executor driver) (2/7)
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000002
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000001
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000001_2: Committed. Elapsed time: 2 ms.
23/11/29 02:49:18 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000005
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000003
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000003_4: Committed. Elapsed time: 3 ms.
23/11/29 02:49:18 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1859 ms on 7e9607ed34df (executor driver) (3/7)
23/11/29 02:49:18 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1858 ms on 7e9607ed34df (executor driver) (4/7)
23/11/29 02:49:18 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1859 ms on 7e9607ed34df (executor driver) (5/7)
23/11/29 02:49:18 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1858 ms on 7e9607ed34df (executor driver) (6/7)
23/11/29 02:49:18 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249162788789672534156199_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249162788789672534156199_0001_m_000000
23/11/29 02:49:18 INFO SparkHadoopMapRedUtil: attempt_202311290249162788789672534156199_0001_m_000000_1: Committed. Elapsed time: 0 ms.
23/11/29 02:49:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:49:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1868 ms on 7e9607ed34df (executor driver) (7/7)
23/11/29 02:49:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:49:18 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 1.924 s
23/11/29 02:49:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:49:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:49:18 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 1.932945 s
23/11/29 02:49:18 INFO FileFormatWriter: Start to commit write Job f4393191-d2c8-45ce-8605-1fa5fa3514e0.
23/11/29 02:49:18 INFO FileFormatWriter: Write Job f4393191-d2c8-45ce-8605-1fa5fa3514e0 committed. Elapsed time: 15 ms.
23/11/29 02:49:18 INFO FileFormatWriter: Finished processing stats for write job f4393191-d2c8-45ce-8605-1fa5fa3514e0.
23/11/29 02:49:18 INFO SparkUI: Stopped Spark web UI at http://7e9607ed34df:4040
23/11/29 02:49:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:49:18 INFO MemoryStore: MemoryStore cleared
23/11/29 02:49:18 INFO BlockManager: BlockManager stopped
23/11/29 02:49:18 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:49:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:49:18 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:49:18 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:49:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-736fd82a-3f2e-4b01-a682-d69fa249982a
23/11/29 02:49:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ad1aeec-85b0-4185-bf4c-49f78b0c073d
Execution 6:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-5e4319f3-d141-4370-bfff-bb00d3444490;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (2179ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (3060ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (550ms)
:: resolution report :: resolve 8196ms :: artifacts dl 5804ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-5e4319f3-d141-4370-bfff-bb00d3444490
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/11ms)
23/11/29 02:49:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:49:36 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:49:36 INFO ResourceUtils: ==============================================================
23/11/29 02:49:36 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:49:36 INFO ResourceUtils: ==============================================================
23/11/29 02:49:36 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:49:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:49:36 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:49:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:49:36 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:49:36 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:49:36 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:49:36 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:49:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:49:36 INFO Utils: Successfully started service 'sparkDriver' on port 36021.
23/11/29 02:49:36 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:49:36 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:49:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:49:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:49:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:49:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21f713f1-2167-4852-96b5-f3db5209d2dd
23/11/29 02:49:36 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:49:36 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:49:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:49:37 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://aae15f510c6a:36021/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://aae15f510c6a:36021/jars/org.tukaani_xz-1.9.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://aae15f510c6a:36021/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://aae15f510c6a:36021/jars/spark.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO Executor: Starting executor ID driver on host aae15f510c6a
23/11/29 02:49:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:49:37 INFO Executor: Fetching spark://aae15f510c6a:36021/jars/org.tukaani_xz-1.9.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO TransportClientFactory: Successfully created connection to aae15f510c6a/172.17.0.2:36021 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:49:37 INFO Utils: Fetching spark://aae15f510c6a:36021/jars/org.tukaani_xz-1.9.jar to /tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/fetchFileTemp4472890382227453976.tmp
23/11/29 02:49:37 INFO Executor: Adding file:/tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:49:37 INFO Executor: Fetching spark://aae15f510c6a:36021/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO Utils: Fetching spark://aae15f510c6a:36021/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/fetchFileTemp16984175004370403749.tmp
23/11/29 02:49:37 INFO Executor: Adding file:/tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:49:37 INFO Executor: Fetching spark://aae15f510c6a:36021/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO Utils: Fetching spark://aae15f510c6a:36021/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/fetchFileTemp8131411578027784432.tmp
23/11/29 02:49:37 INFO Executor: Adding file:/tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:49:37 INFO Executor: Fetching spark://aae15f510c6a:36021/jars/spark.jar with timestamp 1701226176147
23/11/29 02:49:37 INFO Utils: Fetching spark://aae15f510c6a:36021/jars/spark.jar to /tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/fetchFileTemp3535176271805737619.tmp
23/11/29 02:49:37 INFO Executor: Adding file:/tmp/spark-01890573-e909-4910-aebf-9f610080298d/userFiles-12384810-420d-4d4e-b6c0-f4a3049ca255/spark.jar to class loader
23/11/29 02:49:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40869.
23/11/29 02:49:37 INFO NettyBlockTransferService: Server created on aae15f510c6a:40869
23/11/29 02:49:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:49:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, aae15f510c6a, 40869, None)
23/11/29 02:49:37 INFO BlockManagerMasterEndpoint: Registering block manager aae15f510c6a:40869 with 434.4 MiB RAM, BlockManagerId(driver, aae15f510c6a, 40869, None)
23/11/29 02:49:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, aae15f510c6a, 40869, None)
23/11/29 02:49:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, aae15f510c6a, 40869, None)
23/11/29 02:49:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:49:38 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:49:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:49:39 INFO InMemoryFileIndex: It took 49 ms to list leaf files for 1 paths.
23/11/29 02:49:41 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:49:41 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:49:41 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:49:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:49:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:49:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on aae15f510c6a:40869 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:49:41 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:49:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:49:41 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:49:41 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:49:41 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:49:41 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:49:41 INFO DAGScheduler: Missing parents: List()
23/11/29 02:49:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:49:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:49:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:49:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on aae15f510c6a:40869 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:49:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:49:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:49:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:49:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (aae15f510c6a, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:49:42 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:49:42 INFO CodeGenerator: Code generated in 170.050271 ms
23/11/29 02:49:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:49:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 452 ms on aae15f510c6a (executor driver) (1/1)
23/11/29 02:49:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:49:42 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.560 s
23/11/29 02:49:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:49:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:49:42 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.604897 s
23/11/29 02:49:42 INFO CodeGenerator: Code generated in 14.50592 ms
23/11/29 02:49:42 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:49:42 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:49:42 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:49:42 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:49:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:49:42 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:49:42 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:49:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:49:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:49:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on aae15f510c6a:40869 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:49:42 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:49:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:49:42 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:49:42 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:49:42 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:49:42 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:49:42 INFO DAGScheduler: Missing parents: List()
23/11/29 02:49:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:49:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:49:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:49:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on aae15f510c6a:40869 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:49:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:49:42 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:49:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:49:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (aae15f510c6a, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (aae15f510c6a, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (aae15f510c6a, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (aae15f510c6a, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (aae15f510c6a, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (aae15f510c6a, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (aae15f510c6a, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:49:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:49:42 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:49:42 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:49:42 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:49:42 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:49:42 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:49:42 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:49:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:49:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:49:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:49:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on aae15f510c6a:40869 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:49:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on aae15f510c6a:40869 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:49:43 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:49:43 INFO CodeGenerator: Code generated in 105.0171 ms
23/11/29 02:49:44 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000006
23/11/29 02:49:44 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000006_7: Committed. Elapsed time: 7 ms.
23/11/29 02:49:44 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:49:44 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1498 ms on aae15f510c6a (executor driver) (1/7)
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000002
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:49:45 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2267 ms on aae15f510c6a (executor driver) (2/7)
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000004
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:49:45 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000000
23/11/29 02:49:45 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2302 ms on aae15f510c6a (executor driver) (3/7)
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:49:45 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2308 ms on aae15f510c6a (executor driver) (4/7)
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000001
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:49:45 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2318 ms on aae15f510c6a (executor driver) (5/7)
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000005
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:49:45 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2320 ms on aae15f510c6a (executor driver) (6/7)
23/11/29 02:49:45 INFO FileOutputCommitter: Saved output of task 'attempt_202311290249426998622675289765880_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290249426998622675289765880_0001_m_000003
23/11/29 02:49:45 INFO SparkHadoopMapRedUtil: attempt_202311290249426998622675289765880_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:49:45 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:49:45 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2347 ms on aae15f510c6a (executor driver) (7/7)
23/11/29 02:49:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:49:45 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.412 s
23/11/29 02:49:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:49:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:49:45 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.419628 s
23/11/29 02:49:45 INFO FileFormatWriter: Start to commit write Job f68a1ce6-e34c-4b3f-93ac-334b9a8d0a8b.
23/11/29 02:49:45 INFO FileFormatWriter: Write Job f68a1ce6-e34c-4b3f-93ac-334b9a8d0a8b committed. Elapsed time: 14 ms.
23/11/29 02:49:45 INFO FileFormatWriter: Finished processing stats for write job f68a1ce6-e34c-4b3f-93ac-334b9a8d0a8b.
23/11/29 02:49:45 INFO SparkUI: Stopped Spark web UI at http://aae15f510c6a:4040
23/11/29 02:49:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:49:45 INFO MemoryStore: MemoryStore cleared
23/11/29 02:49:45 INFO BlockManager: BlockManager stopped
23/11/29 02:49:45 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:49:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:49:45 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:49:45 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:49:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-1261ba2b-179e-476b-a391-27d16f4d2d2e
23/11/29 02:49:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-01890573-e909-4910-aebf-9f610080298d
Execution 7:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-27dde2e7-06f8-4084-91fa-46561ee042af;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1201ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (889ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (605ms)
:: resolution report :: resolve 8899ms :: artifacts dl 2714ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-27dde2e7-06f8-4084-91fa-46561ee042af
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 02:50:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:50:01 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:50:01 INFO ResourceUtils: ==============================================================
23/11/29 02:50:01 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:50:01 INFO ResourceUtils: ==============================================================
23/11/29 02:50:01 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:50:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:50:01 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:50:01 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:50:01 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:50:01 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:50:01 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:50:01 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:50:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:50:01 INFO Utils: Successfully started service 'sparkDriver' on port 46337.
23/11/29 02:50:01 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:50:01 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:50:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:50:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:50:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:50:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-642a0b53-2be7-4da9-80dd-128d37493c60
23/11/29 02:50:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:50:01 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:50:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:50:01 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://61a36b1e515b:46337/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226201022
23/11/29 02:50:01 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://61a36b1e515b:46337/jars/org.tukaani_xz-1.9.jar with timestamp 1701226201022
23/11/29 02:50:01 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://61a36b1e515b:46337/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226201022
23/11/29 02:50:01 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://61a36b1e515b:46337/jars/spark.jar with timestamp 1701226201022
23/11/29 02:50:01 INFO Executor: Starting executor ID driver on host 61a36b1e515b
23/11/29 02:50:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:50:01 INFO Executor: Fetching spark://61a36b1e515b:46337/jars/org.tukaani_xz-1.9.jar with timestamp 1701226201022
23/11/29 02:50:02 INFO TransportClientFactory: Successfully created connection to 61a36b1e515b/172.17.0.2:46337 after 28 ms (0 ms spent in bootstraps)
23/11/29 02:50:02 INFO Utils: Fetching spark://61a36b1e515b:46337/jars/org.tukaani_xz-1.9.jar to /tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/fetchFileTemp16774017913421473168.tmp
23/11/29 02:50:02 INFO Executor: Adding file:/tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:50:02 INFO Executor: Fetching spark://61a36b1e515b:46337/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226201022
23/11/29 02:50:02 INFO Utils: Fetching spark://61a36b1e515b:46337/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/fetchFileTemp4573894233215391555.tmp
23/11/29 02:50:02 INFO Executor: Adding file:/tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:50:02 INFO Executor: Fetching spark://61a36b1e515b:46337/jars/spark.jar with timestamp 1701226201022
23/11/29 02:50:02 INFO Utils: Fetching spark://61a36b1e515b:46337/jars/spark.jar to /tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/fetchFileTemp6002166284846224782.tmp
23/11/29 02:50:02 INFO Executor: Adding file:/tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/spark.jar to class loader
23/11/29 02:50:02 INFO Executor: Fetching spark://61a36b1e515b:46337/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226201022
23/11/29 02:50:02 INFO Utils: Fetching spark://61a36b1e515b:46337/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/fetchFileTemp14464617493965867385.tmp
23/11/29 02:50:02 INFO Executor: Adding file:/tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298/userFiles-9e9e6232-5066-457b-830c-9ea48c3f5070/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:50:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45551.
23/11/29 02:50:02 INFO NettyBlockTransferService: Server created on 61a36b1e515b:45551
23/11/29 02:50:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:50:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 61a36b1e515b, 45551, None)
23/11/29 02:50:02 INFO BlockManagerMasterEndpoint: Registering block manager 61a36b1e515b:45551 with 434.4 MiB RAM, BlockManagerId(driver, 61a36b1e515b, 45551, None)
23/11/29 02:50:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 61a36b1e515b, 45551, None)
23/11/29 02:50:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 61a36b1e515b, 45551, None)
23/11/29 02:50:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:50:02 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:50:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:50:03 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.
23/11/29 02:50:06 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:06 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:06 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:50:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:50:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:50:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 61a36b1e515b:45551 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:50:06 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:50:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:06 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:50:06 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:50:06 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:50:06 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:06 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:50:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:50:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:50:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 61a36b1e515b:45551 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:50:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:50:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:50:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (61a36b1e515b, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:50:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:50:07 INFO CodeGenerator: Code generated in 151.691896 ms
23/11/29 02:50:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:50:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 408 ms on 61a36b1e515b (executor driver) (1/1)
23/11/29 02:50:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:50:07 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.505 s
23/11/29 02:50:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:50:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:50:07 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.542928 s
23/11/29 02:50:07 INFO CodeGenerator: Code generated in 14.109261 ms
23/11/29 02:50:07 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:50:07 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:07 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:07 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:50:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:50:07 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:50:07 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:50:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:50:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 61a36b1e515b:45551 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:50:07 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:50:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:07 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:50:07 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:50:07 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:50:07 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:07 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:50:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:50:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:50:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 61a36b1e515b:45551 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:50:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:07 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:50:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:50:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (61a36b1e515b, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (61a36b1e515b, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (61a36b1e515b, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (61a36b1e515b, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (61a36b1e515b, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (61a36b1e515b, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (61a36b1e515b, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:50:07 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:50:07 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:50:07 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:50:07 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:50:07 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:50:07 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:50:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:07 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:50:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 61a36b1e515b:45551 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:50:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 61a36b1e515b:45551 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:50:08 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:50:08 INFO CodeGenerator: Code generated in 86.611155 ms
23/11/29 02:50:08 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000006
23/11/29 02:50:08 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000006_7: Committed. Elapsed time: 3 ms.
23/11/29 02:50:08 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:50:08 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1175 ms on 61a36b1e515b (executor driver) (1/7)
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000003
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:50:09 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1997 ms on 61a36b1e515b (executor driver) (2/7)
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000005
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:50:09 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2040 ms on 61a36b1e515b (executor driver) (3/7)
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000004
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000000
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:50:09 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2047 ms on 61a36b1e515b (executor driver) (4/7)
23/11/29 02:50:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2052 ms on 61a36b1e515b (executor driver) (5/7)
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000002
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:50:09 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2056 ms on 61a36b1e515b (executor driver) (6/7)
23/11/29 02:50:09 INFO FileOutputCommitter: Saved output of task 'attempt_20231129025007937041101104360752_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_20231129025007937041101104360752_0001_m_000001
23/11/29 02:50:09 INFO SparkHadoopMapRedUtil: attempt_20231129025007937041101104360752_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:50:09 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:50:09 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2077 ms on 61a36b1e515b (executor driver) (7/7)
23/11/29 02:50:09 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:50:09 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.130 s
23/11/29 02:50:09 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:50:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:50:09 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.135530 s
23/11/29 02:50:09 INFO FileFormatWriter: Start to commit write Job 558bf7ef-4ed1-44a9-9e39-994a6ceaab17.
23/11/29 02:50:09 INFO FileFormatWriter: Write Job 558bf7ef-4ed1-44a9-9e39-994a6ceaab17 committed. Elapsed time: 17 ms.
23/11/29 02:50:09 INFO FileFormatWriter: Finished processing stats for write job 558bf7ef-4ed1-44a9-9e39-994a6ceaab17.
23/11/29 02:50:09 INFO SparkUI: Stopped Spark web UI at http://61a36b1e515b:4040
23/11/29 02:50:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:50:09 INFO MemoryStore: MemoryStore cleared
23/11/29 02:50:09 INFO BlockManager: BlockManager stopped
23/11/29 02:50:09 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:50:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:50:09 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:50:09 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:50:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-572b04ee-0133-4b96-87dd-d64e53403298
23/11/29 02:50:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a70f8b1-0d4c-4bdf-844d-7678458ddbaf
Execution 8:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-74991f19-bff7-41db-9f22-b9f52639293a;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1311ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (1070ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (470ms)
:: resolution report :: resolve 9016ms :: artifacts dl 2871ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-74991f19-bff7-41db-9f22-b9f52639293a
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/15ms)
23/11/29 02:50:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:50:25 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:50:25 INFO ResourceUtils: ==============================================================
23/11/29 02:50:25 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:50:25 INFO ResourceUtils: ==============================================================
23/11/29 02:50:25 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:50:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:50:25 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:50:25 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:50:25 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:50:25 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:50:25 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:50:25 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:50:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:50:25 INFO Utils: Successfully started service 'sparkDriver' on port 43565.
23/11/29 02:50:25 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:50:25 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:50:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:50:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:50:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:50:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-db683c92-b5ea-4cbb-8831-7d9ca04eaf59
23/11/29 02:50:26 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:50:26 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:50:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:50:26 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://15241e1d2090:43565/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://15241e1d2090:43565/jars/org.tukaani_xz-1.9.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://15241e1d2090:43565/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://15241e1d2090:43565/jars/spark.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO Executor: Starting executor ID driver on host 15241e1d2090
23/11/29 02:50:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:50:26 INFO Executor: Fetching spark://15241e1d2090:43565/jars/org.tukaani_xz-1.9.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO TransportClientFactory: Successfully created connection to 15241e1d2090/172.17.0.2:43565 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:50:26 INFO Utils: Fetching spark://15241e1d2090:43565/jars/org.tukaani_xz-1.9.jar to /tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/fetchFileTemp9815022816618313120.tmp
23/11/29 02:50:26 INFO Executor: Adding file:/tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:50:26 INFO Executor: Fetching spark://15241e1d2090:43565/jars/spark.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO Utils: Fetching spark://15241e1d2090:43565/jars/spark.jar to /tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/fetchFileTemp1291962643198046479.tmp
23/11/29 02:50:26 INFO Executor: Adding file:/tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/spark.jar to class loader
23/11/29 02:50:26 INFO Executor: Fetching spark://15241e1d2090:43565/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO Utils: Fetching spark://15241e1d2090:43565/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/fetchFileTemp5908813867385685362.tmp
23/11/29 02:50:26 INFO Executor: Adding file:/tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:50:26 INFO Executor: Fetching spark://15241e1d2090:43565/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226225374
23/11/29 02:50:26 INFO Utils: Fetching spark://15241e1d2090:43565/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/fetchFileTemp9135091612344421880.tmp
23/11/29 02:50:26 INFO Executor: Adding file:/tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82/userFiles-424c6403-d1d9-4ad5-bad8-0ce1f4b6e99a/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:50:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44357.
23/11/29 02:50:26 INFO NettyBlockTransferService: Server created on 15241e1d2090:44357
23/11/29 02:50:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:50:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 15241e1d2090, 44357, None)
23/11/29 02:50:26 INFO BlockManagerMasterEndpoint: Registering block manager 15241e1d2090:44357 with 434.4 MiB RAM, BlockManagerId(driver, 15241e1d2090, 44357, None)
23/11/29 02:50:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 15241e1d2090, 44357, None)
23/11/29 02:50:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 15241e1d2090, 44357, None)
23/11/29 02:50:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:50:27 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:50:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:50:28 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.
23/11/29 02:50:30 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:30 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:30 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:50:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:50:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:50:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 15241e1d2090:44357 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:50:30 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:50:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:31 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:50:31 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:50:31 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:50:31 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:31 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:50:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:50:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:50:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 15241e1d2090:44357 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:50:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:50:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:50:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (15241e1d2090, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:50:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:50:31 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:50:31 INFO CodeGenerator: Code generated in 139.285621 ms
23/11/29 02:50:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:50:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 410 ms on 15241e1d2090 (executor driver) (1/1)
23/11/29 02:50:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:50:31 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.523 s
23/11/29 02:50:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:50:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:50:31 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.567022 s
23/11/29 02:50:31 INFO CodeGenerator: Code generated in 12.826575 ms
23/11/29 02:50:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:50:31 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:31 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:31 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:50:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:50:32 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:50:32 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:50:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:50:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 15241e1d2090:44357 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:50:32 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:50:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:32 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:50:32 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:50:32 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:50:32 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:32 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:50:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:50:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.6 KiB, free 433.6 MiB)
23/11/29 02:50:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 15241e1d2090:44357 (size: 78.6 KiB, free: 434.3 MiB)
23/11/29 02:50:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:32 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:50:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:50:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (15241e1d2090, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (15241e1d2090, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (15241e1d2090, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (15241e1d2090, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (15241e1d2090, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (15241e1d2090, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (15241e1d2090, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:50:32 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:50:32 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:50:32 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:50:32 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:50:32 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:50:32 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:50:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 15241e1d2090:44357 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:50:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 15241e1d2090:44357 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:50:32 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:50:32 INFO CodeGenerator: Code generated in 82.715416 ms
23/11/29 02:50:33 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000006
23/11/29 02:50:33 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:50:33 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:50:33 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1062 ms on 15241e1d2090 (executor driver) (1/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000005
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:50:34 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1998 ms on 15241e1d2090 (executor driver) (2/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000000
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:50:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2047 ms on 15241e1d2090 (executor driver) (3/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000001
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:50:34 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2069 ms on 15241e1d2090 (executor driver) (4/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000003
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000003_4: Committed. Elapsed time: 0 ms.
23/11/29 02:50:34 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2077 ms on 15241e1d2090 (executor driver) (5/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000002
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:50:34 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2082 ms on 15241e1d2090 (executor driver) (6/7)
23/11/29 02:50:34 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250327692173382092787395_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250327692173382092787395_0001_m_000004
23/11/29 02:50:34 INFO SparkHadoopMapRedUtil: attempt_202311290250327692173382092787395_0001_m_000004_5: Committed. Elapsed time: 0 ms.
23/11/29 02:50:34 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:50:34 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2102 ms on 15241e1d2090 (executor driver) (7/7)
23/11/29 02:50:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:50:34 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.155 s
23/11/29 02:50:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:50:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:50:34 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.162484 s
23/11/29 02:50:34 INFO FileFormatWriter: Start to commit write Job 2a7ad71d-c466-4389-a8dc-76ffbb79c117.
23/11/29 02:50:34 INFO FileFormatWriter: Write Job 2a7ad71d-c466-4389-a8dc-76ffbb79c117 committed. Elapsed time: 14 ms.
23/11/29 02:50:34 INFO FileFormatWriter: Finished processing stats for write job 2a7ad71d-c466-4389-a8dc-76ffbb79c117.
23/11/29 02:50:34 INFO SparkUI: Stopped Spark web UI at http://15241e1d2090:4040
23/11/29 02:50:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:50:34 INFO MemoryStore: MemoryStore cleared
23/11/29 02:50:34 INFO BlockManager: BlockManager stopped
23/11/29 02:50:34 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:50:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:50:34 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:50:34 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:50:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-904dacf4-1e50-48e8-989b-4ff01ba8fe46
23/11/29 02:50:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-484697ca-9069-40e4-82e7-0018ddbedb82
Execution 9:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-76fc753a-21bf-46f6-884d-91bb36f839a4;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (1709ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (2960ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (606ms)
:: resolution report :: resolve 8915ms :: artifacts dl 5295ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-76fc753a-21bf-46f6-884d-91bb36f839a4
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/13ms)
23/11/29 02:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:50:52 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:50:52 INFO ResourceUtils: ==============================================================
23/11/29 02:50:52 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:50:52 INFO ResourceUtils: ==============================================================
23/11/29 02:50:52 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:50:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:50:52 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:50:52 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:50:52 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:50:52 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:50:52 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:50:52 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:50:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:50:52 INFO Utils: Successfully started service 'sparkDriver' on port 44719.
23/11/29 02:50:52 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:50:52 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:50:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:50:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:50:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:50:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fbb82117-63f9-43e8-a06e-2f23c2cbd67c
23/11/29 02:50:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:50:52 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:50:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:50:53 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://7c02b23fde1c:44719/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://7c02b23fde1c:44719/jars/org.tukaani_xz-1.9.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://7c02b23fde1c:44719/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://7c02b23fde1c:44719/jars/spark.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO Executor: Starting executor ID driver on host 7c02b23fde1c
23/11/29 02:50:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:50:53 INFO Executor: Fetching spark://7c02b23fde1c:44719/jars/org.tukaani_xz-1.9.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO TransportClientFactory: Successfully created connection to 7c02b23fde1c/172.17.0.2:44719 after 27 ms (0 ms spent in bootstraps)
23/11/29 02:50:53 INFO Utils: Fetching spark://7c02b23fde1c:44719/jars/org.tukaani_xz-1.9.jar to /tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/fetchFileTemp9897926316400701149.tmp
23/11/29 02:50:53 INFO Executor: Adding file:/tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:50:53 INFO Executor: Fetching spark://7c02b23fde1c:44719/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO Utils: Fetching spark://7c02b23fde1c:44719/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/fetchFileTemp666979260579800760.tmp
23/11/29 02:50:53 INFO Executor: Adding file:/tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:50:53 INFO Executor: Fetching spark://7c02b23fde1c:44719/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO Utils: Fetching spark://7c02b23fde1c:44719/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/fetchFileTemp16301074539821205602.tmp
23/11/29 02:50:53 INFO Executor: Adding file:/tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:50:53 INFO Executor: Fetching spark://7c02b23fde1c:44719/jars/spark.jar with timestamp 1701226252395
23/11/29 02:50:53 INFO Utils: Fetching spark://7c02b23fde1c:44719/jars/spark.jar to /tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/fetchFileTemp6024429436091219123.tmp
23/11/29 02:50:53 INFO Executor: Adding file:/tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9/userFiles-bb4ad509-357c-44dd-a14e-4aedb6e4a592/spark.jar to class loader
23/11/29 02:50:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39477.
23/11/29 02:50:53 INFO NettyBlockTransferService: Server created on 7c02b23fde1c:39477
23/11/29 02:50:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:50:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 7c02b23fde1c, 39477, None)
23/11/29 02:50:53 INFO BlockManagerMasterEndpoint: Registering block manager 7c02b23fde1c:39477 with 434.4 MiB RAM, BlockManagerId(driver, 7c02b23fde1c, 39477, None)
23/11/29 02:50:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 7c02b23fde1c, 39477, None)
23/11/29 02:50:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 7c02b23fde1c, 39477, None)
23/11/29 02:50:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:50:54 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:50:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:50:55 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
23/11/29 02:50:57 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:57 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:57 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:50:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:50:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 434.2 MiB)
23/11/29 02:50:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 7c02b23fde1c:39477 (size: 34.0 KiB, free: 434.4 MiB)
23/11/29 02:50:57 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:50:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:57 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:50:57 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:50:57 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:50:57 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:57 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:50:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:50:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:50:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 7c02b23fde1c:39477 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:50:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:50:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:50:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (7c02b23fde1c, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:50:58 INFO CodeGenerator: Code generated in 134.407765 ms
23/11/29 02:50:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2156 bytes result sent to driver
23/11/29 02:50:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 402 ms on 7c02b23fde1c (executor driver) (1/1)
23/11/29 02:50:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:50:58 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.495 s
23/11/29 02:50:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:50:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:50:58 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.538849 s
23/11/29 02:50:58 INFO CodeGenerator: Code generated in 14.082742 ms
23/11/29 02:50:58 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/11/29 02:50:58 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:50:58 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:50:58 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:50:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:50:58 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:50:58 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:50:58 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:50:58 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 7c02b23fde1c:39477 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:50:58 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:50:58 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:50:58 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:50:58 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:50:58 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:50:58 INFO DAGScheduler: Missing parents: List()
23/11/29 02:50:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:50:58 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:50:58 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 02:50:58 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 7c02b23fde1c:39477 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 02:50:58 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:50:58 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:50:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:50:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (7c02b23fde1c, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (7c02b23fde1c, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (7c02b23fde1c, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (7c02b23fde1c, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (7c02b23fde1c, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (7c02b23fde1c, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (7c02b23fde1c, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:50:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:50:58 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:50:58 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:50:58 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:50:58 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:50:58 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:50:58 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:50:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:50:58 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:50:59 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:50:59 INFO CodeGenerator: Code generated in 81.851676 ms
23/11/29 02:50:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 7c02b23fde1c:39477 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:50:59 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 7c02b23fde1c:39477 in memory (size: 34.0 KiB, free: 434.3 MiB)
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000006
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:51:00 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1271 ms on 7c02b23fde1c (executor driver) (1/7)
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000002
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000002_3: Committed. Elapsed time: 2 ms.
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000003
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000003_4: Committed. Elapsed time: 5 ms.
23/11/29 02:51:00 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2019 ms on 7c02b23fde1c (executor driver) (2/7)
23/11/29 02:51:00 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2021 ms on 7c02b23fde1c (executor driver) (3/7)
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000005
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:51:00 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2038 ms on 7c02b23fde1c (executor driver) (4/7)
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000001
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000001_2: Committed. Elapsed time: 1 ms.
23/11/29 02:51:00 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2052 ms on 7c02b23fde1c (executor driver) (5/7)
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000000
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:51:00 INFO FileOutputCommitter: Saved output of task 'attempt_202311290250582757764656750543306_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290250582757764656750543306_0001_m_000004
23/11/29 02:51:00 INFO SparkHadoopMapRedUtil: attempt_202311290250582757764656750543306_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:51:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2067 ms on 7c02b23fde1c (executor driver) (6/7)
23/11/29 02:51:00 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:51:00 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2065 ms on 7c02b23fde1c (executor driver) (7/7)
23/11/29 02:51:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:51:00 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.116 s
23/11/29 02:51:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:51:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:51:00 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.121702 s
23/11/29 02:51:00 INFO FileFormatWriter: Start to commit write Job 8519fbad-46d9-4d8a-9ae9-df5aab97e261.
23/11/29 02:51:00 INFO FileFormatWriter: Write Job 8519fbad-46d9-4d8a-9ae9-df5aab97e261 committed. Elapsed time: 14 ms.
23/11/29 02:51:00 INFO FileFormatWriter: Finished processing stats for write job 8519fbad-46d9-4d8a-9ae9-df5aab97e261.
23/11/29 02:51:00 INFO SparkUI: Stopped Spark web UI at http://7c02b23fde1c:4040
23/11/29 02:51:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:51:00 INFO MemoryStore: MemoryStore cleared
23/11/29 02:51:00 INFO BlockManager: BlockManager stopped
23/11/29 02:51:00 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:51:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:51:00 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 5
23/11/29 02:51:00 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:51:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-5696ea7e-6ec5-42dc-809f-9b8f9002a4f9
23/11/29 02:51:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce33ee59-1ff0-4491-8628-c5b6b105dd3d
Execution 10:
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /tmp
The jars for the packages stored in: /tmp/jars
org.apache.spark#spark-avro_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-11d3d9d8-5f6b-4e9e-8b1c-346d7e937d6f;1.0
	confs: [default]
	found org.apache.spark#spark-avro_2.12;3.3.2 in central
	found org.tukaani#xz;1.9 in central
	found org.spark-project.spark#unused;1.0.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.3.2/spark-avro_2.12-3.3.2.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.3.2!spark-avro_2.12.jar (597ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.9/xz-1.9.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.9!xz.jar (514ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (475ms)
:: resolution report :: resolve 7527ms :: artifacts dl 1606ms
	:: modules in use:
	org.apache.spark#spark-avro_2.12;3.3.2 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.9 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-11d3d9d8-5f6b-4e9e-8b1c-346d7e937d6f
	confs: [default]
	3 artifacts copied, 0 already retrieved (306kB/16ms)
23/11/29 02:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/11/29 02:51:13 INFO SparkContext: Running Spark version 3.3.3
23/11/29 02:51:13 INFO ResourceUtils: ==============================================================
23/11/29 02:51:13 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/29 02:51:13 INFO ResourceUtils: ==============================================================
23/11/29 02:51:13 INFO SparkContext: Submitted application: AppSpark
23/11/29 02:51:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/29 02:51:13 INFO ResourceProfile: Limiting resource is cpu
23/11/29 02:51:13 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/29 02:51:13 INFO SecurityManager: Changing view acls to: spark
23/11/29 02:51:13 INFO SecurityManager: Changing modify acls to: spark
23/11/29 02:51:13 INFO SecurityManager: Changing view acls groups to: 
23/11/29 02:51:13 INFO SecurityManager: Changing modify acls groups to: 
23/11/29 02:51:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
23/11/29 02:51:14 INFO Utils: Successfully started service 'sparkDriver' on port 34215.
23/11/29 02:51:14 INFO SparkEnv: Registering MapOutputTracker
23/11/29 02:51:14 INFO SparkEnv: Registering BlockManagerMaster
23/11/29 02:51:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/29 02:51:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/29 02:51:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/29 02:51:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9c23ccf-5010-4a2b-b1ff-b2c2fdc25516
23/11/29 02:51:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
23/11/29 02:51:14 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/29 02:51:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/29 02:51:14 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar at spark://fd33774109ac:34215/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO SparkContext: Added JAR file:///tmp/jars/org.tukaani_xz-1.9.jar at spark://fd33774109ac:34215/jars/org.tukaani_xz-1.9.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO SparkContext: Added JAR file:///tmp/jars/org.spark-project.spark_unused-1.0.0.jar at spark://fd33774109ac:34215/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO SparkContext: Added JAR file:/resources/spark.jar at spark://fd33774109ac:34215/jars/spark.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO Executor: Starting executor ID driver on host fd33774109ac
23/11/29 02:51:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
23/11/29 02:51:14 INFO Executor: Fetching spark://fd33774109ac:34215/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO TransportClientFactory: Successfully created connection to fd33774109ac/172.17.0.2:34215 after 31 ms (0 ms spent in bootstraps)
23/11/29 02:51:14 INFO Utils: Fetching spark://fd33774109ac:34215/jars/org.apache.spark_spark-avro_2.12-3.3.2.jar to /tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/fetchFileTemp15117913590482846369.tmp
23/11/29 02:51:14 INFO Executor: Adding file:/tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/org.apache.spark_spark-avro_2.12-3.3.2.jar to class loader
23/11/29 02:51:14 INFO Executor: Fetching spark://fd33774109ac:34215/jars/spark.jar with timestamp 1701226273758
23/11/29 02:51:14 INFO Utils: Fetching spark://fd33774109ac:34215/jars/spark.jar to /tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/fetchFileTemp17986781837369064362.tmp
23/11/29 02:51:15 INFO Executor: Adding file:/tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/spark.jar to class loader
23/11/29 02:51:15 INFO Executor: Fetching spark://fd33774109ac:34215/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1701226273758
23/11/29 02:51:15 INFO Utils: Fetching spark://fd33774109ac:34215/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/fetchFileTemp5247635807486115385.tmp
23/11/29 02:51:15 INFO Executor: Adding file:/tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/org.spark-project.spark_unused-1.0.0.jar to class loader
23/11/29 02:51:15 INFO Executor: Fetching spark://fd33774109ac:34215/jars/org.tukaani_xz-1.9.jar with timestamp 1701226273758
23/11/29 02:51:15 INFO Utils: Fetching spark://fd33774109ac:34215/jars/org.tukaani_xz-1.9.jar to /tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/fetchFileTemp9850287578492029530.tmp
23/11/29 02:51:15 INFO Executor: Adding file:/tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00/userFiles-2a721e5f-8247-42df-9bc9-e1ea1065873d/org.tukaani_xz-1.9.jar to class loader
23/11/29 02:51:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32771.
23/11/29 02:51:15 INFO NettyBlockTransferService: Server created on fd33774109ac:32771
23/11/29 02:51:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
23/11/29 02:51:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, fd33774109ac, 32771, None)
23/11/29 02:51:15 INFO BlockManagerMasterEndpoint: Registering block manager fd33774109ac:32771 with 434.4 MiB RAM, BlockManagerId(driver, fd33774109ac, 32771, None)
23/11/29 02:51:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, fd33774109ac, 32771, None)
23/11/29 02:51:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, fd33774109ac, 32771, None)
23/11/29 02:51:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/29 02:51:15 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
23/11/29 02:51:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.
23/11/29 02:51:16 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.
23/11/29 02:51:18 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:51:18 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:51:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/11/29 02:51:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.0 KiB, free 434.2 MiB)
23/11/29 02:51:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 434.2 MiB)
23/11/29 02:51:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on fd33774109ac:32771 (size: 33.9 KiB, free: 434.4 MiB)
23/11/29 02:51:19 INFO SparkContext: Created broadcast 0 from collectAsList at AppSpark.java:27
23/11/29 02:51:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:51:19 INFO SparkContext: Starting job: collectAsList at AppSpark.java:27
23/11/29 02:51:19 INFO DAGScheduler: Got job 0 (collectAsList at AppSpark.java:27) with 1 output partitions
23/11/29 02:51:19 INFO DAGScheduler: Final stage: ResultStage 0 (collectAsList at AppSpark.java:27)
23/11/29 02:51:19 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:51:19 INFO DAGScheduler: Missing parents: List()
23/11/29 02:51:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27), which has no missing parents
23/11/29 02:51:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.5 KiB, free 434.2 MiB)
23/11/29 02:51:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.2 MiB)
23/11/29 02:51:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on fd33774109ac:32771 (size: 4.5 KiB, free: 434.4 MiB)
23/11/29 02:51:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1509
23/11/29 02:51:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at collectAsList at AppSpark.java:27) (first 15 tasks are for partitions Vector(0))
23/11/29 02:51:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
23/11/29 02:51:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (fd33774109ac, executor driver, partition 0, PROCESS_LOCAL, 4900 bytes) taskResourceAssignments Map()
23/11/29 02:51:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
23/11/29 02:51:19 INFO FileScanRDD: Reading File path: file:///resources/schema.avsc, range: 0-2303, partition values: [empty row]
23/11/29 02:51:19 INFO CodeGenerator: Code generated in 154.804492 ms
23/11/29 02:51:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2199 bytes result sent to driver
23/11/29 02:51:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 415 ms on fd33774109ac (executor driver) (1/1)
23/11/29 02:51:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
23/11/29 02:51:19 INFO DAGScheduler: ResultStage 0 (collectAsList at AppSpark.java:27) finished in 0.527 s
23/11/29 02:51:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:51:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
23/11/29 02:51:19 INFO DAGScheduler: Job 0 finished: collectAsList at AppSpark.java:27, took 0.565891 s
23/11/29 02:51:19 INFO CodeGenerator: Code generated in 19.320519 ms
23/11/29 02:51:19 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/11/29 02:51:20 INFO FileSourceStrategy: Pushed Filters: 
23/11/29 02:51:20 INFO FileSourceStrategy: Post-Scan Filters: 
23/11/29 02:51:20 INFO FileSourceStrategy: Output Data Schema: struct<ID: string, Source: string, Severity: int, Start_Time: string, End_Time: string ... 44 more fields>
23/11/29 02:51:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
23/11/29 02:51:20 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
23/11/29 02:51:20 INFO AvroUtils: Compressing Avro output using the snappy codec
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.0 KiB, free 434.0 MiB)
23/11/29 02:51:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 433.9 MiB)
23/11/29 02:51:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on fd33774109ac:32771 (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:51:20 INFO SparkContext: Created broadcast 2 from save at AppSpark.java:39
23/11/29 02:51:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/11/29 02:51:20 INFO SparkContext: Starting job: save at AppSpark.java:39
23/11/29 02:51:20 INFO DAGScheduler: Got job 1 (save at AppSpark.java:39) with 7 output partitions
23/11/29 02:51:20 INFO DAGScheduler: Final stage: ResultStage 1 (save at AppSpark.java:39)
23/11/29 02:51:20 INFO DAGScheduler: Parents of final stage: List()
23/11/29 02:51:20 INFO DAGScheduler: Missing parents: List()
23/11/29 02:51:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39), which has no missing parents
23/11/29 02:51:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 221.6 KiB, free 433.7 MiB)
23/11/29 02:51:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 78.7 KiB, free 433.6 MiB)
23/11/29 02:51:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on fd33774109ac:32771 (size: 78.7 KiB, free: 434.3 MiB)
23/11/29 02:51:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1509
23/11/29 02:51:20 INFO DAGScheduler: Submitting 7 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at save at AppSpark.java:39) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))
23/11/29 02:51:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 7 tasks resource profile 0
23/11/29 02:51:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (fd33774109ac, executor driver, partition 0, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (fd33774109ac, executor driver, partition 1, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (fd33774109ac, executor driver, partition 2, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (fd33774109ac, executor driver, partition 3, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (fd33774109ac, executor driver, partition 4, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (fd33774109ac, executor driver, partition 5, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (fd33774109ac, executor driver, partition 6, PROCESS_LOCAL, 4905 bytes) taskResourceAssignments Map()
23/11/29 02:51:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
23/11/29 02:51:20 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
23/11/29 02:51:20 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
23/11/29 02:51:20 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
23/11/29 02:51:20 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
23/11/29 02:51:20 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
23/11/29 02:51:20 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 8388608-12582912, partition values: [empty row]
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 4194304-8388608, partition values: [empty row]
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 25165824-26288329, partition values: [empty row]
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 20971520-25165824, partition values: [empty row]
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 16777216-20971520, partition values: [empty row]
23/11/29 02:51:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/11/29 02:51:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/11/29 02:51:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 12582912-16777216, partition values: [empty row]
23/11/29 02:51:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on fd33774109ac:32771 in memory (size: 4.5 KiB, free: 434.3 MiB)
23/11/29 02:51:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on fd33774109ac:32771 in memory (size: 33.9 KiB, free: 434.3 MiB)
23/11/29 02:51:20 INFO FileScanRDD: Reading File path: file:///resources/us_accidents.csv, range: 0-4194304, partition values: [empty row]
23/11/29 02:51:20 INFO CodeGenerator: Code generated in 92.052883 ms
23/11/29 02:51:21 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000006_7' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000006
23/11/29 02:51:21 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000006_7: Committed. Elapsed time: 1 ms.
23/11/29 02:51:21 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2541 bytes result sent to driver
23/11/29 02:51:21 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1260 ms on fd33774109ac (executor driver) (1/7)
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000002_3' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000002
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000002_3: Committed. Elapsed time: 1 ms.
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000005_6' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000005
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000005_6: Committed. Elapsed time: 1 ms.
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000001_2' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000001
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000001_2: Committed. Elapsed time: 0 ms.
23/11/29 02:51:22 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 2471 ms on fd33774109ac (executor driver) (2/7)
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000003_4' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000003
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000003_4: Committed. Elapsed time: 1 ms.
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000000_1' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000000
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000000_1: Committed. Elapsed time: 1 ms.
23/11/29 02:51:22 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2484 ms on fd33774109ac (executor driver) (3/7)
23/11/29 02:51:22 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2482 ms on fd33774109ac (executor driver) (4/7)
23/11/29 02:51:22 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 2482 ms on fd33774109ac (executor driver) (5/7)
23/11/29 02:51:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2490 ms on fd33774109ac (executor driver) (6/7)
23/11/29 02:51:22 INFO FileOutputCommitter: Saved output of task 'attempt_202311290251203959118154860434205_0001_m_000004_5' to file:/opt/spark/work-dir/output/_temporary/0/task_202311290251203959118154860434205_0001_m_000004
23/11/29 02:51:22 INFO SparkHadoopMapRedUtil: attempt_202311290251203959118154860434205_0001_m_000004_5: Committed. Elapsed time: 1 ms.
23/11/29 02:51:22 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2498 bytes result sent to driver
23/11/29 02:51:22 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2513 ms on fd33774109ac (executor driver) (7/7)
23/11/29 02:51:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
23/11/29 02:51:22 INFO DAGScheduler: ResultStage 1 (save at AppSpark.java:39) finished in 2.560 s
23/11/29 02:51:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
23/11/29 02:51:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
23/11/29 02:51:22 INFO DAGScheduler: Job 1 finished: save at AppSpark.java:39, took 2.566893 s
23/11/29 02:51:22 INFO FileFormatWriter: Start to commit write Job 50217283-2a54-4805-aab7-55e451782ead.
23/11/29 02:51:22 INFO FileFormatWriter: Write Job 50217283-2a54-4805-aab7-55e451782ead committed. Elapsed time: 26 ms.
23/11/29 02:51:22 INFO FileFormatWriter: Finished processing stats for write job 50217283-2a54-4805-aab7-55e451782ead.
23/11/29 02:51:22 INFO SparkUI: Stopped Spark web UI at http://fd33774109ac:4040
23/11/29 02:51:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/29 02:51:22 INFO MemoryStore: MemoryStore cleared
23/11/29 02:51:22 INFO BlockManager: BlockManager stopped
23/11/29 02:51:22 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/29 02:51:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/29 02:51:22 INFO SparkContext: Successfully stopped SparkContext
Time in sec: 6
23/11/29 02:51:22 INFO ShutdownHookManager: Shutdown hook called
23/11/29 02:51:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa6b483a-dd2c-4bdf-9fd9-b50c40bed544
23/11/29 02:51:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-695637a9-fa11-4a6e-91c7-b7f659c78b00
